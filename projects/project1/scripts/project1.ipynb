{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = correction_missing_values(tX)\n",
    "tX, mean_x, std_x = standardize(tX)\n",
    "tX = normalize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=13202.974980721261, testing loss=2707.455838522011\n",
      "Current iteration=1000, training loss=2319.486070705529, testing loss=578.2460366938533\n",
      "Current iteration=2000, training loss=2221.9820996961935, testing loss=553.0657058362972\n",
      "Current iteration=3000, training loss=2168.3610518956666, testing loss=538.8621953280376\n",
      "Current iteration=4000, training loss=2134.493751755974, testing loss=529.723206455981\n",
      "Current iteration=5000, training loss=2111.230406923831, testing loss=523.3833618045206\n",
      "Current iteration=6000, training loss=2094.2792969131915, testing loss=518.7583848867007\n",
      "Current iteration=7000, training loss=2081.3499343698713, testing loss=515.2542817145379\n",
      "Current iteration=8000, training loss=2071.116476250776, testing loss=512.5173755946313\n",
      "Current iteration=9000, training loss=2062.766467742662, testing loss=510.32471612092024\n",
      "Current iteration=10000, training loss=2055.779514374804, testing loss=508.5295530904445\n",
      "Current iteration=11000, training loss=2049.8097684711583, testing loss=507.03185558531936\n",
      "Current iteration=12000, training loss=2044.6198928118165, testing loss=505.76140791044713\n",
      "Current iteration=13000, training loss=2040.0422912011318, testing loss=504.6676697917246\n",
      "Current iteration=14000, training loss=2035.9555189196162, testing loss=503.71346554223453\n",
      "[ -2.49067286  -0.82527902 -12.91257424  -2.12575053   3.00708394\n",
      "   2.51089695   1.07487633  -2.55452018   5.32599714  -2.37366527\n",
      "   1.30746652  -4.76807196   7.27019009   4.18187955   7.20343408\n",
      "   1.01308746   0.20396451   2.64523511   0.86375651   0.94409508\n",
      "  -0.53841065  -0.6548827    0.38729268   0.4557399    0.29020473\n",
      "  -1.89310808  -0.70564117  -2.97842365   0.0734396   -0.98330293\n",
      "   0.10854873] 2032.2729962851145\n",
      "training loss=2032.269486237467\n",
      "testing loss=502.8717250539258\n",
      "Accuracy from our code:\n",
      "\n",
      "Logistic regression: 73.0 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from logistic_regression import *\n",
    "#Using our code\n",
    "#Logistic regression\n",
    "\n",
    "gamma = 0.0001\n",
    "n_iter = 15000\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w = np.random.rand(tX.shape[1])\n",
    "for i in range(n_iter):\n",
    "    w, loss = learning_by_gradient_descent(y_training, tx_training, w, gamma)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Current iteration={iter}, training loss={l_tr}, testing loss={l_te}\".format(iter=i, l_tr=loss, \n",
    "                                                                                                         l_te=calculate_loss(y_testing, tx_testing, w)))\n",
    "print(w, loss)\n",
    "print(\"training loss={l_tr}\".format(l_tr=calculate_loss(y_training, tx_training, w)))\n",
    "print(\"testing loss={l_te}\".format(l_te=calculate_loss(y_testing, tx_testing, w)))\n",
    "\n",
    "print(\"Accuracy from our code:\\n\")\n",
    "print(\"Logistic regression: \"+str(round(100*np.sum(predict_labels(w, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.14232282  1.05143726 -0.31839768  0.74215337 -0.21427527  0.15107391\n",
      "  0.98440476 -0.048879   -1.69136925  0.77732971 -1.14855687 -0.1084819\n",
      " -2.13172693 -0.47674788 -0.97991641  0.06516325  0.0078632  -2.01249868\n",
      " -0.36849353 -0.07843186 -1.91616335 -2.1234803  -0.17136832 -1.66386988\n",
      "  0.51522189 -1.26887988  0.03714418 -2.50118872 -1.30113526 -2.09634312\n",
      " -1.25128261 -1.72780539 -0.54611592 -1.47744534 -0.65256589 -2.41396657\n",
      " -1.15721227 -2.22339019 -1.37953417 -1.98050226 -0.04503699 -1.56385966\n",
      "  0.15738274 -1.99266606 -3.09017762 -1.96003287 -1.27398758 -1.9252832\n",
      "  0.15117781 -1.4717589  -2.63564193  0.44533038 -1.76300576 -0.36407524\n",
      " -2.35658451 -0.8141005  -0.81080466 -0.04450205 -1.09218705  1.25872496\n",
      " -2.12012705 -1.4008896  -2.20853102  1.58969536 -1.18678582 -1.8572378\n",
      "  0.0915619  -0.36766359 -1.55752558 -1.18020881  0.35392829 -1.43898707\n",
      " -0.11747342 -0.10554302 -1.27075008 -0.6358583  -1.05520394 -4.06065061\n",
      " -1.87447469 -1.31479349  0.29999161 -1.93216933 -0.17116143 -2.14069164\n",
      " -1.63807732 -2.40463141 -0.51714834 -1.08465068  0.72571908 -2.26007209\n",
      " -0.050788   -2.36928117 -0.65207595  0.91676024 -0.30840694 -2.39206717\n",
      " -0.75488368 -0.39681419 -1.55428518 -1.68697754 -2.3313089  -1.90653389\n",
      " -1.89039429  0.12136384 -1.56545163 -3.79179204 -2.28340836 -1.8691214\n",
      " -1.97909264 -1.04145244 -3.0493864  -1.62768558 -0.41067879 -1.22163795\n",
      " -0.05846871 -0.42064427 -3.61611962 -0.88942904 -1.42382749  0.13033346\n",
      " -0.2284356   0.533131   -0.7847891  -1.48546629  2.8120378  -0.06877698\n",
      " -1.74793554  0.48150553 -1.40262471 -0.58514975 -2.03231637 -1.53294829\n",
      "  0.02656105 -3.05265147 -1.49184767 -2.05437292  0.42758577 -3.05816957\n",
      " -0.26814666 -0.96463308 -1.07366482 -0.6258669  -1.92885104 -1.44020683\n",
      " -0.18862753 -1.25607086  0.11870535 -0.98188397 -2.1154388   0.54237114\n",
      " -1.33758065 -2.28273688 -0.12605388 -1.98022695 -0.49982928 -2.15641565\n",
      " -2.23601703 -1.69300203 -1.86710191 -0.58593726 -0.24763894  0.99276335\n",
      "  0.79041268 -1.06873513 -2.19699596 -1.21069937  0.30025127  1.76745359\n",
      " -1.27830221 -0.44614871 -0.9486501  -2.56991623  0.55828401 -0.78826061\n",
      " -0.35215708  0.28810805 -0.88820877  1.68689797 -2.24771001 -1.01810129\n",
      " -0.52761042 -0.75064538  1.27827229  0.23904001 -0.90216518 -0.78786674\n",
      " -0.97310759 -1.33152989  0.44726871  0.24127304 -1.81724155  0.80798209\n",
      " -1.95072984  0.21696147 -1.30737577  0.00978358 -1.61718132 -0.03938066\n",
      " -0.29356317 -1.4332929  -2.64377194 -1.34900074 -1.98002562 -0.32018738\n",
      " -1.473853   -1.45965996 -0.82105    -2.29375653 -0.19035228 -0.87884217\n",
      " -0.74099911 -1.73359048 -1.12580898  0.6931361  -0.34080826 -2.66199536\n",
      " -1.10449843 -3.12615052  0.30461087 -2.32449147 -1.47647353 -0.160122\n",
      " -0.40702478 -1.92811126 -0.79215952 -1.46596277  0.13756199 -0.30216543\n",
      " -1.1037244  -0.34509733 -0.97249952 -0.99164436 -0.30075544 -1.38441962\n",
      "  0.28759686 -0.57253703 -2.04698579  0.19539837 -0.17983033 -1.24209031\n",
      " -0.84871455 -2.02745618  1.25083108 -0.53164312  0.23088957 -2.38983866\n",
      " -1.24936364 -1.32636044  0.5359136   1.95008215  0.03292427  0.17543748\n",
      " -0.60989347 -1.00700196 -0.23703858  0.38071207 -0.15050788 -1.16579256\n",
      "  0.76219855 -1.35462494 -1.67111563 -1.61667015 -1.89840284  0.21889042\n",
      " -0.60535787 -2.4363081   1.2636388  -1.28170189 -0.04335125 -0.766289\n",
      "  0.20487811 -1.75819557 -0.85973202 -1.66031411 -0.16846572 -1.40195134\n",
      " -0.94214718 -1.54978114 -0.05728245 -0.51368065 -1.86375753 -2.09926421\n",
      " -0.88918233 -0.37901591 -1.20167418  0.44393639 -1.38168465 -1.6949286\n",
      "  0.21356284 -0.48966648 -1.25786337 -1.29969077  2.006353   -1.62541301\n",
      " -1.24416659  1.73547956 -1.82999424 -3.66560212  0.64371953 -1.52521519\n",
      " -0.93776404 -2.02673516 -2.66021643  0.01911667 -1.10513128 -2.0028791\n",
      "  3.76982074 -1.85692117 -0.97122998 -0.67689154 -0.16500584  0.22902249\n",
      " -2.64623945  0.38504128 -1.19496322 -1.59670493 -1.55505298 -0.98194537\n",
      " -0.41549573 -0.19080974  0.11194216 -1.07907917  0.27880228 -2.11370439\n",
      " -1.12640077 -0.54461398 -1.1834046  -0.14802156 -1.00823301 -1.78922268\n",
      " -2.35002354 -2.27778134 -1.49734661 -1.72097511 -1.17472681 -0.87734486\n",
      " -2.00787312 -1.68050513  0.11216931 -1.29445629 -0.45838378 -2.07394509\n",
      " -0.72703961 -0.5033465  -1.80276049 -0.60578308 -1.79361027 -1.61706495\n",
      " -1.25723996 -1.78274759  0.61256666 -0.39965953  0.03617729 -1.81749017\n",
      " -0.72947762  1.75828    -1.77934527  2.01339453 -0.93839706 -2.27019562\n",
      " -1.23490039 -0.94058848 -1.22884814 -1.21939747  0.01348028  0.64043094\n",
      "  0.43992861 -1.84395867 -1.36973079 -1.90901788 -0.69417332  0.53802783\n",
      " -1.98066576 -1.45722098 -0.55475641 -1.49846953 -0.93626756 -0.89759852\n",
      "  0.70743928 -2.03434743 -0.6713429  -1.99651257 -2.30348112 -3.95332882\n",
      "  0.14439527 -1.69354148 -0.13723782 -2.4273869  -1.43268032 -1.24317015\n",
      " -1.91742129 -1.98320722 -1.49631065  0.01615863  0.1630598  -0.57938966\n",
      "  0.23607641 -2.51508547 -1.76808844 -0.55691645 -1.09760696 -0.55012067\n",
      " -2.60840147 -1.84797086 -0.22010221 -1.85275912 -1.50584911 -0.89269457\n",
      "  0.20255656  0.68292124 -1.44943252 -0.05133125 -2.8703115  -2.01595153\n",
      " -2.18470322 -1.24730945 -1.76760747 -1.23071449 -1.70530259 -1.13862922\n",
      " -1.44539726 -0.50636129 -1.69340041 -0.35511347 -2.15370188 -1.90338291\n",
      " -0.71541804  0.40088225 -0.45861561  0.04051169 -0.23656959 -0.96841847\n",
      " -1.70445118 -2.24778547 -2.13192933  0.57013575 -1.27245921 -1.91271496\n",
      " -1.75436877 -1.05324778 -0.5272769  -2.63535373  0.24199138 -2.08027715\n",
      " -0.40062754  0.96167042 -0.40800197 -1.63587277  1.57036427 -1.05464932\n",
      " -0.13490065 -1.51014439 -1.12171887 -1.03495702 -1.75399177 -2.13463896\n",
      "  0.69358976 -0.60384654 -2.02300404  0.7612377  -0.20165014 -1.55844102\n",
      " -0.69768169 -0.07823565  0.2112517   0.13047829 -1.74789006 -0.7773917\n",
      "  2.19559622 -1.77219057  0.17690094 -1.18587064 -1.96638906  1.9921994\n",
      " -1.79193515 -1.74275167 -2.54272659 -2.34852323  0.16258227  1.15889624\n",
      "  0.21068023 -0.61296684 -1.39924842  0.1124547  -2.34708911  0.07329625\n",
      " -2.2357899  -1.97825649 -4.40843011 -0.45471772 -0.98766709 -1.35487808\n",
      "  0.82468911 -0.08671486 -1.21841854 -1.02634042  0.24570171 -0.42255413\n",
      " -0.8979271   1.38752931 -0.18779961 -1.96973379 -1.48195827 -1.33335444\n",
      "  0.79187787 -1.04969033 -1.98481786 -1.58083683 -2.57067126 -1.16810204\n",
      " -3.01262532 -0.6310985  -1.95841699 -1.10486315 -0.47739456  0.22577245\n",
      "  1.68430034 -0.51388481 -0.89937142 -1.68518136 -0.18486261 -1.23838267\n",
      " -1.98917331 -1.0178632  -2.38754861 -0.35817878 -1.06475809 -0.86414408\n",
      " -2.12899533 -0.46328213 -0.1861859  -0.56742873  3.07337534 -0.64716495\n",
      "  0.66263879  1.14783792 -2.5699415  -2.10586768 -2.33033773 -1.54402146\n",
      " -1.7601417   0.07062037 -1.03795702 -0.96093824 -0.52773133 -0.84921332\n",
      " -2.27693238  0.46106545 -0.92888002 -1.10599537 -3.4968918  -2.20077968\n",
      " -0.45626614  0.45155329 -2.0924036  -0.34339024 -0.48802264 -0.76884201\n",
      " -1.2466772  -2.2990562  -2.36353803 -1.17222632 -2.69872808 -0.46200342\n",
      " -0.54341524 -1.80126073 -1.21923558 -1.47039259 -0.46532203  1.16605032\n",
      " -0.69362811 -2.59019816 -0.95649111 -0.45783623  0.54401092  0.02013637\n",
      "  0.42263993 -2.11144686 -1.47576498 -0.63727939 -0.95424571 -0.9265558\n",
      "  0.43786997 -1.32191811  0.25222757  0.08907809 -1.47517099 -0.54131087\n",
      " -1.97982029  0.50168923 -1.92540632 -2.42112872 -1.45046863 -2.26021341\n",
      " -2.3516931   0.60193785  0.45223101 -0.60168823 -2.14700535 -0.795402\n",
      " -1.79299104 -2.51670752 -1.55523824 -1.51267402 -0.18210463  0.10879899\n",
      " -0.44174581 -1.85359961 -1.21237806  0.45178067 -0.96632031  0.72693006\n",
      " -0.36642797 -0.30964772 -1.59586674 -1.43291428 -2.03467862 -0.45481335\n",
      " -0.24522493 -0.82330938 -1.38773393 -0.55051246 -1.92514422 -0.9667603\n",
      " -0.96841753 -2.11271029 -0.80926932 -1.12971811 -1.17677832 -1.23599102\n",
      " -2.90062196 -0.78519344 -2.1151093  -2.12053464  0.2253102   0.04996541\n",
      "  0.16045481 -0.80612986 -1.82252785 -1.62620163 -0.13018556  0.91402876\n",
      " -3.69548354  0.72557602 -0.99801294 -0.40574705 -1.0951178  -1.33507302\n",
      "  0.00600887 -2.09186207  0.13482333 -0.07715675 -2.31938552  0.9779737\n",
      " -1.3024825  -1.05533162 -0.32265452 -0.16631279 -1.18624007 -1.15665088\n",
      " -0.21092402 -1.68958286  0.07383489 -1.53745105 -1.45517519 -0.88674142\n",
      " -1.48709654  0.29175699 -0.76080102 -0.16598167 -0.91797088 -1.15183284\n",
      " -1.36341073 -2.11181998 -2.20911299 -2.02940455 -0.16544903 -0.88175189\n",
      " -1.98826999 -1.02381085  0.72968864  0.89865601 -0.75011329 -0.49258395\n",
      " -0.10877564 -0.53884454 -0.86438492 -0.90213707 -1.36995134 -1.2138924\n",
      " -1.9774644  -2.08469789 -0.25363204 -0.826114   -1.10448531 -2.36623621\n",
      " -2.15404087 -0.01017651 -1.63547656 -3.05322554 -2.31479203 -0.66498724\n",
      " -0.18558794  1.43569832 -2.18033658 -0.99272439 -1.40602268 -1.94848427\n",
      " -1.05346522  0.71164642  1.8079065  -1.61825102  0.61906598 -1.20371648\n",
      " -1.31591756 -1.19922093 -0.16651233 -0.45335746 -1.25957625  0.11238249\n",
      " -2.35384228  1.7963579  -0.95362397 -1.99416083  1.30976794 -2.09172327\n",
      " -1.00491335 -0.69971809 -1.33345408 -1.70840054  0.72298583 -1.44898164\n",
      " -3.00281944 -0.23235487  1.37047369  1.05208069 -0.11345109 -1.60652081\n",
      " -0.52605406 -2.16188523  0.5942789  -1.25926096  0.0378685  -0.3665346\n",
      "  1.72004511 -2.05181226  0.96655888 -1.14465444 -1.07569554 -1.29115096\n",
      " -2.90095089  1.80882278  1.57247129 -0.89873799  1.02066588 -0.97726606\n",
      " -0.42883805 -2.04004407 -0.53651773  0.04590123 -2.15173761 -0.97312783\n",
      "  2.13421159 -1.50613281 -0.8411092  -1.98453763  0.63136647 -1.87894716\n",
      " -0.86185265 -1.26887972 -1.74655782 -1.49120474 -2.13842314 -1.17739223\n",
      "  2.14698392  0.24801146 -2.62767689 -0.0664585  -2.40684383  0.19162212\n",
      " -0.62231875 -1.33699729 -1.86816568  1.49108238 -0.2282843  -0.39487583\n",
      " -0.17471788 -2.01091154 -0.80247093  0.51573024 -0.00501809  0.17770803\n",
      " -1.58984536 -0.17424072 -0.08947837  0.46245808 -1.78683217 -0.27066053\n",
      " -2.31045074  0.22186342 -1.78131262 -2.53195329 -3.28660552 -1.25975143\n",
      " -1.7402442  -0.12321417 -0.45911928 -0.39794307 -1.66196151 -1.77294173\n",
      " -0.51406082 -2.14263822 -3.2350473   0.54375172 -1.52457937 -1.16957399\n",
      " -0.64031319 -0.39937132  0.22665909  0.72879191 -1.16700431  0.55504022\n",
      " -0.51872557 -0.53038773  0.54868117 -2.54797455 -2.50914278 -1.0357483\n",
      "  0.76684213 -0.27139501 -1.45661118 -1.0858701   1.32278893 -1.6371762\n",
      " -1.71529899 -2.82152482  0.10987148 -0.92543788 -0.66356745 -1.01136229\n",
      " -0.03466574 -0.28313646 -1.77602499 -0.50316126  0.45273207 -2.13292789\n",
      "  0.46261786  0.23839533 -0.54833218 -1.57705918 -0.29006691 -2.85919563\n",
      " -1.00342404 -2.34456862 -2.04794805  0.73198919 -1.45461307 -0.82342128\n",
      " -1.92577486 -2.37837405 -2.32852782 -1.06945802 -1.71578787 -0.24699644\n",
      " -1.83024931 -1.76646465  2.20620992 -0.07019098 -0.29986957 -0.82665579\n",
      " -2.1781631   2.13022562  0.07604015 -2.37933518 -1.12805781 -0.24201375\n",
      " -1.99223635 -2.11628535 -1.81797864 -2.26632075 -0.20027778 -2.42862886\n",
      " -0.56791046 -2.18432949 -2.27323747 -1.7227097  -1.89666411  1.35607669\n",
      " -0.17655383 -0.12241689 -1.46127882  0.64759439 -1.67558241 -0.66094649\n",
      "  1.18480866  0.22128399 -1.06033872  0.7790334  -0.89448975 -1.89893078\n",
      " -0.21712708 -0.63774516 -0.89331845 -0.64533477  0.39395013 -0.90011278\n",
      " -0.81837278 -2.43312595  1.68021813 -0.82071474 -3.44163322 -3.15414529\n",
      " -0.10149034 -1.83279683 -0.30273129 -0.65444095 -0.57235148  1.61667312\n",
      " -1.82668667 -1.58986343  0.87561275 -1.96401189 -0.06013549 -1.64419735\n",
      " -0.03256167 -1.49011982 -2.21480105 -1.03023334 -1.35625714 -1.73618851\n",
      " -1.69846077 -1.72936605 -0.33110515 -1.23169699 -1.1334087  -0.27888587\n",
      " -1.90508037  0.01755967 -1.23475531 -0.05709599 -1.22649307 -1.32734009\n",
      "  0.60104561 -0.38646825 -1.09810732 -0.90261735 -0.24346505 -1.55753681\n",
      " -1.36696896 -2.16805113 -0.4513859  -1.85048884 -0.29816128 -2.79595916\n",
      "  0.37372708  0.02406438 -0.75411693 -1.31844715 -2.01235654  0.46225892\n",
      "  0.13455883 -1.5155675  -0.93286508 -2.0926546   0.19457284  0.86215966\n",
      " -1.81880207 -0.88110476 -1.27383558 -1.49756167 -0.9303877  -2.18425664\n",
      " -1.37282034 -2.15590607 -2.20862749 -0.19835083 -0.14302627 -0.11655851\n",
      " -0.34958509 -2.05923605 -1.11398553 -1.10889507 -1.84699105 -1.41182878\n",
      " -0.33905052  0.91573879 -1.9411511   0.46411771 -2.05296715 -1.09070343\n",
      " -1.69882186 -1.06970393 -1.32663202 -0.64899276 -1.21512061 -0.20859731\n",
      " -3.33438547 -0.72695682  1.00591347 -1.0378966 ]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.dot(tx_testing, w)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(w, tx_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=13204.066997849734, testing loss=2707.391303628788\n",
      "Current iteration=1000, training loss=2323.237518955365, testing loss=578.5833774545952\n",
      "Current iteration=2000, training loss=2230.9237714691308, testing loss=553.8627539355654\n",
      "Current iteration=3000, training loss=2182.2360575977823, testing loss=540.0583366568821\n",
      "Current iteration=4000, training loss=2152.7775161649447, testing loss=531.2539086419839\n",
      "Current iteration=5000, training loss=2133.40358979678, testing loss=525.1918822733754\n",
      "Current iteration=6000, training loss=2119.889748543378, testing loss=520.7982865514483\n",
      "Current iteration=7000, training loss=2110.022044900256, testing loss=517.489196586293\n",
      "Current iteration=8000, training loss=2102.543579117453, testing loss=514.9194301449003\n",
      "Current iteration=9000, training loss=2096.698912410284, testing loss=512.8726934446728\n",
      "[ -2.10811681  -0.33184585 -10.30199765  -1.2324899    2.62263618\n",
      "   2.44060578   1.13540108  -2.59279571   3.28471398  -1.87757786\n",
      "   1.14260951  -4.14548518   6.33254882   3.45140548   5.63167344\n",
      "   0.79872133   0.078839     1.15096776   0.61441354   0.75898884\n",
      "  -0.76368397  -0.47705132   0.65287725   0.8279343    0.28006468\n",
      "  -1.5004022   -0.70023846  -2.42117963   0.0375839   -0.95546458\n",
      "   0.43750443] 2092.0172992918197\n",
      "training loss=2065.591265080149\n",
      "testing loss=511.2087925753038\n",
      "After regularization: 71.3 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from reg_logistic_regression import *\n",
    "#Reguralized logistic regression\n",
    "\n",
    "gamma = 0.0001\n",
    "lamda_ = 0.1\n",
    "n_iter = 10000\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w_reg = np.random.rand(tX.shape[1])\n",
    "for i in range(n_iter):\n",
    "    w_reg, loss = learning_by_penalized_gradient(y_training, tx_training, w_reg, gamma, lamda_)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Current iteration={iter}, training loss={l_tr}, testing loss={l_te}\".format(iter=i, l_tr=loss, \n",
    "                                                                                                         l_te=calculate_loss(y_testing, tx_testing, w_reg)))\n",
    "print(w_reg, loss)\n",
    "print(\"training loss={l_tr}\".format(l_tr=calculate_loss(y_training, tx_training, w_reg)))\n",
    "print(\"testing loss={l_te}\".format(l_te=calculate_loss(y_testing, tx_testing, w_reg)))\n",
    "\n",
    "print(\"After regularization: \"+str(round(100*np.sum(predict_labels(w_reg, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights=[-3.36066135e+03  4.93617329e-02 -3.13631732e+00 -2.72965654e+00\n",
      " -7.00355544e-01  4.23332738e-01  3.33081824e-01 -3.00803568e-01\n",
      "  2.86701353e+00 -2.49617854e-01 -5.31349592e+04 -1.78070042e+00\n",
      "  1.53263823e+00  9.97322365e-01  9.51532465e+03  1.37178942e-01\n",
      "  9.39604101e-02  9.62015906e+03  2.77519998e-01  2.30665883e-01\n",
      "  1.37176864e+00 -9.77410894e-02 -5.02513386e-01  3.09587984e-01\n",
      " -3.43462834e-01 -4.03004387e-01 -7.22579053e-02 -6.47831367e-01\n",
      "  3.56178937e-02 -1.54837693e-01  4.55817932e+04], loss=0.08226957578474213\n",
      "training loss=0.08226957578474213\n",
      "testing loss=0.08055938490307227\n",
      "After least squares: 76.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "#least squares\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w_lsq, loss = least_squares(y_training, tx_training)\n",
    "print(\"weights={w}, loss={l}\".format(w=w_lsq, l=loss))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, w_lsq)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, w_lsq)))\n",
    "print(\"After least squares: \"+str(round(100*np.sum(predict_labels(w_lsq, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=11.321584492339047\n",
      "Processing 2th experiment, degree=3, rmse=11.321584492339047\n",
      "Processing 3th experiment, degree=7, rmse=11.321584492339047\n",
      "Processing 4th experiment, degree=12, rmse=11.321584492339047\n"
     ]
    }
   ],
   "source": [
    "def polynomial_regression():\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 3, 7, 12]\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        tx = build_poly(tX, degree)\n",
    "        weights, mse = least_squares(y, tX)\n",
    "        rmse = np.sqrt(2 * mse)\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "              i=ind + 1, d=degree, loss=rmse))\n",
    "        \n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.000, training loss=0.083, testing loss=0.081\n",
      "lambda=0.000, training loss=0.083, testing loss=0.081\n",
      "lambda=0.000, training loss=0.084, testing loss=0.081\n",
      "lambda=0.000, training loss=0.086, testing loss=0.082\n",
      "lambda=0.000, training loss=0.089, testing loss=0.084\n",
      "lambda=0.001, training loss=0.092, testing loss=0.086\n",
      "lambda=0.001, training loss=0.096, testing loss=0.090\n",
      "lambda=0.003, training loss=0.101, testing loss=0.095\n",
      "lambda=0.007, training loss=0.105, testing loss=0.100\n",
      "lambda=0.016, training loss=0.107, testing loss=0.104\n",
      "lambda=0.037, training loss=0.110, testing loss=0.107\n",
      "lambda=0.085, training loss=0.112, testing loss=0.108\n",
      "lambda=0.193, training loss=0.115, testing loss=0.109\n",
      "lambda=0.439, training loss=0.121, testing loss=0.111\n",
      "lambda=1.000, training loss=0.130, testing loss=0.116\n",
      "weights=[0.05898849 0.01702444 0.01349676 0.01675819 0.01916236 0.01891701\n",
      " 0.01868751 0.01543015 0.01700008 0.01694654 0.01876702 0.01527165\n",
      " 0.02002863 0.01888765 0.01951832 0.01725549 0.01712848 0.01684741\n",
      " 0.0173858  0.01740115 0.01739162 0.01723937 0.01856934 0.01865364\n",
      " 0.01810351 0.01680362 0.01675715 0.01686174 0.01719905 0.01671958\n",
      " 0.01859722], loss=0.12956722492822362\n",
      "training loss=0.1168530151919449\n",
      "testing loss=0.11587231092601533\n",
      "After ridge regression: 67.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ridge_regression import ridge_regression\n",
    "#ridge regression\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    # ridge regression with a given lambda\n",
    "    weight_ridge, loss = ridge_regression(y_training, tx_training, lambda_)\n",
    "    print(\"lambda={lam:.3f}, training loss={l_tr:.3f}, testing loss={l_te:.3f}\".format(lam=lambda_, l_tr=loss, \n",
    "                                                                                       l_te=compute_loss(y_testing, tx_testing, weight_ridge)))\n",
    "    \n",
    "print(\"weights={w}, loss={l}\".format(w=weight_ridge, l=loss))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, weight_ridge)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, weight_ridge)))\n",
    "\n",
    "print(\"After ridge regression: \"+str(round(100*np.sum(predict_labels(weight_ridge, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=10.68415524953714\n",
      "Current iteration=1000, loss=0.1006983499804822\n",
      "Current iteration=2000, loss=0.09432945739651882\n",
      "Current iteration=3000, loss=0.09105909807248724\n",
      "Current iteration=4000, loss=0.08919778187398651\n",
      "weights=[-0.67632045  0.1963382  -1.51889326 -0.17343314  0.47233771  0.7712353\n",
      "  0.18353181 -0.526078    0.31597263 -0.11399315 -0.23923952 -0.58794283\n",
      "  1.53959726  0.62252622  1.06487568  0.40417022  0.22069267  0.16713433\n",
      " -0.04947574  0.48755469 -0.09969004 -0.036905    0.15744464  0.23181153\n",
      "  0.16792303 -0.20231044  0.12625754 -0.20213052  0.00415655 -0.16511453\n",
      "  0.24322293], loss=0.08803858500144947\n",
      "training loss=0.08803764894542244\n",
      "testing loss=0.08708009963533041\n",
      "After gradient descent: 74.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import *\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_ws, gradient_losses= gradient_descent(y_training, tx_training, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=gradient_ws, l=gradient_losses))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, gradient_ws)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, gradient_ws)))\n",
    "print(\"After gradient descent: \"+str(round(100*np.sum(predict_labels(gradient_ws, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=10.684155249537138\n",
      "Current iteration=100, loss=0.11611652153666797\n",
      "Current iteration=200, loss=0.14554730629189477\n",
      "Current iteration=300, loss=0.15729584012939785\n",
      "Current iteration=400, loss=0.14270265710658306\n",
      "Current iteration=500, loss=0.14298708683238764\n",
      "Current iteration=600, loss=0.10556115934044442\n",
      "Current iteration=700, loss=0.10730606578847834\n",
      "Current iteration=800, loss=0.14551074619988377\n",
      "Current iteration=900, loss=0.21453318010213285\n",
      "Current iteration=1000, loss=0.11551455058310207\n",
      "Current iteration=1100, loss=0.16175518591596363\n",
      "Current iteration=1200, loss=0.1205090926539504\n",
      "Current iteration=1300, loss=0.12327826509625146\n",
      "Current iteration=1400, loss=0.10193669583300485\n",
      "Current iteration=1500, loss=0.0981311189563209\n",
      "Current iteration=1600, loss=0.10670949634004609\n",
      "Current iteration=1700, loss=0.10871143412603702\n",
      "Current iteration=1800, loss=0.09593688576563207\n",
      "Current iteration=1900, loss=0.09624031797805332\n",
      "Current iteration=2000, loss=0.14465157112595062\n",
      "Current iteration=2100, loss=0.11531081540132122\n",
      "Current iteration=2200, loss=0.12581218995274956\n",
      "Current iteration=2300, loss=0.09500207807191174\n",
      "Current iteration=2400, loss=0.12523628169823547\n",
      "Current iteration=2500, loss=0.12174955822264694\n",
      "Current iteration=2600, loss=0.10160210166529182\n",
      "Current iteration=2700, loss=0.15615198040815262\n",
      "Current iteration=2800, loss=0.18584399019535516\n",
      "Current iteration=2900, loss=0.0972637838642432\n",
      "Current iteration=3000, loss=0.10865722189086531\n",
      "Current iteration=3100, loss=0.11356751220163719\n",
      "Current iteration=3200, loss=0.09288599100999133\n",
      "Current iteration=3300, loss=0.13428916213334555\n",
      "Current iteration=3400, loss=0.19894353148093608\n",
      "Current iteration=3500, loss=0.11667871788847228\n",
      "Current iteration=3600, loss=0.09954323456578638\n",
      "Current iteration=3700, loss=0.13818435891922054\n",
      "Current iteration=3800, loss=0.21580122317832234\n",
      "Current iteration=3900, loss=0.0959180464846572\n",
      "Current iteration=4000, loss=0.08985200046465622\n",
      "Current iteration=4100, loss=0.08982641574038834\n",
      "Current iteration=4200, loss=0.11899354662930639\n",
      "Current iteration=4300, loss=0.09148029602972145\n",
      "Current iteration=4400, loss=0.0958351315616195\n",
      "Current iteration=4500, loss=0.09805403685822828\n",
      "Current iteration=4600, loss=0.08936670407373677\n",
      "Current iteration=4700, loss=0.10812971209843943\n",
      "Current iteration=4800, loss=0.25566428874790276\n",
      "Current iteration=4900, loss=0.09245414306084061\n",
      "weights=[-7.56840847e-01  2.13088766e-01 -1.41284105e+00 -1.61766921e-01\n",
      "  5.21453761e-01  7.59636439e-01  1.41150350e-01 -5.92373135e-01\n",
      "  3.14893809e-01 -7.70022195e-03 -2.74330821e-01 -7.22916502e-01\n",
      "  1.55342814e+00  7.01136737e-01  1.03249140e+00  4.39626916e-01\n",
      "  1.62270978e-01 -7.86730428e-04 -8.92054353e-02  5.89802809e-01\n",
      "  1.81253515e-01 -1.19404676e-01  1.84092815e-01  2.27113752e-01\n",
      "  1.45230550e-01 -1.34219555e-01  8.51293647e-02 -1.88963510e-01\n",
      " -3.93527489e-02 -1.33615058e-01  2.38569442e-01], loss=0.08914888593335403\n",
      "training loss=0.08930297820116996\n",
      "testing loss=0.08823516920672592\n",
      "After stochastic gradient descent: 72.7 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_ws, sgd_losses = stochastic_gradient_descent(y_training, tx_training, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=sgd_ws, l=sgd_losses))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, sgd_ws)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, sgd_ws)))\n",
    "print(\"After stochastic gradient descent: \"+str(round(100*np.sum(predict_labels(sgd_ws, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test = correction_missing_values(tX_test)\n",
    "tX_test, _, _ = standardize(tX_test)\n",
    "tX_test = normalize(tX_test)\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'test_log_regr1.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, tX_test)\n",
    "y_pred[np.where(y_pred == 0)] = -1\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
