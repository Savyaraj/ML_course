{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 30)\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from scikit learn - logistic regression: 73.94 %\n",
      "\n",
      "Accuracy from scikit learn - Support Vector Machine: 100.0 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_logistic = LogisticRegression(random_state=0).fit(tX, y)\n",
    "clf_SVC = SVC(gamma='auto').fit(tX,y)\n",
    "print(\"Accuracy from scikit learn - logistic regression: \"+str(round(100*clf_logistic.score(tX, y),5))+' %\\n')\n",
    "print(\"Accuracy from scikit learn - Support Vector Machine: \"+str(round(100*clf_SVC.score(tX, y),5))+' %\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = correction_missing_values(tX)\n",
    "tX, mean_x, std_x = standardize(tX)\n",
    "tX = normalize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=12069.895477545064, testing loss=2544.2458939989087\n",
      "Current iteration=1000, training loss=2372.1386679665757, testing loss=594.2273101647609\n",
      "Current iteration=2000, training loss=2268.5375690846236, testing loss=572.8773442103042\n",
      "Current iteration=3000, training loss=2211.1289797952068, testing loss=561.4429379299635\n",
      "Current iteration=4000, training loss=2175.264305444743, testing loss=554.5424512868783\n",
      "[-2.15828335  0.55796622 -7.64638796 -0.18189761  2.48732421  2.58449386\n",
      "  2.25846217 -2.63031346  2.37794201 -1.04645436  0.78612107 -4.75297684\n",
      "  4.98373774  2.20945646  4.58372719 -0.19600261 -0.23988488 -0.65749014\n",
      " -1.09974711  0.33786728  0.03712057  0.0395684   1.00966977  0.59898212\n",
      "  0.81702451 -0.23017078 -0.12195788 -1.46115228 -0.18911729 -0.51821403\n",
      "  0.68000168] 2151.06436902832\n",
      "training loss=2151.044144240237\n",
      "testing loss=550.0478687537226\n",
      "Accuracy from our code:\n",
      "\n",
      "Logistic regression: 71.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from logistic_regression import *\n",
    "#Using our code\n",
    "#Logistic regression\n",
    "\n",
    "gamma = 0.0001\n",
    "n_iter = 5000\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w = np.random.rand(tX.shape[1])\n",
    "for i in range(n_iter):\n",
    "    w, loss = learning_by_gradient_descent(y_training, tx_training, w, gamma)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Current iteration={iter}, training loss={l_tr}, testing loss={l_te}\".format(iter=i, l_tr=loss, \n",
    "                                                                                                         l_te=calculate_loss(y_testing, tx_testing, w)))\n",
    "print(w, loss)\n",
    "print(\"training loss={l_tr}\".format(l_tr=calculate_loss(y_training, tx_training, w)))\n",
    "print(\"testing loss={l_te}\".format(l_te=calculate_loss(y_testing, tx_testing, w)))\n",
    "\n",
    "print(\"Accuracy from our code:\\n\")\n",
    "print(\"Logistic regression: \"+str(round(100*np.sum(predict_labels(w, tx_testing, True)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.99629057e+00 -1.01944841e+00  1.84195640e+00 -9.62464318e-02\n",
      " -1.04445767e+00 -1.51976562e+00 -5.95129063e-01 -1.56049638e+00\n",
      " -7.44749655e-01 -5.82832931e-01 -1.30774066e+00 -1.91277301e-01\n",
      " -5.59798614e-01 -4.49127861e-01 -1.70788108e+00 -1.44863517e+00\n",
      " -5.84854340e-01  6.48899793e-01 -2.54098076e-01  2.85980826e-03\n",
      " -2.45973502e+00 -1.82483760e+00 -1.36789082e+00 -2.92494046e+00\n",
      " -7.53231522e-01 -4.97355608e-01  9.41809233e-01 -1.20619970e+00\n",
      " -8.57427921e-01 -9.46745985e-01 -8.09979040e-01 -1.14941032e+00\n",
      "  1.38833505e+00 -9.08308819e-01 -1.47858335e+00 -1.08747132e+00\n",
      " -5.42575581e-01 -6.66006812e-01 -3.39314148e-02 -1.46838705e+00\n",
      " -9.93746812e-01 -3.57126610e-01 -1.03472359e+00 -1.04894929e+00\n",
      " -1.27913841e+00 -5.12177558e-01  1.99397385e-01 -1.39734848e+00\n",
      " -4.68228142e-01  2.54059353e+00  8.50101654e-01 -9.31737326e-01\n",
      " -4.96107642e-01  8.13078635e-02 -1.20471648e+00 -2.82986689e-01\n",
      " -5.16677770e-01 -1.85023744e+00 -5.75296040e-01 -1.82808700e+00\n",
      " -8.60181731e-01 -4.66047852e-02 -1.19532234e-01  1.11097254e+00\n",
      " -1.08381848e+00 -9.67735119e-01 -1.84421216e+00 -4.05881298e-01\n",
      " -1.35577440e+00 -1.71971320e+00 -8.95823313e-02 -1.23585833e+00\n",
      " -1.82858720e+00  6.90118773e-02 -1.00258989e+00  9.22304968e-02\n",
      " -1.06040802e+00 -1.51926120e+00  5.81774272e-01  4.43317426e-01\n",
      "  1.04585942e+00 -1.52949024e+00 -2.66095454e+00 -5.75805518e-01\n",
      " -5.76211060e-01 -3.94448783e-01 -7.71428405e-01 -9.21863042e-01\n",
      " -6.26772736e-01 -1.21310812e+00 -2.02511430e+00  3.56688923e-01\n",
      " -8.46422058e-02 -4.77626162e-02 -1.48051366e+00  3.02088438e+00\n",
      " -1.56919628e+00 -2.76351993e-01 -3.74708770e-01 -7.72305943e-01\n",
      " -1.58891452e+00  3.16242617e-01 -1.25606293e+00 -5.02745728e-01\n",
      " -1.44627598e+00 -1.63344852e+00  5.23992361e-01 -6.07632292e-01\n",
      " -5.45049865e-01 -8.85739612e-01 -1.89120340e+00 -1.01704801e+00\n",
      "  1.61642758e-01  5.07692177e-01 -8.73132359e-01 -5.85544411e-03\n",
      " -1.24399245e+00 -5.48515257e-01 -7.60533263e-01  3.70360062e-01\n",
      " -1.89028728e+00 -3.63016317e-01 -5.97378457e-01 -6.31026164e-01\n",
      " -5.42846176e-01 -1.19504888e+00 -1.65896766e+00 -1.35532969e+00\n",
      " -3.35775402e-01 -5.15137777e-01 -1.67347737e+00 -1.29216979e+00\n",
      " -1.92218812e+00 -1.44904298e+00 -1.08224525e+00 -5.66758254e-01\n",
      " -1.25267362e+00 -6.53469901e-01 -1.35586568e-01 -1.17653328e+00\n",
      " -6.90753813e-01 -1.58314335e+00 -4.31908632e-01 -1.67658659e+00\n",
      " -7.98003506e-01 -1.33505640e+00 -1.31914808e+00 -5.87973329e-01\n",
      " -1.27737789e-02  1.10959003e-01 -7.83205798e-02 -1.27449958e+00\n",
      " -6.63550150e-01 -3.00743344e-01 -3.14660677e-01 -9.52392848e-02\n",
      " -2.02207802e+00  4.10164046e-01 -1.45232134e+00 -1.62612989e+00\n",
      "  1.43038324e-01 -1.97601621e+00 -8.19827736e-01 -2.20833508e+00\n",
      " -1.68046765e+00  5.85659941e-01 -7.02080769e-01 -4.50635376e-01\n",
      "  2.47955397e-01 -5.15692432e-01 -1.93869122e+00 -1.83340467e+00\n",
      " -8.02304020e-01 -6.06571284e-01 -7.30432900e-01 -4.28737120e-01\n",
      " -8.77268161e-01 -4.77022725e-01 -1.58104520e+00 -3.88017581e-01\n",
      "  1.72692337e-01 -7.32627688e-01 -1.64429265e+00 -1.38443086e+00\n",
      " -1.01599778e+00 -2.11194899e+00 -3.48290325e-01 -4.55008869e-01\n",
      " -8.17099249e-01  1.62853942e-01 -9.53729653e-01 -7.97127718e-01\n",
      " -6.99367476e-01 -9.72589367e-01 -5.90755376e-01 -1.52241192e+00\n",
      " -1.13911810e+00 -5.94445828e-01  3.47273934e-01  4.37920281e-01\n",
      " -1.13462998e+00 -9.33917584e-01  1.40555704e-01  4.13148958e-01\n",
      " -4.73576506e-01 -3.60752954e-01 -1.38830854e+00 -1.54541051e+00\n",
      " -1.35361905e+00 -1.11528467e+00 -1.65892520e+00  5.18084333e-02\n",
      " -9.56116758e-02 -8.87314513e-01 -7.54375821e-01 -1.92268483e+00\n",
      "  2.73738319e-02 -1.33208811e+00  1.62795432e+00 -1.54740704e+00\n",
      " -1.51124044e+00 -3.02165149e-01  5.93786300e-01 -1.34415115e+00\n",
      " -1.50074951e+00 -1.52979323e+00 -1.00155235e+00 -7.96460237e-01\n",
      "  1.03978662e-02 -1.10682853e+00 -5.01079804e-01 -2.75324623e-01\n",
      " -1.08124200e+00 -1.20415332e+00 -7.66696308e-01 -5.54748055e-01\n",
      " -1.16839969e+00 -2.82949374e-01 -6.52241795e-01 -5.67421027e-01\n",
      " -8.53531446e-01 -1.47776254e+00 -1.34174740e+00  1.56206456e-01\n",
      " -1.61765538e+00 -1.11898093e+00 -1.34702881e+00 -3.62532050e-01\n",
      " -9.27541322e-01 -1.35916601e+00 -2.25128284e-02  2.91047513e-01\n",
      " -1.79914904e+00 -1.28413559e+00 -4.33993861e-01 -7.49185409e-01\n",
      " -2.92172545e-01 -1.37434540e+00  8.34071203e-01 -1.37734098e+00\n",
      " -1.19129940e-01 -7.62358328e-01 -6.77418167e-01 -1.50776186e+00\n",
      " -9.10588228e-01 -1.63327273e+00  1.97980345e-01 -1.52890542e+00\n",
      " -7.65287657e-01 -6.22377167e-02 -1.31992117e+00  4.64368088e-02\n",
      " -5.10879447e-02  4.27502548e-02  4.90114443e-01  8.15596608e-01\n",
      " -9.32831551e-01 -9.05981059e-02 -3.81965728e-01 -8.94845271e-01\n",
      " -1.23310573e+00 -4.01494304e-01 -9.41341995e-01 -3.00956717e-01\n",
      " -1.15454909e+00  4.78732088e-01 -8.88922893e-01 -9.21532716e-01\n",
      " -2.93782791e-01 -1.26721310e+00  6.93894963e-01 -2.03034314e+00\n",
      " -9.22850910e-01  1.25742507e+00 -1.58701410e+00  1.12414999e+00\n",
      " -5.30662765e-01 -1.07337432e+00  4.92753215e-01 -4.62562220e-01\n",
      "  3.44896270e-01 -1.33572582e+00 -1.73232275e+00 -2.95374896e-01\n",
      " -2.17796625e-01 -1.14352431e+00 -1.83695107e+00 -1.66089703e+00\n",
      "  1.78265443e-01 -1.18784182e+00 -2.13040692e+00  4.18778171e-02\n",
      " -1.77296565e+00 -3.05959196e-01 -9.07503722e-01 -3.07973024e-01\n",
      " -1.36389313e+00 -3.72763996e-01 -1.06301305e+00 -1.47310278e+00\n",
      " -9.28983999e-01  1.89110667e-01 -1.07126405e+00 -8.22783427e-01\n",
      " -2.03399546e+00 -2.53691077e-01 -1.58930278e+00  5.77461661e-02\n",
      "  9.25802248e-01  1.85203118e-01  4.92872716e-01 -1.85210602e+00\n",
      "  1.92153438e+00 -1.34578161e+00  8.72971381e-01 -9.59687418e-01\n",
      "  1.03334417e+00 -1.09900820e+00  8.27586578e-01 -4.07082391e-01\n",
      " -2.14922533e+00 -1.26505613e+00 -2.01507447e+00 -1.19762719e+00\n",
      " -1.10611811e+00 -4.22310615e-01  1.72546431e+00 -1.06318539e+00\n",
      "  2.24173225e-01 -1.41866877e+00 -1.60410839e+00 -1.16354090e+00\n",
      " -1.99040854e-01 -8.84617827e-01 -8.27720418e-01  3.55996947e-01\n",
      " -4.49052234e-01 -4.99232793e-01  6.31750783e-01  1.04340423e+00\n",
      " -2.42868060e-01 -1.78656248e+00  2.01801628e-01 -9.06079131e-01\n",
      " -1.71774276e+00  1.76592601e-01 -1.77547027e+00 -2.58986444e+00\n",
      " -1.86360314e+00 -6.96256017e-01 -1.48841438e+00  7.83929001e-01\n",
      " -1.26241237e+00 -6.62558276e-01  8.61982327e-01 -1.18069040e+00\n",
      " -1.92474380e+00 -4.82046714e-01 -1.14242572e+00  4.64862697e-02\n",
      " -8.21882931e-01 -4.48875639e-01  1.63093613e-01 -1.15788458e-01\n",
      " -1.29639216e+00  2.96373734e-01 -5.54669770e-01 -2.06324755e+00\n",
      " -1.52405339e+00 -6.36083288e-01 -1.37356429e+00 -4.96095239e-01\n",
      " -2.58044591e-01 -1.13318836e+00  1.30647617e-01 -8.59329368e-02\n",
      " -1.68606692e+00 -1.40201867e+00 -9.94990725e-01 -2.58413787e-01\n",
      "  3.34721980e-01 -1.36850723e+00 -9.72049134e-01 -4.96204338e-01\n",
      "  2.06964080e+00 -8.36837509e-01 -1.54003475e+00 -3.14277803e-01\n",
      " -5.16035910e-01 -1.27898940e+00 -1.33703783e+00 -9.83924021e-01\n",
      " -2.21536345e+00 -3.94533672e-01 -1.03623520e+00 -3.21451892e-01\n",
      " -1.36793960e+00 -1.20543493e+00 -1.06357916e+00 -3.15630673e-01\n",
      " -6.19361415e-02 -1.34937169e+00 -1.56790788e+00 -1.22835584e+00\n",
      " -1.12578425e+00 -1.02401833e+00 -1.14107739e+00 -7.97138255e-01\n",
      "  1.77557067e+00 -1.03598984e+00  1.55493508e-01  2.90335444e-02\n",
      " -1.30654821e+00 -1.50212416e+00 -2.15580465e-01  1.13690553e-01\n",
      "  9.36451213e-01 -2.56797876e-01 -1.05384709e+00  5.50008829e-01\n",
      " -1.49020613e+00 -2.41988892e-01 -1.16999656e+00  2.27629876e-01\n",
      " -9.46662031e-01 -9.17176728e-02 -4.02841018e-01 -7.66470717e-01\n",
      " -1.34554226e+00 -1.93693394e+00  5.30073108e-01 -1.66150521e+00\n",
      " -9.97061912e-01 -1.01040276e+00  3.36944247e-01 -2.35036139e+00\n",
      " -1.27030542e+00 -1.67579025e+00 -9.32447072e-01 -1.04579838e+00\n",
      " -1.33866506e+00 -1.23550512e+00 -5.74254932e-01 -1.36904484e+00\n",
      " -1.52579693e-02  2.66812565e+00  2.95400933e-01 -7.81271454e-01\n",
      " -1.53823246e+00 -1.72040518e+00 -5.46427961e-01 -5.79612527e-01\n",
      "  1.37447144e+00 -6.08601383e-02  1.78845916e-02 -2.71426617e-01\n",
      " -1.13206851e+00 -1.29945096e+00 -3.40051359e-01 -1.60839972e+00\n",
      " -8.45664473e-01 -2.01423996e+00 -9.76856788e-01 -1.99210870e+00\n",
      " -4.37033594e-01 -1.07630772e+00 -2.11707513e+00 -7.10606123e-01\n",
      " -3.22261973e+00  1.42967839e-03 -8.43282392e-01 -1.64081713e+00\n",
      " -1.88496822e+00  3.52915325e-02 -3.87081353e-01 -1.20843684e+00\n",
      " -3.53015841e-01 -9.23384442e-01 -1.36099685e+00 -1.25324268e+00\n",
      "  8.57169409e-04 -4.55586560e-01  3.00350263e-01 -1.24541635e+00\n",
      " -1.18410992e+00 -5.97298020e-01 -1.05892538e+00  1.95027137e-01\n",
      " -1.11467081e+00  7.78011511e-01 -1.28948087e+00 -1.61610609e+00\n",
      " -1.38360234e+00 -3.36524226e+00 -1.79702958e+00 -6.39991444e-02\n",
      " -6.98622588e-01 -1.61965062e+00 -1.79208076e-01  9.03377715e-01\n",
      " -1.44963946e+00 -8.70778603e-01 -2.03456541e-01 -7.39898465e-01\n",
      " -1.79791729e+00 -1.71734136e+00 -1.31482200e+00  2.25032992e-01\n",
      "  1.99955993e-02 -1.07511494e+00 -1.43910822e+00 -2.33756944e-01\n",
      " -1.78121077e-01  1.77813877e+00 -4.28478653e-01 -1.37541550e+00\n",
      "  2.08458266e-01  2.77885231e-01 -9.90921334e-01 -8.39442802e-01\n",
      " -2.05984439e+00 -2.13651463e+00 -1.87017635e+00 -1.16232038e+00\n",
      " -4.71210797e-01 -3.14902667e-01  6.51049579e-01 -1.30730392e+00\n",
      " -6.47778574e-01 -1.53727555e+00 -1.82275815e+00 -3.39717855e-02\n",
      " -1.62689172e+00 -6.13756455e-01  1.19959101e+00 -1.19684778e+00\n",
      " -1.81084614e+00 -1.03551904e-01 -1.15628028e+00  4.35783013e-02\n",
      " -1.48314314e+00 -1.74786707e-01  4.15143847e-01  9.57141565e-01\n",
      " -3.93062123e-01 -1.21788508e+00 -1.04087767e+00 -4.74278504e-01\n",
      " -6.80409953e-01 -3.77286163e-01 -4.38852272e-01 -1.29004682e+00\n",
      " -6.66576906e-01 -4.62371116e-02 -1.29801649e+00 -1.50285565e+00\n",
      " -1.00660996e-01 -1.70508354e+00  5.67579136e-01  9.20669049e-01\n",
      " -2.57637848e-01 -1.45754295e+00 -1.48030295e+00 -1.13485436e-01\n",
      " -1.58516410e+00 -3.35431210e-01 -3.34962512e-01 -7.20919299e-01\n",
      " -9.06174882e-01  2.89083697e-01 -8.82127550e-01  1.30976142e-01\n",
      " -2.07693618e+00 -1.52490873e+00 -1.44383001e+00 -2.56897956e-01\n",
      " -1.48296136e+00 -8.10241774e-01 -6.98751914e-01 -6.90041345e-01\n",
      "  1.59006430e+00 -3.40607637e-01 -2.30357311e+00 -1.34544921e+00\n",
      "  1.45100318e-01 -1.62569551e+00 -1.91359789e+00 -4.60936714e-01\n",
      " -4.59043663e-02 -6.08515591e-01 -2.35173746e+00 -2.93496728e-01\n",
      "  1.35394864e+00 -2.54481277e-01 -2.24257496e+00 -1.34469969e-01\n",
      " -1.54257982e+00 -5.74757864e-01 -1.30160827e+00 -1.21840615e+00\n",
      " -1.75259746e+00 -2.13573189e-02 -4.16590920e-01 -5.44777355e-01\n",
      " -5.67160937e-01 -9.86705511e-01 -2.42334257e+00 -9.06003114e-01\n",
      "  6.70769139e-01 -1.76064838e+00  5.31763472e-02 -1.29095029e+00\n",
      " -2.13705159e+00 -1.30885803e+00 -1.11827227e+00 -7.30971077e-01\n",
      " -1.62274492e+00 -2.05104414e+00 -1.84281096e+00 -1.07848822e-01\n",
      "  4.67769895e-01  4.46565508e-01  5.86419762e-01 -1.34558836e+00\n",
      " -7.53764202e-01  5.71447244e-01 -5.48684888e-01 -1.30393911e+00\n",
      " -8.75564724e-01  5.24320881e-01 -1.71521811e+00 -9.94216227e-01\n",
      " -2.12506986e+00 -1.15223284e+00  3.55837557e-01 -7.07043000e-01\n",
      " -2.71797936e-01 -1.60912845e+00  4.99514347e-02 -2.22502106e-01\n",
      "  5.54533826e-01 -1.64128554e-01 -1.53913201e+00 -4.04339021e-01\n",
      "  6.47681364e-01 -1.74095137e+00  9.02790223e-01 -2.05329277e+00\n",
      " -1.50054755e+00 -1.73573569e+00 -1.05558229e+00 -1.39387313e+00\n",
      "  6.46532301e-02  1.64023803e+00 -2.34537817e-01 -7.75815114e-01\n",
      " -9.92255608e-01 -9.72536536e-01 -1.04739270e+00 -1.52994745e+00\n",
      " -3.06009692e-01 -8.19670652e-01 -1.62873907e+00 -1.33477448e+00\n",
      " -1.03170329e+00 -1.38208129e+00 -1.21864718e+00 -8.33316239e-01\n",
      " -1.17367291e-01 -1.80653907e+00 -1.45216257e+00 -1.28866587e-01\n",
      " -1.13187841e+00 -5.99895600e-01 -1.03795120e-01 -6.25666746e-01\n",
      " -1.40823490e+00 -1.83462655e+00  7.09099098e-01 -2.84473819e-01\n",
      " -3.34494149e-01 -1.60060173e+00 -4.10488413e-01 -9.33995255e-01\n",
      "  8.26543533e-01 -1.45157778e+00 -8.59817544e-01 -1.20737791e+00\n",
      " -2.25811351e+00 -5.81573076e-01  1.22270167e-01 -1.98016497e+00\n",
      " -1.76086017e+00 -1.39429029e+00 -2.61111401e-01  1.96674099e-01\n",
      "  1.47548934e+00  7.92906062e-01 -1.02638438e+00 -1.60469470e+00\n",
      " -1.54452457e+00 -8.22235066e-01  5.19613286e-01 -8.43236219e-01\n",
      " -1.22371724e+00  3.35660197e-01 -3.84668530e-01 -1.49227370e+00\n",
      "  2.45913603e-01 -2.33401949e-01 -5.16257567e-02  6.26230594e-01\n",
      " -1.37901663e+00  1.07988221e-01 -2.25167417e+00 -1.03958739e+00\n",
      "  1.58514742e-01 -7.95713003e-01 -1.90049656e+00  4.11782720e-01\n",
      " -3.63346665e-01 -7.56421963e-01 -5.96603248e-02  2.57892343e-01\n",
      " -3.59188629e-01 -1.80399835e+00 -6.46540930e-01 -9.54178771e-01\n",
      " -1.15161668e+00 -1.38187038e+00 -1.40169917e+00 -1.11558577e+00\n",
      " -9.69389897e-02 -1.19104387e+00 -1.32867983e+00 -1.51038938e+00\n",
      " -6.13094919e-01 -6.90118019e-01 -1.46287875e+00 -1.03634617e+00\n",
      "  1.17104707e+00 -1.32406103e+00 -5.04908205e-01 -1.33633487e+00\n",
      " -1.16399315e+00 -6.32485135e-01 -6.03565990e-01 -6.61778348e-01\n",
      " -1.10940388e-01 -1.77524319e+00 -2.78673583e-01  3.12032002e-02\n",
      "  4.10532294e-02 -1.46298180e-01  2.09681386e-02 -1.34897567e+00\n",
      " -8.21408764e-01  4.66059819e-02 -1.55209138e-01 -8.00662359e-01\n",
      " -2.10519830e-01  4.07907177e-01  1.10063828e+00 -8.96942219e-01\n",
      " -7.22587024e-01 -8.00542273e-01 -2.59420122e+00  9.09165325e-01\n",
      " -2.83147665e+00 -1.34749231e+00 -1.33790012e+00 -1.12269260e+00\n",
      " -2.21572849e-01 -4.92439981e-01 -3.98695484e-01 -8.29427342e-02\n",
      " -9.85718846e-01 -1.36101832e+00 -9.84663112e-01 -8.82337865e-01\n",
      " -1.15745367e+00 -1.27986458e+00 -2.74537222e-01 -1.61023003e+00\n",
      " -3.48987875e-01 -1.07217957e+00 -1.10642532e+00  3.05968984e-03\n",
      " -1.19401493e+00 -4.26345282e-01 -8.02277193e-01 -1.35487177e+00\n",
      " -8.09038796e-01 -1.55586332e-01 -3.21735575e-01  1.26162065e+00\n",
      " -9.10473954e-01 -1.56221308e+00 -8.15611279e-01 -1.03171736e+00\n",
      " -1.44578235e+00 -1.31164167e+00 -5.44052527e-01 -1.65701699e+00\n",
      "  7.61529790e-02 -1.46694235e+00 -1.13844814e+00 -9.35189958e-02\n",
      " -7.44501765e-01 -7.45929711e-01 -1.33083718e+00  3.69232182e-01\n",
      "  7.48820659e-01 -5.86887644e-01 -4.79056316e-01 -3.73342863e-01\n",
      "  1.13435743e-01  6.73977890e-01 -3.28828295e-01 -1.57872417e+00\n",
      "  2.39471790e-01 -1.09262809e+00 -4.71171211e-01 -3.45746495e-01\n",
      " -1.22171699e+00 -5.73490874e-01 -1.13783613e+00  1.47245632e-01\n",
      " -3.72761302e-01 -1.32299201e+00 -1.94098615e+00 -1.21287470e+00\n",
      " -1.79384077e+00 -4.18331462e-01 -1.14910913e+00 -1.78489805e+00\n",
      " -1.05853611e+00 -7.98494363e-01 -1.71638369e+00 -1.97492493e+00\n",
      " -9.70710204e-01 -1.04910850e+00 -8.90875611e-01 -8.90440051e-01\n",
      " -2.67256033e-01 -3.25805015e-01 -7.06189039e-01 -1.28648641e+00\n",
      " -2.08469240e+00 -1.65045464e+00  8.93176897e-01  1.12339094e-01\n",
      " -1.25317701e+00 -2.62756380e-01 -1.74466093e+00  1.63055374e+00\n",
      "  7.71755369e-01 -5.75527916e-01  1.04743697e+00  5.03161638e-01\n",
      " -6.85260306e-01 -9.08810651e-01 -2.06038509e+00 -1.96881701e+00\n",
      " -2.08970665e+00  8.54284601e-01 -1.07342201e+00 -9.90050091e-01\n",
      " -1.27581533e+00 -1.04555193e+00 -1.98651419e+00 -9.55859741e-01\n",
      " -1.97474650e+00  2.67358684e+00  4.99444609e-01 -1.98034309e+00\n",
      " -5.16289996e-01 -1.75595387e-01 -4.53801602e-01 -5.31882179e-01\n",
      " -1.34867029e+00 -6.44279804e-01 -3.94812251e-01 -5.38782238e-01\n",
      " -1.08849409e+00 -1.31418535e+00 -5.76187643e-01  4.15298111e-01\n",
      " -3.68092531e-01 -1.03152076e+00 -1.15686519e+00 -1.61279599e+00\n",
      "  8.85904465e-01 -4.07916907e-01 -1.94147855e-01  3.46663224e-01\n",
      " -3.49068721e-01 -9.61666718e-01 -7.55872074e-01 -9.80089933e-01\n",
      " -1.43485472e+00 -4.98197314e-01 -7.13376451e-01 -1.13395079e+00\n",
      " -1.27484515e+00 -9.72714164e-01 -4.09198372e-02 -2.65912164e+00\n",
      " -1.02003415e+00 -5.75805518e-01 -8.71497510e-01 -5.61678133e-02\n",
      " -7.39062950e-01 -1.04153323e+00 -6.77831184e-01 -6.94586394e-01\n",
      " -6.12475882e-01 -5.38668596e-01 -1.00950019e+00 -7.03884019e-01\n",
      " -5.47869464e-01 -1.80294333e+00 -1.05888374e+00  1.13958956e+00\n",
      " -1.56729894e+00 -9.74668139e-01 -9.50497801e-01 -2.59183187e-01\n",
      " -1.82926431e+00 -7.63601913e-01 -1.51039260e+00 -2.49364737e-01\n",
      " -1.24389871e-01 -1.37724783e+00 -1.09219548e+00 -1.09834481e+00\n",
      " -4.43087440e-01 -1.17435293e+00 -1.15188755e+00 -1.29049416e+00\n",
      " -1.59507597e-01 -8.87691473e-01 -1.34239891e+00 -1.65659399e-01\n",
      " -1.65218811e+00 -3.82025836e-01  1.28399096e+00 -2.58150323e-01\n",
      " -1.88575689e+00 -7.91720655e-01 -2.57150917e-01 -7.84399081e-01\n",
      " -7.64650877e-03  1.41063461e-01 -1.41667094e+00 -5.58052558e-01\n",
      " -7.89860333e-01 -7.99939854e-01 -2.34356015e-01 -1.07073025e+00\n",
      " -1.38120214e+00 -2.10501925e-01 -5.91525876e-01 -1.74696603e+00\n",
      " -5.05692493e-01 -2.22841992e-01 -9.85393292e-01 -9.16910217e-01\n",
      " -1.66104870e-01 -1.97136014e-01 -9.42104601e-01 -1.04743363e+00\n",
      "  1.69760650e+00  9.17265914e-01 -3.23922371e-01 -1.36739687e-01\n",
      "  6.45705862e-02 -9.27453586e-01 -1.16973956e+00 -9.09281092e-01]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(tx_testing, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tx_testing, True)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=13720.06492500284, testing loss=2785.304007421693\n",
      "Current iteration=1000, training loss=2362.319772141506, testing loss=589.8460042721323\n",
      "Current iteration=2000, training loss=2269.0212763233167, testing loss=570.3903531978822\n",
      "Current iteration=3000, training loss=2220.283298889599, testing loss=560.1655245515751\n",
      "Current iteration=4000, training loss=2191.1695427790455, testing loss=553.9913390646244\n",
      "[-1.5366772  -1.10426569  0.455911   -7.36998048 -0.2791438   2.67240298\n",
      "  2.35101208  2.22472092 -2.52860406  2.7727881  -1.26313471  1.65968718\n",
      " -4.22428687  4.42097021  2.28735284  4.71263942 -0.46310806 -0.07154646\n",
      " -1.13498138 -0.55259006  0.28062755 -0.12360441  0.32180645  0.93693649\n",
      "  0.92944392  0.77488873 -0.01933643  0.02257525 -1.57102133 -0.11061072\n",
      " -0.21903541  0.27578281] 2172.316646587313\n",
      "training loss=2154.8688633676047\n",
      "testing loss=549.9402074982319\n",
      "After regularization: 71.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from reg_logistic_regression import *\n",
    "#Reguralized logistic regression\n",
    "\n",
    "gamma = 0.0001\n",
    "lamda_ = 0.105\n",
    "n_iter = 5000\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w_reg = np.random.rand(tX.shape[1])\n",
    "for i in range(n_iter):\n",
    "    w_reg, loss = learning_by_penalized_gradient(y_training, tx_training, w_reg, gamma, lamda_)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Current iteration={iter}, training loss={l_tr}, testing loss={l_te}\".format(iter=i, l_tr=loss, \n",
    "                                                                                                         l_te=calculate_loss(y_testing, tx_testing, w_reg)))\n",
    "print(w_reg, loss)\n",
    "print(\"training loss={l_tr}\".format(l_tr=calculate_loss(y_training, tx_training, w_reg)))\n",
    "print(\"testing loss={l_te}\".format(l_te=calculate_loss(y_testing, tx_testing, w_reg)))\n",
    "\n",
    "print(\"After regularization: \"+str(round(100*np.sum(predict_labels(w_reg, tx_testing, True)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.98837391 -1.02012168  1.80298439 -0.19836248 -1.05487973 -1.53350173\n",
      " -0.61267    -1.56109411 -0.77104332 -0.58236695 -1.18617932 -0.20911353\n",
      " -0.55723694 -0.49741633 -1.65789815 -1.43591709 -0.56799873  0.62856248\n",
      " -0.3026253  -0.02626168 -2.39061991 -1.76349525 -1.36995493 -2.90925297\n",
      " -0.74966663 -0.45291054  0.94601049 -1.17133601 -0.83549857 -0.90332201\n",
      " -0.80985258 -1.03842364  1.33157583 -0.90123775 -1.41497705 -1.12443734\n",
      " -0.54007471 -0.71474392 -0.05457188 -1.41019214 -0.96276277 -0.40897977\n",
      " -1.07759123 -1.0034532  -1.26545281 -0.57813602  0.13786478 -1.45693195\n",
      " -0.46571041  2.44406012  0.77123727 -0.93185304 -0.41279985  0.10322298\n",
      " -1.16136967 -0.27574748 -0.53622017 -1.72035504 -0.58287386 -1.79889454\n",
      " -0.84684605 -0.05005943 -0.1782027   1.12900973 -1.08332525 -0.96412704\n",
      " -1.76925136 -0.41936174 -1.25258516 -1.70526865 -0.06863796 -1.2461983\n",
      " -1.77305501  0.03223931 -1.04571266  0.05868362 -1.0842864  -1.48520988\n",
      "  0.54621272  0.41132333  1.00736361 -1.45355869 -2.42082191 -0.5512214\n",
      " -0.55500816 -0.41331257 -0.75955228 -0.92785099 -0.59185249 -1.22242495\n",
      " -2.01744332  0.33725886 -0.11326392 -0.05480155 -1.46513458  3.06611157\n",
      " -1.54299652 -0.29976304 -0.36509534 -0.70896647 -1.54865398  0.29457383\n",
      " -1.23410791 -0.52826184 -1.45166615 -1.54758873  0.47763206 -0.63024786\n",
      " -0.52420157 -0.84109856 -1.83144219 -0.97184379  0.13332601  0.48513586\n",
      " -0.87185769 -0.05777648 -1.20907137 -0.53691876 -0.76858118  0.32318153\n",
      " -1.84128945 -0.34292416 -0.60444189 -0.62774508 -0.51634014 -1.10478393\n",
      " -1.6562826  -1.25452103 -0.31540334 -0.50781352 -1.6554005  -1.33756406\n",
      " -1.92881057 -1.43357546 -1.08939583 -0.59192958 -1.2614463  -0.65445526\n",
      " -0.18341794 -1.20091318 -0.66074113 -1.55202275 -0.38344447 -1.65187757\n",
      " -0.76898573 -1.33370904 -1.27584097 -0.56293286 -0.01459835  0.07980996\n",
      " -0.1359419  -1.26708713 -0.66178154 -0.33261326 -0.313781   -0.10814116\n",
      " -2.01578939  0.41800218 -1.45395217 -1.64098722  0.09582106 -1.90104781\n",
      " -0.75439055 -2.15699269 -1.72179564  0.55268541 -0.66172546 -0.4657422\n",
      "  0.21472072 -0.54580912 -1.93829273 -1.80883345 -0.82517676 -0.5507256\n",
      " -0.71055806 -0.40875621 -0.82314909 -0.53436092 -1.5949615  -0.3969892\n",
      "  0.12479752 -0.71046762 -1.61964529 -1.37368184 -0.99485371 -2.07188071\n",
      " -0.3671714  -0.41646289 -0.85732042  0.17663841 -0.9178992  -0.80479525\n",
      " -0.70714736 -0.93961643 -0.6567818  -1.51433359 -1.09264684 -0.5709861\n",
      "  0.30634534  0.40946358 -1.12382063 -0.86180659  0.10924577  0.35614442\n",
      " -0.45013167 -0.35957248 -1.33237131 -1.46350346 -1.28706852 -1.08481895\n",
      " -1.6102005   0.05325739 -0.11085972 -0.91529618 -0.6727377  -1.89346033\n",
      "  0.09852019 -1.33324558  1.60010528 -1.55467195 -1.51884832 -0.2885709\n",
      "  0.60874511 -1.38311677 -1.48809918 -1.50219949 -0.94264824 -0.76897639\n",
      " -0.02978384 -1.10265989 -0.52262435 -0.31944527 -1.04425906 -1.16409967\n",
      " -0.77147387 -0.5542175  -1.17621037 -0.28130511 -0.60208277 -0.62197243\n",
      " -0.84620451 -1.52390246 -1.3414385   0.14890526 -1.57684862 -1.15019284\n",
      " -1.37131891 -0.34207555 -0.9455586  -1.36148041 -0.05620985  0.3094999\n",
      " -1.68013197 -1.2731866  -0.50291931 -0.76828221 -0.34681311 -1.40229429\n",
      "  0.76833823 -1.30785526 -0.15085112 -0.73797236 -0.68161378 -1.4942758\n",
      " -0.91758306 -1.55987588  0.10577777 -1.50798214 -0.74754661 -0.05344935\n",
      " -1.30157991 -0.00435725 -0.07953654  0.06729677  0.48530871  0.76751913\n",
      " -0.91469984 -0.08851003 -0.40081875 -0.82569449 -1.14859288 -0.4182255\n",
      " -0.87298666 -0.34347083 -1.1803669   0.42099786 -0.88368855 -0.90173806\n",
      " -0.32007869 -1.16770146  0.72816516 -1.9953511  -0.888982    1.22966528\n",
      " -1.4722291   1.08466852 -0.53643516 -1.00917168  0.4708589  -0.53173485\n",
      "  0.32902246 -1.34270896 -1.68446164 -0.26904147 -0.26987327 -1.08438185\n",
      " -1.73409777 -1.64578242  0.15136876 -1.14181332 -2.11156673 -0.00906777\n",
      " -1.72966776 -0.30026577 -0.86014516 -0.36582016 -1.30318117 -0.40591683\n",
      " -1.12024716 -1.43991793 -0.90618698  0.14887018 -1.05998111 -0.8033403\n",
      " -1.92057214 -0.28845553 -1.48691672 -0.00364288  0.94506442  0.13629939\n",
      "  0.47825842 -1.79938841  1.9032535  -1.35494364  0.7787573  -0.88171878\n",
      "  1.00185417 -1.09131839  0.84767208 -0.48685202 -2.05578102 -1.2464249\n",
      " -1.93184558 -1.15532058 -1.13317778 -0.37082976  1.6390433  -1.01740632\n",
      "  0.26848823 -1.40088534 -1.60591253 -1.14690492 -0.20181085 -0.86167064\n",
      " -0.78172628  0.26584484 -0.47601496 -0.47294094  0.56713376  0.99364706\n",
      " -0.217385   -1.68308219  0.18842878 -0.9324848  -1.70932807  0.14608072\n",
      " -1.73006207 -2.59273806 -1.80555838 -0.70366119 -1.47200579  0.7656043\n",
      " -1.31901306 -0.63590271  0.80612622 -1.20859228 -1.92409886 -0.46110976\n",
      " -1.13506321 -0.00685847 -0.85214117 -0.41517272  0.21109597 -0.16745728\n",
      " -1.21280383  0.32816079 -0.59970132 -2.00264784 -1.49960853 -0.65998263\n",
      " -1.28023578 -0.51949369 -0.2968474  -1.10310721  0.09115076 -0.14743115\n",
      " -1.67724002 -1.38860374 -0.91399758 -0.26289     0.30343493 -1.25586818\n",
      " -0.95408325 -0.56626747  1.99316425 -0.85996942 -1.55117518 -0.39264341\n",
      " -0.49722157 -1.27899268 -1.28905416 -0.93500092 -2.13658313 -0.35365151\n",
      " -1.01594623 -0.32613462 -1.33512738 -1.19845859 -1.08495429 -0.38310585\n",
      " -0.13032267 -1.38271784 -1.6131547  -1.17328195 -1.10728566 -0.98342491\n",
      " -1.12961409 -0.82251179  1.75349934 -1.02485081  0.08808957 -0.00959016\n",
      " -1.28345571 -1.4242871  -0.18299926  0.0718139   0.83528433 -0.24036439\n",
      " -1.0875867   0.52421407 -1.40530627 -0.26118394 -1.17129647  0.1777099\n",
      " -0.91840862 -0.13535586 -0.43162903 -0.68235991 -1.30632689 -1.92834778\n",
      "  0.56306328 -1.6376636  -0.95004437 -0.98358182  0.32820716 -2.29556405\n",
      " -1.27194022 -1.60262499 -0.85314274 -0.99138194 -1.35683431 -1.23845257\n",
      " -0.61254398 -1.33583577 -0.03851843  2.55641799  0.23158031 -0.79511858\n",
      " -1.52607581 -1.69091582 -0.56221385 -0.57466359  1.34448066 -0.0966755\n",
      " -0.01722036 -0.32467753 -1.11542218 -1.31252213 -0.34978933 -1.58372302\n",
      " -0.80770863 -1.99288859 -0.96800001 -2.01009076 -0.37841997 -1.0961575\n",
      " -2.06169989 -0.72418216 -3.1320728  -0.02328991 -0.75557703 -1.60437118\n",
      " -1.85456034  0.06418518 -0.36450295 -1.16577641 -0.36031047 -0.98018952\n",
      " -1.33757927 -1.19545245 -0.05292235 -0.43469146  0.3312657  -1.25106374\n",
      " -1.12650003 -0.57602814 -0.93685234  0.16852766 -1.06080607  0.74659328\n",
      " -1.27171108 -1.65093873 -1.34553841 -3.37934939 -1.73594642 -0.10080176\n",
      " -0.68512687 -1.61066705 -0.21153784  0.88786666 -1.44695771 -0.83182559\n",
      " -0.21839084 -0.77217094 -1.77723792 -1.69761918 -1.28589278  0.17459789\n",
      " -0.04471418 -1.04266924 -1.37812629 -0.28979629 -0.14078695  1.72537069\n",
      " -0.46234091 -1.35053707  0.18598215  0.29748526 -1.00098703 -0.81952677\n",
      " -1.98341619 -2.09409766 -1.76018229 -1.0863012  -0.48264114 -0.32379952\n",
      "  0.63308093 -1.34418984 -0.64495191 -1.51559968 -1.78179276 -0.10087526\n",
      " -1.61556572 -0.60686476  1.20880126 -1.20537374 -1.80588964 -0.13513965\n",
      " -1.13527987  0.04221428 -1.482338   -0.18687085  0.39695153  0.92784105\n",
      " -0.37179788 -1.24809338 -1.07914657 -0.49813566 -0.73258708 -0.42655517\n",
      " -0.45384427 -1.2315712  -0.63014325 -0.12118173 -1.27750975 -1.51370567\n",
      " -0.12414352 -1.67194372  0.5645799   0.80765912 -0.28256241 -1.44698927\n",
      " -1.45663061 -0.155721   -1.49198428 -0.37109511 -0.38165664 -0.7105083\n",
      " -0.87400142  0.29015382 -0.90564387  0.17948473 -1.96182995 -1.45179602\n",
      " -1.40620981 -0.25778459 -1.44172615 -0.81869669 -0.68235274 -0.67069794\n",
      "  1.60866785 -0.3151985  -2.16568495 -1.29333104  0.06940378 -1.59247611\n",
      " -1.88899273 -0.5097124  -0.1308568  -0.54840393 -2.3098674  -0.29622412\n",
      "  1.29142208 -0.3102726  -2.17429778 -0.17149551 -1.51446712 -0.60532202\n",
      " -1.30526813 -1.20002264 -1.70075081 -0.02774806 -0.49031543 -0.55142962\n",
      " -0.60877758 -1.00867848 -2.40941881 -0.89643796  0.63394397 -1.66590858\n",
      "  0.0498223  -1.19922438 -2.05329743 -1.30287746 -1.1197329  -0.71202112\n",
      " -1.56060531 -2.02701062 -1.78500283 -0.12636922  0.38669858  0.46899216\n",
      "  0.68269845 -1.35418317 -0.74418326  0.63932378 -0.53727336 -1.31955755\n",
      " -0.8854092   0.40432129 -1.68659277 -0.93442958 -2.025336   -1.12841689\n",
      "  0.39478705 -0.67466192 -0.28472815 -1.57902198 -0.02199189 -0.20071101\n",
      "  0.56014126 -0.21981084 -1.50087379 -0.36999145  0.55518988 -1.67295734\n",
      "  0.82823494 -2.00392948 -1.46624649 -1.65973926 -0.99028687 -1.39207637\n",
      "  0.01956008  1.61381615 -0.24834387 -0.7674556  -0.94106573 -0.98502105\n",
      " -1.07965051 -1.56335505 -0.2740506  -0.80714004 -1.595214   -1.34447658\n",
      " -1.05549116 -1.36104864 -1.11576061 -0.79387697 -0.14221855 -1.8491439\n",
      " -1.38052013 -0.16099912 -1.17331628 -0.57585867 -0.06109842 -0.6748054\n",
      " -1.3847837  -1.83604061  0.67694964 -0.37856176 -0.33983987 -1.46318188\n",
      " -0.4672579  -0.92698725  0.82351643 -1.39449818 -0.82894571 -1.21830248\n",
      " -2.19189639 -0.58063453  0.07176149 -1.81265999 -1.75359488 -1.36810482\n",
      " -0.28447113  0.23522546  1.42283757  0.75971216 -1.04509158 -1.50357821\n",
      " -1.46770155 -0.83117917  0.50357047 -0.83986899 -1.24638287  0.34164125\n",
      " -0.4161225  -1.48157444  0.21554453 -0.22737209 -0.10429969  0.57024841\n",
      " -1.36635331  0.06521329 -2.16509701 -1.02383898  0.08126966 -0.86630776\n",
      " -1.82777374  0.46725708 -0.37257094 -0.78184783 -0.08820271  0.31948521\n",
      " -0.36308835 -1.77447746 -0.63243929 -0.98172835 -1.06620265 -1.40224047\n",
      " -1.4235735  -1.12992653 -0.11849093 -1.20823443 -1.31538224 -1.49887309\n",
      " -0.62865463 -0.68842232 -1.4575617  -1.04665083  1.10870804 -1.26250971\n",
      " -0.49954405 -1.36037636 -1.08516759 -0.63785226 -0.61128043 -0.72452625\n",
      " -0.16967722 -1.74519276 -0.32901371  0.05117803 -0.05448229 -0.18050238\n",
      "  0.01349471 -1.31658508 -0.81729965  0.02368849 -0.16888103 -0.76308533\n",
      " -0.20774754  0.40812719  1.11165324 -0.8516882  -0.73465031 -0.86428675\n",
      " -2.55129417  0.88179581 -2.68644053 -1.38671206 -1.30003156 -1.15082345\n",
      " -0.2218233  -0.56768301 -0.4658403  -0.15579539 -0.98533742 -1.37659756\n",
      " -0.99786173 -0.87469235 -1.18480857 -1.24115441 -0.1667963  -1.57488369\n",
      " -0.35511238 -1.06437202 -1.08819785 -0.05057029 -1.18931591 -0.45126288\n",
      " -0.66435068 -1.33448819 -0.79139577 -0.16211095 -0.27694468  1.16868448\n",
      " -0.95204032 -1.45985377 -0.81101471 -1.03011057 -1.44545294 -1.32194376\n",
      " -0.55587973 -1.63310198  0.11141409 -1.45178239 -1.12727247 -0.15613072\n",
      " -0.621621   -0.8012375  -1.34100948  0.31390623  0.72551095 -0.58453593\n",
      " -0.42988793 -0.43011736  0.13692646  0.65626001 -0.33666294 -1.53495315\n",
      "  0.14041901 -1.10480334 -0.43327181 -0.29446924 -1.15925344 -0.61864279\n",
      " -1.09777415  0.12764349 -0.3238797  -1.32917441 -1.87067228 -1.21122198\n",
      " -1.77006642 -0.4406549  -1.1392331  -1.78137017 -1.04849461 -0.85468981\n",
      " -1.73439806 -1.88799864 -0.96608721 -1.07658826 -0.90101294 -0.93091919\n",
      " -0.31460226 -0.30900741 -0.6771767  -1.26760237 -2.04029476 -1.6168211\n",
      "  0.79375482  0.05631234 -1.21012878 -0.30234346 -1.71562893  1.67168264\n",
      "  0.72353447 -0.64242511  0.93337985  0.53323094 -0.64592818 -0.85560421\n",
      " -1.99270071 -1.89311425 -2.07322917  0.88483688 -1.068086   -0.9644963\n",
      " -1.22453914 -1.01833573 -1.82157783 -0.89398761 -1.96061186  2.60162403\n",
      "  0.46420539 -1.96805366 -0.54720718 -0.17424986 -0.48386897 -0.48686133\n",
      " -1.32633557 -0.6719673  -0.42646163 -0.50897486 -1.10572705 -1.2365297\n",
      " -0.54925738  0.40334039 -0.39046032 -1.02028323 -1.13702002 -1.60144058\n",
      "  0.89662789 -0.43011748 -0.27156149  0.40020457 -0.31949218 -1.00473812\n",
      " -0.71572679 -0.99059937 -1.39044567 -0.52991382 -0.71769353 -1.14290222\n",
      " -1.21228852 -0.89101078 -0.07047796 -2.51838222 -0.94897238 -0.5512214\n",
      " -0.91940312 -0.10037375 -0.76295902 -1.05893565 -0.7077499  -0.70348001\n",
      " -0.64780199 -0.48937398 -0.99581912 -0.72492459 -0.58089115 -1.81511755\n",
      " -1.02158522  1.12309021 -1.59010014 -0.92849039 -0.96716604 -0.27714294\n",
      " -1.78866106 -0.79091054 -1.46586149 -0.25461196 -0.14275589 -1.34981257\n",
      " -1.04861466 -1.09499948 -0.5000044  -1.14384745 -1.06794747 -1.24405236\n",
      " -0.23003117 -0.86933054 -1.31114838 -0.21049336 -1.62275484 -0.36128835\n",
      "  1.2815588  -0.26896365 -1.74877276 -0.7606754  -0.29703585 -0.81037427\n",
      " -0.03675743  0.12173971 -1.46425031 -0.53636391 -0.82613138 -0.77850505\n",
      " -0.24974828 -1.09921279 -1.32634224 -0.24308126 -0.56273397 -1.70278055\n",
      " -0.57189305 -0.24700312 -0.98363211 -0.87647385 -0.13579853 -0.18575231\n",
      " -1.00026533 -1.03149416  1.66499781  0.89062032 -0.37169025 -0.12257787\n",
      "  0.07882404 -0.90888524 -1.18059876 -0.93097412]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(tx_testing, w_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w_reg, tx_testing, True)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "164\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_search import *\n",
    "# optimising the lambda for regularized logistic regression \n",
    "\n",
    "gamma = 0.0001\n",
    "n_iter = 10000\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "n_intervals = 20\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w_reg = np.random.rand(tX.shape[1])\n",
    "\n",
    "lambdas = generate_lambda(n_intervals)\n",
    "min_loss, best_lambda = grid_search(y_training, y_testing, tx_training, tx_testing, w_reg, lambdas, gamma, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The optimum lambda is : \" + str(best_lambda))\n",
    "print(\"The corresponding loss is : \" + str(min_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights=[ 4.18126411e-01  4.58273615e-05 -3.19506450e-03 -3.01310197e-03\n",
      " -9.36241702e-05 -1.31006068e-02  3.00232104e-04 -1.26272195e-02\n",
      "  1.82852409e-01  3.09856862e-05 -8.57916264e+00 -1.24072735e-01\n",
      "  4.55818088e-02  3.03746120e-02  8.58312415e+00 -3.69547831e-03\n",
      " -6.64431604e-04  8.58477005e+00 -8.53597406e-03  2.14622354e-03\n",
      "  2.23206425e-03  1.12569733e-03 -1.27005121e-04 -1.50436523e-01\n",
      " -7.29926563e-04  1.34798869e-03 -4.44571869e-04 -1.46663794e-03\n",
      "  2.63242792e-03 -5.87587880e-03  8.57943402e+00], loss=0.0838234401482619\n",
      "training loss=0.08382344014826187\n",
      "testing loss=0.08764292383411\n",
      "After least squares: 74.8 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "#least squares\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w_lsq, loss = least_squares(y_training, tx_training)\n",
    "print(\"weights={w}, loss={l}\".format(w=w_lsq, l=loss))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, w_lsq)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, w_lsq)))\n",
    "print(\"After least squares: \"+str(round(100*np.sum(predict_labels(w_lsq, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.64486650e-02  5.35020820e-02  1.08276990e+00  5.44430755e-01\n",
      "  3.48519650e-01  1.74483320e-01  3.06513173e-01  8.39105322e-02\n",
      "  2.58203655e-01  4.91867075e-01  8.25618104e-03  5.24852754e-01\n",
      "  3.57811610e-01  2.98187846e-01  1.85451492e-01 -3.90713214e-02\n",
      "  3.67218035e-01  5.38579088e-01  3.67880742e-01  4.57733237e-01\n",
      " -2.83197924e-01 -4.40342469e-03 -1.72402142e-02 -2.43252653e-01\n",
      "  3.16089306e-01  3.43846863e-01  7.62261945e-01  2.88861136e-01\n",
      "  2.68702776e-01  3.16121506e-01  3.74174665e-01  4.53080689e-02\n",
      "  7.81919269e-01  3.70170175e-01  5.26271381e-02  2.51676263e-02\n",
      "  4.68430598e-01  2.95837278e-01  5.73112470e-01  2.30864383e-01\n",
      "  2.65900501e-01  4.15444330e-01  1.03985184e-01  2.83327675e-01\n",
      "  2.39100189e-01  4.78192809e-01  6.97538159e-01  1.16114007e-01\n",
      "  5.60037664e-01  1.56834113e+00  5.32702030e-01  2.71040881e-01\n",
      "  4.84184256e-01  3.68423365e-01  9.62039046e-02  5.12989106e-01\n",
      "  2.65544355e-01  2.82280012e-01  4.57484077e-01 -8.59902146e-02\n",
      "  3.41289495e-01  5.77507069e-01  5.32569182e-01  6.19079768e-01\n",
      "  3.06798404e-01  2.55801496e-01 -7.08525005e-03  3.58247056e-01\n",
      "  1.46457633e-01  1.69175850e-01  5.38106609e-01  1.78353554e-01\n",
      "  1.06917742e-01  5.38994084e-01  1.88820031e-01  5.92511515e-01\n",
      "  3.59267767e-01  1.43161494e-01  7.12052536e-01  5.70372883e-01\n",
      "  8.52266166e-01  3.70365950e-02 -4.33556363e-01  5.41304080e-01\n",
      "  4.32178292e-01  4.96007931e-01  3.47519007e-01  4.78072820e-01\n",
      " -2.21848144e-02  3.24252431e-01  3.02209219e-02  6.90509944e-01\n",
      "  5.29197123e-01  4.18981035e-01  1.94082018e-01  1.29471240e+00\n",
      " -7.32855652e-03  4.85058841e-01  4.38448294e-01  3.38746829e-01\n",
      "  2.07952106e-01  6.24799645e-01  5.96008743e-02  4.57812192e-01\n",
      "  5.62197281e-02  1.00740027e-02  6.85261406e-01  3.56489895e-01\n",
      "  4.57335267e-01  3.68280143e-01  1.70309744e-01  1.21722463e-01\n",
      "  5.64368145e-01  8.22803170e-01  1.76058427e-01  4.88346381e-01\n",
      "  3.10867760e-01  2.53631210e-01  1.73663052e-01  5.68023950e-01\n",
      "  8.54140242e-02  3.57056540e-01  4.34557246e-01  3.91516549e-01\n",
      "  4.17713236e-01  2.97155248e-01 -9.05496048e-02  1.40968643e-01\n",
      "  4.08225386e-01  4.37850666e-01  1.34234217e-03  3.25239691e-01\n",
      " -2.95686916e-02  1.07129023e-01  2.59874033e-01  4.50469435e-01\n",
      "  8.61104461e-02  3.22259674e-01  3.99931646e-01  1.60796665e-01\n",
      "  3.95928647e-01  2.68642937e-02  3.05168111e-01  1.01667636e-01\n",
      "  3.74895316e-01  8.04915162e-02  3.47217843e-01  4.88490533e-01\n",
      "  5.34027286e-01  6.90219342e-01  5.39603468e-01  2.37125096e-01\n",
      "  4.52933323e-01  5.22584549e-01  3.93266400e-01  5.36169792e-01\n",
      " -7.39941237e-02  5.38403192e-01  6.67734605e-03 -1.11900398e-02\n",
      "  4.69168517e-01  2.61742191e-01  2.39807219e-01 -1.87862456e-01\n",
      " -5.04281640e-02  6.62480130e-01  4.84464139e-01  4.12700645e-01\n",
      "  5.83043890e-01  2.83724875e-01 -5.62770766e-03  1.35838996e-02\n",
      "  2.99763967e-01  3.92102338e-01  4.39297604e-01  4.76681556e-01\n",
      "  3.73185786e-01  4.87836094e-01 -1.66678198e-02  6.21344261e-01\n",
      "  7.18903141e-01 -4.33842544e-02  1.65680779e-01  2.31904315e-01\n",
      "  2.57390751e-01  9.24139395e-02  5.54230942e-01  4.73662957e-01\n",
      "  3.29230545e-01  5.47063208e-01  3.31411087e-01  2.28456644e-01\n",
      "  4.48058901e-01  1.72948276e-01  3.09121080e-01 -9.88688098e-03\n",
      "  2.26979044e-01  4.97896890e-01  5.22985463e-01  4.08991460e-01\n",
      "  1.53908531e-01  7.28373277e-01  6.98332247e-01  5.16283893e-01\n",
      "  3.26174270e-01  5.45307370e-01  2.73669981e-01  2.25742947e-01\n",
      "  2.34465841e-01  2.42826334e-01  2.39209866e-01  5.57716675e-01\n",
      "  4.31310246e-01 -5.48622645e-01  4.09284596e-01  1.30947271e-01\n",
      "  5.22879785e-01  1.99170819e-01  9.06065857e-01  1.98954148e-01\n",
      "  1.29791656e-01  4.36489705e-01  6.45589483e-01  1.61930120e-01\n",
      "  5.09549013e-02  3.60065853e-02  1.89198549e-01  3.27358232e-01\n",
      "  5.19770929e-01  3.73809928e-01  3.26726891e-01  4.36107929e-01\n",
      "  3.23283746e-01  1.30447323e-01  2.88471840e-01  4.79145839e-01\n",
      "  1.14887788e-01  4.97240030e-01  3.05954153e-01  4.84686341e-01\n",
      "  3.50406441e-01 -9.88728165e-04  9.74660239e-02  5.49691646e-01\n",
      "  2.54023656e-01  2.22868831e-01 -3.15862822e-02  4.55709638e-01\n",
      "  2.59896779e-01  1.60154097e-01  4.26286078e-01  5.66540897e-01\n",
      "  8.12270156e-02  1.51112434e-01  3.12910606e-01  2.96739491e-01\n",
      "  4.73254110e-01  1.34797976e-02  5.94646385e-01  1.19598284e-01\n",
      "  5.22587722e-01  2.19410919e-01  4.56430765e-01  1.37306670e-01\n",
      "  3.16363981e-01  8.74513344e-02  6.14710993e-01  5.99940833e-02\n",
      "  4.14074474e-01  5.89256299e-01  1.70458029e-01  4.55103607e-01\n",
      "  5.89773598e-01  5.85061122e-01  5.42581111e-01  7.97832173e-01\n",
      "  3.62373034e-01  4.03620486e-01  4.73773633e-01  3.92591020e-01\n",
      "  2.49515882e-01  3.70818119e-01  3.55444138e-01  5.27700566e-01\n",
      "  1.55483850e-01  5.30589294e-01  2.01881427e-01  9.78180692e-02\n",
      "  3.91074251e-01  1.93620972e-01  7.13300261e-01  9.05141212e-03\n",
      "  1.82096203e-01  8.69866187e-01  9.35715848e-02  7.35448087e-01\n",
      "  4.37841653e-01  3.16330226e-01  6.26558377e-01  2.54978418e-01\n",
      "  6.73090891e-01  2.35324578e-01  1.91868730e-01  3.81127517e-01\n",
      "  4.83495912e-01  2.78362851e-01  7.04025793e-02  1.12988821e-01\n",
      "  4.42718350e-01  2.30254970e-01 -1.59414678e-01  5.42913922e-01\n",
      "  2.12009715e-01  5.73033684e-01  3.71056546e-01  5.84380147e-01\n",
      "  2.69177056e-01  4.42872811e-01  1.60374173e-01  6.06520380e-02\n",
      "  5.17023787e-01  4.77670833e-01  2.84243693e-01  4.13784941e-01\n",
      "  1.00443391e-01  4.94324513e-01  2.75596857e-02  6.78165091e-01\n",
      "  8.21293596e-01  6.38986559e-01  7.24981820e-01  1.52557947e-01\n",
      "  1.10771440e+00  2.04818919e-01  6.56373327e-01  2.82748003e-01\n",
      "  9.52452103e-01  2.02153884e-01  9.03419414e-01  4.86194589e-01\n",
      " -1.86267712e-01  3.37404245e-01  7.66863227e-02  2.73481300e-01\n",
      "  2.12024105e-01  1.58289628e-01  1.03235924e+00  2.95055609e-01\n",
      "  4.44242845e-01  2.08102738e-01  8.07340479e-02  2.07972107e-01\n",
      "  3.81014173e-01  3.46114484e-01  1.91485107e-01  4.95672678e-01\n",
      "  5.09157645e-01  2.06228992e-01  6.14345046e-01  8.71069482e-01\n",
      "  3.25940489e-01  2.04966015e-01  3.78762887e-01  2.99213910e-01\n",
      "  6.15925131e-02  5.33163582e-01  8.04674672e-02 -8.75530228e-02\n",
      "  1.17542865e-01  4.83333615e-01  2.06519084e-01  8.52080362e-01\n",
      "  7.94348409e-02  2.41645950e-01  7.74727604e-01  3.22686625e-01\n",
      " -1.43856810e-02  4.91964961e-01  1.27742053e-01  3.78166258e-01\n",
      "  2.83264270e-01  6.51910507e-01  4.06607686e-01  5.66654955e-01\n",
      "  1.27661347e-01  6.05814535e-01  3.87423670e-01 -9.88092405e-02\n",
      "  3.23966590e-02  3.93551001e-01  2.12019302e-01  4.13955893e-01\n",
      "  5.28047597e-01  2.60775861e-01  6.82809085e-01  5.60029592e-01\n",
      " -2.43096911e-02  1.85069835e-01  3.12431855e-01  3.46938449e-01\n",
      "  7.06603466e-01  1.65824312e-01  3.20521112e-01  5.18394454e-01\n",
      "  1.12591648e+00  3.08559529e-01  5.83993959e-02  4.73612425e-01\n",
      "  3.61955171e-01  2.21249196e-01 -2.87333651e-03  3.31080825e-01\n",
      "  5.00946000e-02  5.46361064e-01  1.44810028e-01  4.85897821e-01\n",
      "  2.54239562e-01  2.30523463e-01  2.12585103e-01  2.49250445e-01\n",
      "  5.94036146e-01  1.85512131e-01 -4.60390591e-02  1.09574292e-01\n",
      "  5.14282208e-02  3.65896344e-01  1.80170467e-01  2.67978290e-01\n",
      "  1.77056295e+00  2.68389968e-01  4.58168944e-01  4.57072845e-01\n",
      "  2.19583679e-01  8.25960835e-02  5.32104843e-01  6.74957973e-01\n",
      "  6.96352855e-01  5.28270466e-01  1.95017756e-01  4.55663840e-01\n",
      "  1.32128560e-01  5.72801648e-01  1.57577660e-01  6.31784457e-01\n",
      "  3.87105198e-01  5.75059129e-01  5.77572657e-01  3.98336287e-01\n",
      "  3.55808216e-01 -1.20785658e-01  6.46345740e-01  2.21571103e-02\n",
      "  3.33704114e-01  9.24144427e-02  5.93917992e-01  6.70704344e-02\n",
      "  2.11065743e-01 -2.64548794e-03  3.15561545e-01  9.55482871e-02\n",
      "  1.21085510e-01  2.16457551e-01  2.55740927e-01  7.23063145e-02\n",
      "  6.21541209e-01  1.13425871e+00  3.96685200e-01  3.98187729e-01\n",
      "  2.49333945e-01  7.07781964e-02  4.76521775e-01  4.63438891e-01\n",
      "  6.73638987e-01  3.92153276e-01  6.38580674e-01  4.07032624e-01\n",
      "  2.86445333e-01  8.96229936e-02  4.47647882e-01  1.77420718e-01\n",
      "  3.58665173e-01  8.88108393e-02  2.30492219e-01  1.72877400e-01\n",
      "  3.11784567e-01  1.71494816e-01  8.22688830e-02  4.43998913e-01\n",
      " -4.21341213e-01  5.54503961e-01  2.38408329e-01  1.09430288e-02\n",
      "  2.01137963e-01  3.00742590e-01  2.71288530e-01  3.39327095e-01\n",
      "  5.68681748e-01  2.71213846e-01  2.46531701e-01  2.31923463e-01\n",
      "  6.04174938e-01  4.45594296e-01  6.73027346e-01  1.54373887e-01\n",
      "  3.07173235e-01  3.23672115e-01  1.09187000e-01  4.71835634e-01\n",
      "  1.78282270e-01  8.42852317e-01  3.06284660e-01  2.09288773e-02\n",
      "  1.20832369e-01 -7.81400194e-02  1.22827485e-01  5.75587455e-01\n",
      "  4.96753213e-01  4.53506003e-02  4.50934204e-01  8.12772453e-01\n",
      "  7.87773105e-02  3.31060691e-01  5.67575625e-01  3.94812396e-01\n",
      "  1.55060905e-01  1.17734844e-01  5.77094000e-02  6.05585343e-01\n",
      "  5.20524325e-01  1.58194177e-01  1.09786991e-01  4.83093088e-01\n",
      "  6.45122928e-01  1.04287036e+00  4.93331056e-01  2.51088924e-01\n",
      "  4.40611277e-01  3.60077389e-01  3.21183270e-01  3.72297946e-01\n",
      "  1.24001813e-01  3.04369949e-02  2.06383777e-01  2.31502154e-01\n",
      "  3.51868530e-01  5.36175184e-01  6.55183793e-01  2.65779538e-02\n",
      "  3.01524847e-01  1.06997303e-01 -6.35011850e-02  4.42799904e-01\n",
      "  3.66594251e-04  4.00359702e-01  7.54661565e-01  1.60070769e-01\n",
      " -1.29997332e-01  4.46720596e-01  2.50321489e-01  5.80133983e-01\n",
      "  4.94419313e-02  6.32155795e-01  4.95806858e-01  5.38415523e-01\n",
      "  5.34981814e-01  2.88366678e-01  2.97925347e-01  1.95404460e-01\n",
      "  3.72706063e-01  4.73655616e-01  4.34708999e-01  2.42456544e-01\n",
      "  3.97289243e-01  5.04246359e-01  2.44238248e-01  2.42304385e-02\n",
      "  5.19554455e-01  5.58502041e-02  7.65422171e-01  7.44300120e-01\n",
      "  5.32308324e-01  2.20967236e-01  8.12347236e-02  4.53643904e-01\n",
      "  1.00088037e-01  5.34376194e-01  4.52387557e-01  2.28882759e-01\n",
      "  3.15981137e-01  5.61872477e-01  2.98688247e-01  7.99974538e-01\n",
      " -1.07849394e-01  4.19292909e-02  2.05231576e-01  4.87976968e-01\n",
      "  1.22018110e-01  3.76123043e-01  3.82675809e-01  4.71664764e-01\n",
      "  6.66393353e-01  4.22711068e-01 -2.67174714e-01  1.27900945e-01\n",
      "  5.51711129e-01 -2.06361222e-02 -7.58928409e-02  3.21770011e-01\n",
      "  6.96689555e-01  3.66091633e-01 -1.73109339e-01  5.52011128e-01\n",
      "  9.04336379e-01  3.64699041e-01 -1.51140492e-02  5.12587876e-01\n",
      "  1.91634521e-01  3.73860311e-01  1.73727171e-01  2.50288613e-01\n",
      "  1.79684835e-01  4.24770302e-01  4.05897897e-01  5.28325137e-01\n",
      "  1.92942621e-01  1.70209926e-01 -4.07988189e-02  1.46251357e-01\n",
      "  5.88116384e-01  7.62252959e-02  6.00242971e-01  8.36495040e-02\n",
      "  3.06963399e-02  2.39080980e-01  1.83117040e-01  2.56465518e-01\n",
      "  3.97629391e-02  9.51806182e-02 -5.76464647e-02  5.10776894e-01\n",
      "  6.79861713e-01  5.66214315e-01  4.70576868e-01  1.95422214e-01\n",
      "  3.16045230e-01  5.65079751e-01  3.75814629e-01  1.03444810e-01\n",
      "  3.22717899e-01  5.01511361e-01  1.63557948e-01  3.42765256e-01\n",
      " -6.67778900e-03  1.74591793e-01  5.77899524e-01  6.05527540e-01\n",
      "  4.91288994e-01  2.05755191e-01  4.99884931e-01  5.23487366e-01\n",
      "  6.25852952e-01  5.47392245e-01  4.40170970e-02  5.39416223e-01\n",
      "  8.40383142e-01  1.74862572e-01  8.20360742e-01 -6.19845753e-02\n",
      "  1.43900021e-01  1.72413170e-01  3.03955136e-01  1.55293519e-01\n",
      "  5.48863912e-01  9.35084485e-01  6.04671274e-01  4.75632546e-01\n",
      "  5.01860687e-01  3.92656920e-01  8.83177789e-02  6.30829322e-03\n",
      "  5.71924626e-01  3.67228782e-01 -4.68529565e-02  1.61896821e-01\n",
      "  2.49407347e-01 -7.90047215e-02  2.69870238e-01  1.63328744e-01\n",
      "  5.81640390e-01 -7.05484981e-02  2.12285928e-01  4.50867571e-01\n",
      "  1.74614017e-01  4.93453721e-01  4.16946470e-01  2.82625216e-01\n",
      "  2.09518879e-01 -4.87572192e-02  6.57883803e-01  4.51900562e-01\n",
      "  5.27377813e-01  7.74025601e-02  4.60805845e-01  3.87270995e-01\n",
      "  7.21634147e-01  2.27507193e-01  3.60670906e-01  2.15173789e-01\n",
      "  7.12611221e-02  2.18189598e-01  4.54668187e-01 -1.76396589e-01\n",
      "  8.50568589e-02  1.44077905e-01  5.13000928e-01  6.19657851e-01\n",
      "  8.35869236e-01  7.39066469e-01  1.56476656e-01  7.57430179e-03\n",
      "  1.57896807e-01  5.30842509e-01  5.47221070e-01  2.36127376e-01\n",
      "  1.30624745e-01  4.57551149e-01  4.83328379e-01  6.30947014e-02\n",
      "  4.42843857e-01  3.36498781e-01  4.63085237e-01  7.17019719e-01\n",
      "  1.43408173e-01  6.05914761e-01 -8.50218436e-03  2.48638460e-01\n",
      "  5.66246762e-01  1.53661424e-01 -5.51554881e-02  4.17662355e-01\n",
      "  5.21851731e-01  3.31647404e-01  5.23168209e-01  6.18373183e-01\n",
      "  3.70533552e-01  1.28161398e-01  3.01258666e-01  2.46789766e-01\n",
      "  3.20314144e-01  5.13601552e-02  8.98652177e-02  2.81587299e-01\n",
      "  5.35545044e-01  9.47158572e-02  2.73691894e-01  7.16916927e-03\n",
      "  4.92814421e-01  3.07020394e-01  2.28794573e-02  3.42869964e-01\n",
      "  8.93804807e-01  1.89986966e-01  4.98371383e-01  1.94611949e-01\n",
      "  3.32635441e-01  4.55287392e-01  4.67542478e-01  4.27075492e-01\n",
      "  5.23733516e-01  6.36865375e-02  4.28897776e-01  5.35352482e-01\n",
      " -7.75986547e-01  5.63682240e-01  5.01527969e-01  2.00275807e-01\n",
      "  2.89658434e-01  4.21346015e-01  6.04563050e-01  3.86281016e-01\n",
      "  4.95958522e-01  6.51298124e-01  7.15646101e-01  3.50431272e-01\n",
      "  1.89982458e-01  2.63276962e-01 -8.71840582e-02  7.73462009e-01\n",
      "  6.77270462e-03  3.56277369e-02  4.94289812e-01  1.75880067e-01\n",
      "  3.86379530e-01 -4.14218693e-01  5.12532454e-01  4.18158258e-01\n",
      " -1.39334843e-01  1.02506936e-01  2.17216068e-01  3.75322503e-01\n",
      "  2.69818583e-01  2.20210904e-01  5.98145031e-01  1.02210492e-01\n",
      "  4.07061846e-04  2.00752899e-01  3.15661022e-01  5.05038358e-01\n",
      "  2.21582862e-01  3.89123306e-01  4.65114426e-01  2.37604202e-01\n",
      "  3.84118153e-01  6.16539629e-01  5.27823582e-01  1.00346889e+00\n",
      "  3.74784509e-01  3.26716692e-01  3.43867720e-01  2.06258314e-01\n",
      " -3.99068126e-02  1.24498053e-01  4.93900697e-01 -2.20352458e-02\n",
      "  6.47325336e-01  2.63074841e-01  1.86453841e-01  5.01651248e-01\n",
      "  6.59703763e-02 -2.94507456e-02  4.11415656e-02  6.48787655e-01\n",
      "  7.75690587e-01  3.76049176e-01  2.45957991e-01  4.20244639e-01\n",
      "  5.26909068e-01  5.38146657e-01  5.58655834e-01  1.73045616e-01\n",
      "  5.17800415e-01  3.39221241e-01  4.14601613e-01  4.45503816e-01\n",
      "  3.26334055e-01  3.32974220e-01  2.68058900e-01  3.91039645e-01\n",
      "  5.34943446e-01  3.19630152e-01  1.01490242e-01  1.82482710e-01\n",
      "  1.23684007e-02  4.80778927e-01  1.43029391e-01 -1.11888997e-01\n",
      "  3.16407318e-01  3.91270470e-01  1.55004667e-01 -1.21298799e-01\n",
      "  3.62108695e-01  2.44916632e-01  3.40584719e-01  1.67193383e-01\n",
      "  4.80175066e-01  4.98702791e-01  4.35854225e-01  7.97911132e-02\n",
      " -1.84888674e-01 -1.21591016e-02  7.69172318e-01  5.41863723e-01\n",
      "  2.90078859e-01  4.81469079e-01 -3.87439497e-02  8.55216116e-01\n",
      "  6.79855710e-01  4.56698928e-01  8.26839833e-01  6.67056082e-01\n",
      "  4.25163053e-01  3.41368897e-01  1.17763574e-01  1.21815283e-01\n",
      "  5.98501632e-03  5.81790182e-01  3.59544858e-01  3.00044132e-01\n",
      "  3.11194191e-01  2.10923426e-01  1.18406942e-01  3.35757986e-01\n",
      "  2.30141023e-01  1.21952565e+00  6.14990416e-01 -2.15845594e-01\n",
      "  3.93655093e-01  5.46421114e-01  4.30895154e-01  5.05293328e-01\n",
      "  1.49365715e-01  3.84379460e-01  3.49618378e-01  3.77813986e-01\n",
      "  2.86885046e-01  1.33952897e-01  2.68679852e-01  5.75985433e-01\n",
      "  4.54401131e-01  2.66233474e-01  3.66552141e-02 -8.35004010e-02\n",
      "  5.64313418e-01  2.88665855e-01  4.61384898e-01  6.38141377e-01\n",
      "  4.82125435e-01  3.57148394e-01  1.32862100e-01  1.29284877e-01\n",
      "  1.24333225e-01  4.31187664e-01  3.63020520e-01  2.86550557e-01\n",
      "  9.17283972e-04  2.93003065e-01  4.57423043e-01  9.48187028e-02\n",
      "  2.74971257e-01  5.41304080e-01  2.37137131e-01  4.56473025e-01\n",
      "  3.98333473e-01  3.01146200e-01  3.06328886e-01  2.92941901e-01\n",
      "  3.96606810e-01  3.23969183e-01  2.16318678e-01  4.34292639e-01\n",
      "  4.69693991e-01 -3.77005137e-02  1.50991604e-01  7.40723827e-01\n",
      " -4.73521045e-02  1.13731481e-01  2.86901287e-01  5.64265136e-01\n",
      "  6.57725390e-02  3.97673196e-01  7.69010346e-02  5.84336420e-01\n",
      "  5.20006881e-01  1.84026682e-01  3.69031449e-01  2.32829997e-01\n",
      "  3.87198486e-01  2.36068023e-01  2.08447967e-01  2.17148127e-01\n",
      "  4.38540592e-01  3.33409779e-01  3.44604262e-01  5.00414992e-01\n",
      "  5.03844178e-02  5.82426324e-01  8.91863536e-01  5.47271064e-01\n",
      "  5.23520787e-02  4.56113105e-01  4.66077057e-01  2.74304494e-01\n",
      "  4.47715432e-01  4.66973819e-01  1.62053896e-02  4.18030880e-01\n",
      "  3.30031400e-01  3.89506791e-01  3.65133934e-01  8.02449079e-02\n",
      "  1.01571131e-01  5.35207730e-01  4.51233072e-01  5.40585008e-02\n",
      "  4.53353011e-01  5.29959873e-01  3.26468584e-01  3.57639557e-01\n",
      "  5.28466685e-01  5.55526001e-01  1.63864639e-01  2.35616528e-01\n",
      "  8.41018116e-01  7.64085778e-01  4.48174238e-01  5.04261612e-01\n",
      "  5.62170143e-01  2.92301652e-01  1.31463301e-01  3.90014605e-01]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(tx_testing, w_lsq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w_lsq, tx_testing, False)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=11.321584492339047\n",
      "Processing 2th experiment, degree=3, rmse=11.321584492339047\n",
      "Processing 3th experiment, degree=7, rmse=11.321584492339047\n",
      "Processing 4th experiment, degree=12, rmse=11.321584492339047\n"
     ]
    }
   ],
   "source": [
    "def polynomial_regression():\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 3, 7, 12]\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        tx = build_poly(tX, degree)\n",
    "        print(tx.shape)\n",
    "        weights, mse = least_squares(y, tX)\n",
    "        rmse = np.sqrt(2 * mse)\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "              i=ind + 1, d=degree, loss=rmse))\n",
    "        \n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.000, training loss=0.083, testing loss=0.081\n",
      "lambda=0.000, training loss=0.083, testing loss=0.081\n",
      "lambda=0.000, training loss=0.084, testing loss=0.081\n",
      "lambda=0.000, training loss=0.086, testing loss=0.082\n",
      "lambda=0.000, training loss=0.089, testing loss=0.084\n",
      "lambda=0.001, training loss=0.092, testing loss=0.086\n",
      "lambda=0.001, training loss=0.096, testing loss=0.090\n",
      "lambda=0.003, training loss=0.101, testing loss=0.095\n",
      "lambda=0.007, training loss=0.105, testing loss=0.100\n",
      "lambda=0.016, training loss=0.107, testing loss=0.104\n",
      "lambda=0.037, training loss=0.110, testing loss=0.107\n",
      "lambda=0.085, training loss=0.112, testing loss=0.108\n",
      "lambda=0.193, training loss=0.115, testing loss=0.109\n",
      "lambda=0.439, training loss=0.121, testing loss=0.111\n",
      "lambda=1.000, training loss=0.130, testing loss=0.116\n",
      "weights=[0.05898849 0.01702444 0.01349676 0.01675819 0.01916236 0.01891701\n",
      " 0.01868751 0.01543015 0.01700008 0.01694654 0.01876702 0.01527165\n",
      " 0.02002863 0.01888765 0.01951832 0.01725549 0.01712848 0.01684741\n",
      " 0.0173858  0.01740115 0.01739162 0.01723937 0.01856934 0.01865364\n",
      " 0.01810351 0.01680362 0.01675715 0.01686174 0.01719905 0.01671958\n",
      " 0.01859722], loss=0.12956722492822362\n",
      "training loss=0.1168530151919449\n",
      "testing loss=0.11587231092601533\n",
      "After ridge regression: 67.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ridge_regression import ridge_regression\n",
    "#ridge regression\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    # ridge regression with a given lambda\n",
    "    weight_ridge, loss = ridge_regression(y_training, tx_training, lambda_)\n",
    "    print(\"lambda={lam:.3f}, training loss={l_tr:.3f}, testing loss={l_te:.3f}\".format(lam=lambda_, l_tr=loss, \n",
    "                                                                                       l_te=compute_loss(y_testing, tx_testing, weight_ridge)))\n",
    "    \n",
    "print(\"weights={w}, loss={l}\".format(w=weight_ridge, l=loss))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, weight_ridge)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, weight_ridge)))\n",
    "\n",
    "print(\"After ridge regression: \"+str(round(100*np.sum(predict_labels(weight_ridge, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=10.68415524953714\n",
      "Current iteration=1000, loss=0.1006983499804822\n",
      "Current iteration=2000, loss=0.09432945739651882\n",
      "Current iteration=3000, loss=0.09105909807248724\n",
      "Current iteration=4000, loss=0.08919778187398651\n",
      "weights=[-0.67632045  0.1963382  -1.51889326 -0.17343314  0.47233771  0.7712353\n",
      "  0.18353181 -0.526078    0.31597263 -0.11399315 -0.23923952 -0.58794283\n",
      "  1.53959726  0.62252622  1.06487568  0.40417022  0.22069267  0.16713433\n",
      " -0.04947574  0.48755469 -0.09969004 -0.036905    0.15744464  0.23181153\n",
      "  0.16792303 -0.20231044  0.12625754 -0.20213052  0.00415655 -0.16511453\n",
      "  0.24322293], loss=0.08803858500144947\n",
      "training loss=0.08803764894542244\n",
      "testing loss=0.08708009963533041\n",
      "After gradient descent: 74.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import *\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_ws, gradient_losses= gradient_descent(y_training, tx_training, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=gradient_ws, l=gradient_losses))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, gradient_ws)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, gradient_ws)))\n",
    "print(\"After gradient descent: \"+str(round(100*np.sum(predict_labels(gradient_ws, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=10.684155249537138\n",
      "Current iteration=100, loss=0.11611652153666797\n",
      "Current iteration=200, loss=0.14554730629189477\n",
      "Current iteration=300, loss=0.15729584012939785\n",
      "Current iteration=400, loss=0.14270265710658306\n",
      "Current iteration=500, loss=0.14298708683238764\n",
      "Current iteration=600, loss=0.10556115934044442\n",
      "Current iteration=700, loss=0.10730606578847834\n",
      "Current iteration=800, loss=0.14551074619988377\n",
      "Current iteration=900, loss=0.21453318010213285\n",
      "Current iteration=1000, loss=0.11551455058310207\n",
      "Current iteration=1100, loss=0.16175518591596363\n",
      "Current iteration=1200, loss=0.1205090926539504\n",
      "Current iteration=1300, loss=0.12327826509625146\n",
      "Current iteration=1400, loss=0.10193669583300485\n",
      "Current iteration=1500, loss=0.0981311189563209\n",
      "Current iteration=1600, loss=0.10670949634004609\n",
      "Current iteration=1700, loss=0.10871143412603702\n",
      "Current iteration=1800, loss=0.09593688576563207\n",
      "Current iteration=1900, loss=0.09624031797805332\n",
      "Current iteration=2000, loss=0.14465157112595062\n",
      "Current iteration=2100, loss=0.11531081540132122\n",
      "Current iteration=2200, loss=0.12581218995274956\n",
      "Current iteration=2300, loss=0.09500207807191174\n",
      "Current iteration=2400, loss=0.12523628169823547\n",
      "Current iteration=2500, loss=0.12174955822264694\n",
      "Current iteration=2600, loss=0.10160210166529182\n",
      "Current iteration=2700, loss=0.15615198040815262\n",
      "Current iteration=2800, loss=0.18584399019535516\n",
      "Current iteration=2900, loss=0.0972637838642432\n",
      "Current iteration=3000, loss=0.10865722189086531\n",
      "Current iteration=3100, loss=0.11356751220163719\n",
      "Current iteration=3200, loss=0.09288599100999133\n",
      "Current iteration=3300, loss=0.13428916213334555\n",
      "Current iteration=3400, loss=0.19894353148093608\n",
      "Current iteration=3500, loss=0.11667871788847228\n",
      "Current iteration=3600, loss=0.09954323456578638\n",
      "Current iteration=3700, loss=0.13818435891922054\n",
      "Current iteration=3800, loss=0.21580122317832234\n",
      "Current iteration=3900, loss=0.0959180464846572\n",
      "Current iteration=4000, loss=0.08985200046465622\n",
      "Current iteration=4100, loss=0.08982641574038834\n",
      "Current iteration=4200, loss=0.11899354662930639\n",
      "Current iteration=4300, loss=0.09148029602972145\n",
      "Current iteration=4400, loss=0.0958351315616195\n",
      "Current iteration=4500, loss=0.09805403685822828\n",
      "Current iteration=4600, loss=0.08936670407373677\n",
      "Current iteration=4700, loss=0.10812971209843943\n",
      "Current iteration=4800, loss=0.25566428874790276\n",
      "Current iteration=4900, loss=0.09245414306084061\n",
      "weights=[-7.56840847e-01  2.13088766e-01 -1.41284105e+00 -1.61766921e-01\n",
      "  5.21453761e-01  7.59636439e-01  1.41150350e-01 -5.92373135e-01\n",
      "  3.14893809e-01 -7.70022195e-03 -2.74330821e-01 -7.22916502e-01\n",
      "  1.55342814e+00  7.01136737e-01  1.03249140e+00  4.39626916e-01\n",
      "  1.62270978e-01 -7.86730428e-04 -8.92054353e-02  5.89802809e-01\n",
      "  1.81253515e-01 -1.19404676e-01  1.84092815e-01  2.27113752e-01\n",
      "  1.45230550e-01 -1.34219555e-01  8.51293647e-02 -1.88963510e-01\n",
      " -3.93527489e-02 -1.33615058e-01  2.38569442e-01], loss=0.08914888593335403\n",
      "training loss=0.08930297820116996\n",
      "testing loss=0.08823516920672592\n",
      "After stochastic gradient descent: 72.7 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_ws, sgd_losses = stochastic_gradient_descent(y_training, tx_training, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=sgd_ws, l=sgd_losses))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, sgd_ws)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, sgd_ws)))\n",
    "print(\"After stochastic gradient descent: \"+str(round(100*np.sum(predict_labels(sgd_ws, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test = correction_missing_values(tX_test)\n",
    "tX_test, _, _ = standardize(tX_test)\n",
    "tX_test = normalize(tX_test)\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'test_lsq2.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_lsq, tX_test, False)\n",
    "y_pred[np.where(y_pred == 0)] = -1\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
