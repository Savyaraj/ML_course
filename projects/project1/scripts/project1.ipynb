{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38114, 0, 0, 0, 177457, 177457, 177457, 0, 0, 0, 0, 0, 177457, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 99913, 99913, 99913, 177457, 177457, 177457, 0]\n"
     ]
    }
   ],
   "source": [
    "print([np.sum(tX[:,i]==-999) for i in range(tX.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "seed = 6\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_training = np.delete(tx_training, (4,5,6,12,26,27,28), axis=1)\n",
    "tx_training, col_median_tX = correction_missing_values(tx_training)\n",
    "#print([(np.max(tX[:,i]),np.min(tX[:,i]),np.mean(tX[:,i]),np.median(tX[:,i])) for i in range(tX.shape[1])])\n",
    "#plt.boxplot(tx_training)\n",
    "tx_training, perc_25, perc_75, col_median_tX = remove_outliers(tx_training)\n",
    "#tx_training, max_, min_ = normalize(tx_training)\n",
    "tx_training, mean, std = standardize(tx_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_testing = np.delete(tx_testing, (4,5,6,12,26,27,28), axis=1)\n",
    "tx_testing = correction_missing_values_test(tx_testing, col_median_tX)\n",
    "tx_testing = remove_outliers_test(tx_testing, perc_25, perc_75, col_median_tX)\n",
    "#tx_testing = (tx_testing - min_) / (max_ - min_)\n",
    "tx_testing = (tx_testing - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from scikit learn - logistic regression: 78.85 %\n",
      "\n",
      "Accuracy from scikit learn - Support Vector Machine: 96.25 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_logistic = LogisticRegression(random_state=0).fit(tx_training, y_training)\n",
    "clf_SVC = SVC(gamma='auto').fit(tx_training, y_training)\n",
    "print(\"Accuracy from scikit learn - logistic regression: \"+str(round(100*clf_logistic.score(tx_training, y_training),5))+' %\\n')\n",
    "print(\"Accuracy from scikit learn - Support Vector Machine: \"+str(round(100*clf_SVC.score(tx_training, y_training),5))+' %\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=5521.413283753469, testing loss=1260.454865442583\n",
      "Current iteration=10, training loss=2831.4255880359387, testing loss=687.4562267101987\n",
      "Current iteration=20, training loss=2204.258403944562, testing loss=563.861775675578\n",
      "Current iteration=30, training loss=2019.4559514421726, testing loss=529.3800951470937\n",
      "Current iteration=40, training loss=1947.8456766483139, testing loss=517.0578460250651\n",
      "Current iteration=50, training loss=1915.3428246331996, testing loss=512.0994962269394\n",
      "Current iteration=60, training loss=1898.8732012918622, testing loss=510.01874568679307\n",
      "Current iteration=70, training loss=1889.8459935832577, testing loss=509.1796589371522\n",
      "Current iteration=80, training loss=1884.6050025078605, testing loss=508.9050868244675\n",
      "[-0.88740606  1.01985769 -0.68726902 -0.09986496 -0.1851694   0.06088645\n",
      " -0.10154572 -0.03415832 -0.30745994  0.37593095  0.18073761 -0.01015003\n",
      "  0.03778521  0.04551408 -0.07930566  0.0932264  -0.06457739  0.01397209\n",
      "  0.06873418  0.10032532  0.21199918  0.0244286   0.04515024  0.0349298 ] 1883.1479162850283\n",
      "training loss=1882.8259057834543\n",
      "testing loss=508.87843643135466\n",
      "Accuracy from our code:\n",
      "\n",
      "Logistic regression: 76.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from logistic_regression import *\n",
    "#Using our code\n",
    "#Logistic regression\n",
    "\n",
    "gamma = 0.0001\n",
    "n_iter = 85\n",
    "\n",
    "tx_training = np.c_[np.ones((y_training.shape[0], 1)), tx_training]\n",
    "tx_testing = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing]\n",
    "\n",
    "w = np.random.rand(tx_training.shape[1])\n",
    "\n",
    "for i in range(n_iter):\n",
    "    w, loss = learning_by_gradient_descent(y_training, tx_training, w, gamma)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Current iteration={iter}, training loss={l_tr}, testing loss={l_te}\".format(iter=i, l_tr=loss, \n",
    "                                                                                                         l_te=calculate_loss(y_testing, tx_testing, w)))\n",
    "print(w, loss)\n",
    "print(\"training loss={l_tr}\".format(l_tr=calculate_loss(y_training, tx_training, w)))\n",
    "print(\"testing loss={l_te}\".format(l_te=calculate_loss(y_testing, tx_testing, w)))\n",
    "\n",
    "print(\"Accuracy from our code:\\n\")\n",
    "print(\"Logistic regression: \"+str(round(100*np.sum(predict_labels(w, tx_testing, True)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "284\n",
      "338\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tx_testing, True)\n",
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_testing==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on all data points from train.csv\n",
    "y_, tX_, ids_ = load_csv_data('../data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_ = np.delete(tX_, (4,5,6,12,26,27,28), axis=1)\n",
    "tX_ = correction_missing_values_test(tX_, col_median_tX)\n",
    "tX_ = remove_outliers_test(tX_, perc_25, perc_75, col_median_tX)\n",
    "tX_ = (tX_ - min_) / (max_ - min_)\n",
    "tX_ = (tX_ - mean) / std\n",
    "tX_ = np.c_[np.ones((tX_.shape[0], 1)), tX_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71472\n",
      "85667\n",
      "Logistic regression: 77.5292 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_, True)\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_==1]))\n",
    "print(\"Logistic regression: \"+str(round(100*np.sum(y_pred==y_)/len(y_),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=5522.318831095153, testing loss=1260.4333781867192\n",
      "Current iteration=10, training loss=2831.5040238577826, testing loss=687.3783248511893\n",
      "Current iteration=20, training loss=2204.334581461228, testing loss=563.8113249033254\n",
      "Current iteration=30, training loss=2019.5940856386665, testing loss=529.3483253001523\n",
      "Current iteration=40, training loss=1948.0357998134643, testing loss=517.0361769260752\n",
      "Current iteration=50, training loss=1915.5701962888809, testing loss=512.0828977498253\n",
      "Current iteration=60, training loss=1899.1268346255213, testing loss=510.0044088618023\n",
      "Current iteration=70, training loss=1890.118269395124, testing loss=509.16597625588525\n",
      "Current iteration=80, training loss=1884.8906396331927, testing loss=508.8911341349611\n",
      "[-0.8892887   1.02793143 -0.70016142 -0.10307377 -0.18741807  0.05434073\n",
      " -0.0966743  -0.03384179 -0.30361892  0.36787893  0.17993639 -0.01237023\n",
      "  0.03351597  0.04774421 -0.07715804  0.08822358 -0.06233701  0.01085407\n",
      "  0.0697599   0.09030497  0.208854    0.02442397  0.03895345  0.04389875] 1881.9755854479163\n",
      "training loss=1881.4347966323626\n",
      "testing loss=508.87038880719297\n",
      "After regularization: 76.5 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from reg_logistic_regression import *\n",
    "#Reguralized logistic regression\n",
    "\n",
    "gamma = 0.0001\n",
    "lamda_ = 0.105\n",
    "n_iter = 90\n",
    "\n",
    "tx_training = np.c_[np.ones((y_training.shape[0], 1)), tx_training]\n",
    "tx_testing = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing]\n",
    "\n",
    "w_reg = np.random.rand(tx_training.shape[1])\n",
    "for i in range(n_iter):\n",
    "    w_reg, loss = learning_by_penalized_gradient(y_training, tx_training, w_reg, gamma, lamda_)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Current iteration={iter}, training loss={l_tr}, testing loss={l_te}\".format(iter=i, l_tr=loss, \n",
    "                                                                                                         l_te=calculate_loss(y_testing, tx_testing, w_reg)))\n",
    "print(w_reg, loss)\n",
    "print(\"training loss={l_tr}\".format(l_tr=calculate_loss(y_training, tx_training, w_reg)))\n",
    "print(\"testing loss={l_te}\".format(l_te=calculate_loss(y_testing, tx_testing, w_reg)))\n",
    "\n",
    "print(\"After regularization: \"+str(round(100*np.sum(predict_labels(w_reg, tx_testing, True)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(w_reg, tx_testing, True)\n",
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_testing==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on all data points from train.csv\n",
    "y_, tX_, ids_ = load_csv_data('../data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_ = np.delete(tX_, (4,5,6,12,26,27,28), axis=1)\n",
    "tX_ = correction_missing_values_test(tX_, col_median_tX)\n",
    "tX_ = remove_outliers_test(tX_, perc_25, perc_75, col_median_tX)\n",
    "tX_ = (tX_ - min_) / (max_ - min_)\n",
    "tX_ = (tX_ - mean) / std\n",
    "tX_ = np.c_[np.ones((tX_.shape[0], 1)), tX_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71468\n",
      "85667\n",
      "Regularized logistic regression: 77.554 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w_reg, tX_, True)\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_==1]))\n",
    "print(\"Regularized logistic regression: \"+str(round(100*np.sum(y_pred==y_)/len(y_),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_search import *\n",
    "# optimising the lambda for regularized logistic regression \n",
    "\n",
    "gamma = 0.0001\n",
    "n_iter = 10000\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "n_intervals = 20\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w_reg = np.random.rand(tX.shape[1])\n",
    "\n",
    "lambdas = generate_lambda(n_intervals)\n",
    "min_loss, best_lambda = grid_search(y_training, y_testing, tx_training, tx_testing, w_reg, lambdas, gamma, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The optimum lambda is : \" + str(best_lambda))\n",
    "print(\"The corresponding loss is : \" + str(min_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights=[ 3.66598441e-01  2.09398782e-01 -3.05068879e-03 -1.99818081e-02\n",
      " -1.83384785e-01 -2.66200489e-02  3.07285822e-02  4.52252769e-02\n",
      " -4.47537037e-02 -3.10521859e-03 -5.95299805e-02 -3.76918551e-03\n",
      "  7.79476558e-03 -2.41872499e-02 -1.72349806e-02 -2.32072324e-04\n",
      " -1.85236296e-02  2.80616785e-02 -4.32533197e-03  5.53024378e-03\n",
      "  4.05712481e-03 -3.72715217e-03 -2.29538811e-02  9.83291248e-03\n",
      " -5.47150075e-04  5.86382336e-02  3.54795335e-02 -1.12674819e-02\n",
      "  5.87072500e-02 -3.34374623e-03 -2.06295721e-03 -4.72162526e-03\n",
      " -1.72343279e-02 -4.89215164e-04  7.56382855e-03 -8.47417844e-03\n",
      " -2.58754396e-03 -2.59801853e-02  2.87535229e-03  4.10879532e-04\n",
      "  2.79087785e-03 -3.04015839e-02 -4.17405857e-03  1.79978944e-02\n",
      " -6.36495793e-03 -2.52412325e-03  3.76673750e-03  8.94330230e-03\n",
      " -1.46389377e-03  3.42666271e-02  1.50008577e-03 -1.81931039e-02\n",
      "  3.50202375e-04 -6.25274602e-03  9.15348524e-04  7.63380754e-02\n",
      "  2.34957727e-02 -3.80639468e-02  2.57594706e-02 -1.55082641e-02\n",
      "  3.98318897e-03 -1.41403584e-03  3.01647940e-02  5.39518174e-04\n",
      "  1.49200055e-02  3.34820492e-03 -2.13973190e-03 -1.75506880e-02\n",
      "  3.56894965e-02 -9.72747885e-03], loss=0.0697153219398077\n",
      "training loss=0.0697153219398077\n",
      "testing loss=0.07828847199031279\n",
      "After least squares: 78.7 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "#least squares\n",
    "\n",
    "#try polynomial\n",
    "degree = 3\n",
    "tx_training = build_poly(tx_training, degree)\n",
    "tx_training = np.c_[np.ones((y_training.shape[0], 1)), tx_training]\n",
    "tx_testing = build_poly(tx_testing, degree)\n",
    "tx_testing = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing]\n",
    "\n",
    "w_lsq, loss = least_squares(y_training, tx_training)\n",
    "print(\"weights={w}, loss={l}\".format(w=w_lsq, l=loss))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, w_lsq)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, w_lsq)))\n",
    "print(\"After least squares: \"+str(round(100*np.sum(predict_labels(w_lsq, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "287\n",
      "338\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w_lsq, tx_testing, False)\n",
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_testing==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on all data points from train.csv\n",
    "y_, tX_, ids_ = load_csv_data('../data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_ = np.delete(tX_, (4,5,6,12,26,27,28), axis=1)\n",
    "tX_ = correction_missing_values_test(tX_, col_median_tX)\n",
    "tX_ = remove_outliers_test(tX_, perc_25, perc_75, col_median_tX)\n",
    "#tX_ = (tX_ - min_) / (max_ - min_)\n",
    "tX_ = (tX_ - mean) / std\n",
    "tX_ = build_poly(tX_, degree)\n",
    "tX_ = np.c_[np.ones((tX_.shape[0], 1)), tX_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75823\n",
      "85667\n",
      "Least squares: 80.3456 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w_lsq, tX_, False)\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_==1]))\n",
    "print(\"Least squares: \"+str(round(100*np.sum(y_pred==y_)/len(y_),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss=0.07906238228549702\n",
      "testing loss=0.08644314556672941\n",
      "Processing 1th experiment, degree=1, training rmse=0.3976490469886656, testing rmse=0.08644314556672941\n",
      "training loss=0.07257482112180884\n",
      "testing loss=0.08039932152981177\n",
      "Processing 2th experiment, degree=2, training rmse=0.380985094516331, testing rmse=0.08039932152981177\n",
      "training loss=0.0697153219398077\n",
      "testing loss=0.07828847199031279\n",
      "Processing 3th experiment, degree=3, training rmse=0.3734041294356764, testing rmse=0.07828847199031279\n",
      "training loss=0.06881541109575426\n",
      "testing loss=0.07782891476206491\n",
      "Processing 4th experiment, degree=4, training rmse=0.3709862830233869, testing rmse=0.07782891476206491\n",
      "training loss=30.954725931273213\n",
      "testing loss=34.610948733122264\n",
      "Processing 5th experiment, degree=5, training rmse=7.8682559606653895, testing rmse=34.610948733122264\n",
      "training loss=5.199442406434914\n",
      "testing loss=5.231264891349185\n",
      "Processing 6th experiment, degree=6, training rmse=3.2247301922594747, testing rmse=5.231264891349185\n",
      "training loss=5.810118049533278\n",
      "testing loss=5.6879516296408505\n",
      "Processing 7th experiment, degree=7, training rmse=3.4088467403311866, testing rmse=5.6879516296408505\n",
      "training loss=1.193676713602778\n",
      "testing loss=1.4272511852704153\n",
      "Processing 8th experiment, degree=8, training rmse=1.5451062834658191, testing rmse=1.4272511852704153\n"
     ]
    }
   ],
   "source": [
    "#choosing polynomial degree\n",
    "def polynomial_regression():\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        tx_training_ = build_poly(tx_training, degree)\n",
    "        tx_training_ = np.c_[np.ones((y_training.shape[0], 1)), tx_training_]\n",
    "        tx_testing_ = build_poly(tx_testing, degree)\n",
    "        tx_testing_ = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing_]\n",
    "\n",
    "        weights, mse = least_squares(y_training, tx_training_)\n",
    "        print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training_, weights)))\n",
    "        print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing_, weights)))\n",
    "        rmse = np.sqrt(2 * mse)\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, training rmse={tr_loss}, testing rmse={te_loss}\".format(\n",
    "              i=ind + 1, d=degree, tr_loss=rmse, te_loss=compute_loss(y_testing, tx_testing_, weights)))\n",
    "        \n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_least_squares(y, x, k_indices, k, degree):\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "\n",
    "    y_testing = y[te_indice]\n",
    "    y_training = y[tr_indice]\n",
    "    tx_testing = tX[te_indice]\n",
    "    tx_training = tX[tr_indice]\n",
    "\n",
    "    tx_training = np.delete(tx_training, (4,5,6,12,26,27,28), axis=1)\n",
    "    tx_training, col_median_tX = correction_missing_values(tx_training)\n",
    "    tx_training, perc_25, perc_75, col_median_tX = remove_outliers(tx_training)\n",
    "    tx_training, max_, min_ = normalize(tx_training)\n",
    "    tx_training, mean, std = standardize(tx_training)\n",
    "    \n",
    "    tx_testing = np.delete(tx_testing, (4,5,6,12,26,27,28), axis=1)\n",
    "    tx_testing = correction_missing_values_test(tx_testing, col_median_tX)\n",
    "    tx_testing = remove_outliers_test(tx_testing, perc_25, perc_75, col_median_tX)\n",
    "    tx_testing = (tx_testing - min_) / (max_ - min_)\n",
    "    tx_testing = (tx_testing - mean) / std\n",
    "    \n",
    "    tx_training_ = build_poly(tx_training, degree)\n",
    "    tx_training_ = np.c_[np.ones((y_training.shape[0], 1)), tx_training_]\n",
    "    tx_testing_ = build_poly(tx_testing, degree)\n",
    "    tx_testing_ = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing_]\n",
    "\n",
    "    w_lsq, loss = least_squares(y_training, tx_training_)\n",
    "\n",
    "    loss_tr = np.sqrt(2 * compute_loss(y_training, tx_training_, w_lsq))\n",
    "    loss_te = np.sqrt(2 * compute_loss(y_testing, tx_testing_, w_lsq))\n",
    "\n",
    "    return loss_tr, loss_te, w_lsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0.3878334931510904)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def best_degree_selection_least_squares(degrees, k_fold, seed = 1):\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    #vary degree\n",
    "    for degree in degrees:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te, _ = cross_validation_least_squares(y, tX, k_indices, k, degree)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "\n",
    "    x_label = 'degree'\n",
    "    cross_validation_visualization(degrees, x_label, rmse_tr, rmse_te)\n",
    "    \n",
    "    ind_best_degree = np.argmin(rmse_te)\n",
    "    return degrees[ind_best_degree], rmse_te[ind_best_degree]\n",
    "\n",
    "best_degree_selection_least_squares(np.arange(1,11), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ridge_regression import *\n",
    "#ridge regression\n",
    "\n",
    "#degree = 2\n",
    "#tx_training = build_poly(tx_training, degree)\n",
    "#tx_training = np.c_[np.ones((y_training.shape[0], 1)), tx_training]\n",
    "#tx_testing = build_poly(tx_testing, degree)\n",
    "#tx_testing = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing]\n",
    "\n",
    "#lambdas = np.logspace(-10, -2, 50)\n",
    "#for ind, lambda_ in enumerate(lambdas):\n",
    "    # ridge regression with a given lambda\n",
    "    #w_ridge, loss = ridge_regression(y_training, tx_training, lambda_)\n",
    "    #print(\"lambda={lam}, training rmse={l_tr}, testing rmse={l_te}\".format(lam=lambda_, l_tr=np.sqrt(2*loss), \n",
    "                                                                                       #l_te=np.sqrt(2*compute_loss(y_testing, tx_testing, w_ridge))))\n",
    "    \n",
    "#print(\"weights={w}, loss={l}\".format(w=w_ridge, l=loss))\n",
    "#print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, w_ridge)))\n",
    "#print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, w_ridge)))\n",
    "\n",
    "#print(\"After ridge regression: \"+str(round(100*np.sum(predict_labels(w_ridge, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda=2.329951810515372e-10, training rmse=0.37002483837927225, testing rmse=0.39364374050304274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ridge_regression import *\n",
    "def cross_validation_ridge_regression(y, x, k_indices, k, lambda_, degree):\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "\n",
    "    y_testing = y[te_indice]\n",
    "    y_training = y[tr_indice]\n",
    "    tx_testing = tX[te_indice]\n",
    "    tx_training = tX[tr_indice]\n",
    "\n",
    "    tx_training = np.delete(tx_training, (4,5,6,12,26,27,28), axis=1)\n",
    "    tx_training, col_median_tX = correction_missing_values(tx_training)\n",
    "    tx_training, perc_25, perc_75, col_median_tX = remove_outliers(tx_training)\n",
    "    tx_training, max_, min_ = normalize(tx_training)\n",
    "    tx_training, mean, std = standardize(tx_training)\n",
    "    \n",
    "    tx_testing = np.delete(tx_testing, (4,5,6,12,26,27,28), axis=1)\n",
    "    tx_testing = correction_missing_values_test(tx_testing, col_median_tX)\n",
    "    tx_testing = remove_outliers_test(tx_testing, perc_25, perc_75, col_median_tX)\n",
    "    tx_testing = (tx_testing - min_) / (max_ - min_)\n",
    "    tx_testing = (tx_testing - mean) / std\n",
    "    \n",
    "    tx_training_ = build_poly(tx_training, degree)\n",
    "    tx_training_ = np.c_[np.ones((y_training.shape[0], 1)), tx_training_]\n",
    "    tx_testing_ = build_poly(tx_testing, degree)\n",
    "    tx_testing_ = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing_]\n",
    "\n",
    "    w_ridge, _ = ridge_regression(y_training, tx_training_, lambda_)\n",
    "\n",
    "    loss_tr = np.sqrt(2 * compute_loss(y_training, tx_training_, w_ridge))\n",
    "    loss_te = np.sqrt(2 * compute_loss(y_testing, tx_testing_, w_ridge))\n",
    "\n",
    "    return loss_tr, loss_te, w_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0.3827734675914003, 6.723357536499335e-10)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_degree_selection_ridge_regression(degrees, lambdas, k_fold, seed = 1):\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    #for each degree, we compute the best lambdas and the associated rmse\n",
    "    best_lambdas = []\n",
    "    best_rmses = []\n",
    "    #vary degree\n",
    "    for degree in degrees:\n",
    "        # cross validation\n",
    "        rmse_te = []\n",
    "        for lambda_ in lambdas:\n",
    "            rmse_te_tmp = []\n",
    "            for k in range(k_fold):\n",
    "                _, loss_te, _ = cross_validation_ridge_regression(y, tX, k_indices, k, lambda_, degree)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            rmse_te.append(np.mean(rmse_te_tmp))\n",
    "        \n",
    "        ind_lambda_opt = np.argmin(rmse_te)\n",
    "        best_lambdas.append(lambdas[ind_lambda_opt])\n",
    "        best_rmses.append(rmse_te[ind_lambda_opt])\n",
    "        \n",
    "    ind_best_degree =  np.argmin(best_rmses)\n",
    "        \n",
    "    return degrees[ind_best_degree], best_rmses[ind_best_degree], best_lambdas[ind_best_degree]\n",
    "\n",
    "best_degree_selection_ridge_regression(np.arange(1,11), np.logspace(-10, -2, 30), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.329951810515372e-10, 0.3827733968108855)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def best_lambda_selection_ridge_regression(degree, lambdas, k_fold, seed = 1):\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # cross validation: TODO\n",
    "    for lambda_ in lambdas:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te, _ = cross_validation_ridge_regression(y, tX, k_indices, k, lambda_, degree)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "\n",
    "    x_label = 'lambda'\n",
    "    cross_validation_visualization(lambdas, x_label, rmse_tr, rmse_te)\n",
    "    ind_best_lambda = np.argmin(rmse_te)\n",
    "    return lambdas[ind_best_lambda], rmse_te[ind_best_lambda]\n",
    "\n",
    "best_lambda_selection_ridge_regression(4, np.logspace(-10, -4, 50), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights=[ 2.97272641e-01  2.53159561e-01  3.95576693e-02 -4.10265667e-02\n",
      " -8.94857750e-03 -2.16164727e-01 -1.67991094e-02  5.62833322e-02\n",
      " -9.39519533e-03  6.24670655e-02 -6.69265434e-02 -1.03605106e-02\n",
      "  4.86122888e-03 -5.38496462e-02 -1.84390100e-03  3.88211772e-03\n",
      "  1.07214395e-03 -4.74537903e-03 -3.66453473e-02 -6.38351841e-03\n",
      "  3.77370722e-03 -2.83579954e-02  8.64163497e-03  7.72484431e-03\n",
      " -1.51218304e-03  1.14560813e-02  1.80415565e-04 -5.67742710e-03\n",
      "  1.06907215e-03 -3.64110833e-02  2.15273240e-02 -3.34954809e-03\n",
      "  1.36209056e-04  6.56039580e-02  1.49318509e-01 -6.98864641e-03\n",
      " -7.86525197e-02  5.33045021e-02  1.76518719e-02 -1.43456980e-02\n",
      "  1.55662070e-03 -3.82099424e-03 -1.59039827e-02 -6.81440388e-04\n",
      " -3.52938108e-04  4.05536656e-03  1.37950836e-02 -1.22384157e-03\n",
      " -7.31066217e-03 -1.21095098e-02  5.60115622e-03 -5.19500889e-03\n",
      "  9.33688040e-04  4.97796708e-03 -1.78083169e-02 -5.39032950e-03\n",
      " -3.63483954e-03  1.72855545e-02  1.05118246e-02 -3.40856869e-03\n",
      " -5.78573564e-03  2.28698019e-02  1.27849893e-02 -1.22091008e-02\n",
      "  2.15236770e-03  2.89856670e-02  4.32455943e-03 -1.61411925e-02\n",
      " -1.22609056e-04 -3.32498566e-03 -4.61824903e-03  1.46854868e-03\n",
      " -3.24770848e-06  1.70277996e-01 -3.37515981e-01  2.81986858e-01\n",
      " -8.22025521e-02 -3.71084091e-02 -1.23534712e-02  3.25671759e-02\n",
      " -7.43230068e-03  2.83937407e-03  4.16854385e-02 -6.14933390e-04\n",
      " -1.90186159e-03  1.04151886e-02  6.13167638e-03 -9.76865565e-04\n",
      " -2.38883913e-04 -2.45097543e-01  5.33520069e-01 -3.18294895e-01\n",
      "  5.90478202e-02], loss=0.06845919050831176\n",
      "training loss=0.06845919050831176\n",
      "testing loss=0.07747771519100678\n",
      "After ridge regression: 78.8 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ridge_regression import *\n",
    "#ridge regression\n",
    "\n",
    "degree = 4\n",
    "tx_training = build_poly(tx_training, degree)\n",
    "tx_training = np.c_[np.ones((y_training.shape[0], 1)), tx_training]\n",
    "tx_testing = build_poly(tx_testing, degree)\n",
    "tx_testing = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing]\n",
    "\n",
    "#w_ridge, loss = ridge_regression(y_training, tx_training, 0.0012067926406393288)\n",
    "w_ridge, loss = ridge_regression(y_training, tx_training, 2.329951810515372e-10)\n",
    "print(\"weights={w}, loss={l}\".format(w=w_ridge, l=loss))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, w_ridge)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, w_ridge)))\n",
    "print(\"After ridge regression: \"+str(round(100*np.sum(predict_labels(w_ridge, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(w_ridge, tx_testing, False)\n",
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_testing==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "#test on all data points from train.csv\n",
    "y_, tX_, ids_ = load_csv_data('../data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_ = np.delete(tX_, (4,5,6,12,26,27,28), axis=1)\n",
    "tX_ = correction_missing_values_test(tX_, col_median_tX)\n",
    "tX_ = remove_outliers_test(tX_, perc_25, perc_75, col_median_tX)\n",
    "#tX_ = (tX_ - min_) / (max_ - min_)\n",
    "tX_ = (tX_ - mean) / std\n",
    "tX_ = build_poly(tX_, degree)\n",
    "tX_ = np.c_[np.ones((tX_.shape[0], 1)), tX_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74017\n",
      "85667\n",
      "Ridge regression: 80.4312 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w_ridge, tX_, False)\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_==1]))\n",
    "print(\"Ridge regression: \"+str(round(100*np.sum(y_pred==y_)/len(y_),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import *\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_ws, gradient_losses= gradient_descent(y_training, tx_training, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=gradient_ws, l=gradient_losses))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, gradient_ws)))\n",
    "print(\"After gradient descent: \"+str(round(100*np.sum(predict_labels(gradient_ws, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=26.950348424835816, testing loss=12.851884235183869\n",
      "Current iteration=500, training loss=0.06984341421487937, testing loss=0.0783533954319692\n",
      "Current iteration=1000, training loss=0.06974060763748247, testing loss=0.07835845842213904\n",
      "Current iteration=1500, training loss=0.06973059243824471, testing loss=0.07836371245544836\n",
      "Current iteration=2000, training loss=0.06972681840811316, testing loss=0.07835774279792841\n",
      "Current iteration=2500, training loss=0.06972423221687665, testing loss=0.07834955658539292\n",
      "Current iteration=3000, training loss=0.06972225203087797, testing loss=0.07834168913596307\n",
      "Current iteration=3500, training loss=0.0697207141645145, testing loss=0.07833470773719328\n",
      "Current iteration=4000, training loss=0.06971951777683436, testing loss=0.07832865028739691\n",
      "Current iteration=4500, training loss=0.06971858685429363, testing loss=0.0783234247470055\n",
      "Current iteration=5000, training loss=0.06971786247559236, testing loss=0.07831891865972682\n",
      "Current iteration=5500, training loss=0.06971729881321206, testing loss=0.07831502780913721\n",
      "Current iteration=6000, training loss=0.06971686020922295, testing loss=0.07831166209684343\n",
      "Current iteration=6500, training loss=0.06971651891721228, testing loss=0.07830874519573357\n",
      "Current iteration=7000, training loss=0.06971625334681161, testing loss=0.07830621274904202\n",
      "Current iteration=7500, training loss=0.06971604669785546, testing loss=0.07830401045786095\n",
      "Current iteration=8000, training loss=0.0697158858975894, testing loss=0.07830209239523243\n",
      "Current iteration=8500, training loss=0.06971576077367855, testing loss=0.07830041959653197\n",
      "Current iteration=9000, training loss=0.06971566341069874, testing loss=0.07829895889964306\n",
      "Current iteration=9500, training loss=0.06971558764940115, testing loss=0.07829768199399663\n",
      "Current iteration=10000, training loss=0.06971552869707431, testing loss=0.07829656463999911\n",
      "Current iteration=10500, training loss=0.06971548282435361, testing loss=0.0782955860264849\n",
      "Current iteration=11000, training loss=0.06971544712929943, testing loss=0.07829472823991426\n",
      "Current iteration=11500, training loss=0.06971541935381817, testing loss=0.07829397582421675\n",
      "Current iteration=12000, training loss=0.06971539774081387, testing loss=0.07829331541438443\n",
      "Current iteration=12500, training loss=0.0697153809230331, testing loss=0.07829273543027623\n",
      "Current iteration=13000, training loss=0.06971536783657155, testing loss=0.07829222581976131\n",
      "Current iteration=13500, training loss=0.06971535765357066, testing loss=0.07829177784244738\n",
      "Current iteration=14000, training loss=0.06971534972984685, testing loss=0.07829138388692337\n",
      "Current iteration=14500, training loss=0.06971534356413997, testing loss=0.07829103731578631\n",
      "Current iteration=15000, training loss=0.06971533876640314, testing loss=0.07829073233379262\n",
      "Current iteration=15500, training loss=0.06971533503312824, testing loss=0.07829046387533074\n",
      "Current iteration=16000, training loss=0.06971533212814575, testing loss=0.07829022750809872\n",
      "Current iteration=16500, training loss=0.06971532986768451, testing loss=0.07829001935042441\n",
      "Current iteration=17000, training loss=0.0697153281087462, testing loss=0.07828983600011256\n",
      "Current iteration=17500, training loss=0.06971532674005915, testing loss=0.07828967447306448\n",
      "Current iteration=18000, training loss=0.0697153256750393, testing loss=0.07828953215021173\n",
      "Current iteration=18500, training loss=0.06971532484631275, testing loss=0.07828940673154378\n",
      "Current iteration=19000, training loss=0.06971532420145368, testing loss=0.07828929619620774\n",
      "Current iteration=19500, training loss=0.06971532369966792, testing loss=0.07828919876781974\n",
      "weights=[ 3.50000000e-01  2.09399017e-01 -5.40752064e-03 -1.11999844e-01\n",
      " -1.83391312e-01 -3.12002503e-02  9.71016059e-02  4.52296623e-02\n",
      " -7.30915704e-02 -1.43360709e-02 -5.95385286e-02 -5.77717157e-03\n",
      "  3.22571398e-02 -2.41990171e-02 -2.26470071e-02 -8.21228901e-04\n",
      " -1.85256426e-02  7.82157701e-02 -5.86843704e-02  5.51068572e-03\n",
      "  6.33609139e-03 -1.50063329e-02 -2.29518733e-02  4.61601104e-02\n",
      " -3.30973822e-02  5.86465279e-02  1.65170964e-02 -1.40617601e-02\n",
      "  5.87041346e-02 -9.40956520e-03 -2.72101125e-02 -4.71910319e-03\n",
      " -1.83824506e-02 -1.17092055e-03  7.56176090e-03 -7.57741631e-03\n",
      " -5.06491274e-03 -2.59784581e-02  7.54274298e-03  4.70960295e-03\n",
      "  2.79051415e-03 -3.10695641e-02 -9.51379367e-03  1.80002637e-02\n",
      " -5.74860017e-03 -4.98682021e-03  3.77381841e-03  1.91436889e-02\n",
      " -1.12755678e-02  3.42647513e-02  1.31993895e-03 -3.51455621e-02\n",
      "  3.49802847e-04 -1.37699764e-02  6.52352696e-03  7.59508186e-02\n",
      "  2.70913578e-02 -1.02754351e-01  2.57784799e-02 -3.17081781e-02\n",
      "  2.56587873e-02 -1.41527498e-03  5.00530721e-02  2.36537942e-03\n",
      "  1.49245672e-02  4.79180403e-03 -7.14482343e-03 -1.69700751e-02\n",
      "  5.06547374e-02 -3.58589901e-02], training loss=0.06971532330989935\n",
      "training loss=0.06971532330921211\n",
      "testing loss=0.07828911304544788\n",
      "After gradient descent: 78.8 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import *\n",
    "\n",
    "degree = 3\n",
    "tx_training = build_poly(tx_training, degree)\n",
    "tx_training, mean_, std_ = standardize(tx_training)\n",
    "tx_training = np.c_[np.ones((y_training.shape[0], 1)), tx_training]\n",
    "tx_testing = build_poly(tx_testing, degree)\n",
    "tx_testing = (tx_testing - mean_) / std_\n",
    "tx_testing = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing]\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 20000\n",
    "gamma = 0.25\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tx_training.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_ws, gradient_losses= gradient_descent(y_training, tx_training, y_testing, tx_testing, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=gradient_ws, l=gradient_losses))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, gradient_ws)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, gradient_ws)))\n",
    "print(\"After gradient descent: \"+str(round(100*np.sum(predict_labels(gradient_ws, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(gradient_ws, tx_testing, False)\n",
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_testing==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on all data points from train.csv\n",
    "y_, tX_, ids_ = load_csv_data('../data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_ = np.delete(tX_, (4,5,6,12,26,27,28), axis=1)\n",
    "tX_ = correction_missing_values_test(tX_, col_median_tX)\n",
    "tX_ = remove_outliers_test(tX_, perc_25, perc_75, col_median_tX)\n",
    "tX_ = (tX_ - min_) / (max_ - min_)\n",
    "tX_ = (tX_ - mean) / std\n",
    "tX_ = build_poly(tX_, degree)\n",
    "tX_ = (tX_ - mean_) / std_\n",
    "tX_ = np.c_[np.ones((tX_.shape[0], 1)), tX_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75821\n",
      "85667\n",
      "Gradient descent: 80.3456 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(gradient_ws, tX_, False)\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_==1]))\n",
    "print(\"Gradient descent: \"+str(round(100*np.sum(y_pred==y_)/len(y_),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=10.684155249537138\n",
      "Current iteration=100, loss=0.11611652153666797\n",
      "Current iteration=200, loss=0.14554730629189477\n",
      "Current iteration=300, loss=0.15729584012939785\n",
      "Current iteration=400, loss=0.14270265710658306\n",
      "Current iteration=500, loss=0.14298708683238764\n",
      "Current iteration=600, loss=0.10556115934044442\n",
      "Current iteration=700, loss=0.10730606578847834\n",
      "Current iteration=800, loss=0.14551074619988377\n",
      "Current iteration=900, loss=0.21453318010213285\n",
      "Current iteration=1000, loss=0.11551455058310207\n",
      "Current iteration=1100, loss=0.16175518591596363\n",
      "Current iteration=1200, loss=0.1205090926539504\n",
      "Current iteration=1300, loss=0.12327826509625146\n",
      "Current iteration=1400, loss=0.10193669583300485\n",
      "Current iteration=1500, loss=0.0981311189563209\n",
      "Current iteration=1600, loss=0.10670949634004609\n",
      "Current iteration=1700, loss=0.10871143412603702\n",
      "Current iteration=1800, loss=0.09593688576563207\n",
      "Current iteration=1900, loss=0.09624031797805332\n",
      "Current iteration=2000, loss=0.14465157112595062\n",
      "Current iteration=2100, loss=0.11531081540132122\n",
      "Current iteration=2200, loss=0.12581218995274956\n",
      "Current iteration=2300, loss=0.09500207807191174\n",
      "Current iteration=2400, loss=0.12523628169823547\n",
      "Current iteration=2500, loss=0.12174955822264694\n",
      "Current iteration=2600, loss=0.10160210166529182\n",
      "Current iteration=2700, loss=0.15615198040815262\n",
      "Current iteration=2800, loss=0.18584399019535516\n",
      "Current iteration=2900, loss=0.0972637838642432\n",
      "Current iteration=3000, loss=0.10865722189086531\n",
      "Current iteration=3100, loss=0.11356751220163719\n",
      "Current iteration=3200, loss=0.09288599100999133\n",
      "Current iteration=3300, loss=0.13428916213334555\n",
      "Current iteration=3400, loss=0.19894353148093608\n",
      "Current iteration=3500, loss=0.11667871788847228\n",
      "Current iteration=3600, loss=0.09954323456578638\n",
      "Current iteration=3700, loss=0.13818435891922054\n",
      "Current iteration=3800, loss=0.21580122317832234\n",
      "Current iteration=3900, loss=0.0959180464846572\n",
      "Current iteration=4000, loss=0.08985200046465622\n",
      "Current iteration=4100, loss=0.08982641574038834\n",
      "Current iteration=4200, loss=0.11899354662930639\n",
      "Current iteration=4300, loss=0.09148029602972145\n",
      "Current iteration=4400, loss=0.0958351315616195\n",
      "Current iteration=4500, loss=0.09805403685822828\n",
      "Current iteration=4600, loss=0.08936670407373677\n",
      "Current iteration=4700, loss=0.10812971209843943\n",
      "Current iteration=4800, loss=0.25566428874790276\n",
      "Current iteration=4900, loss=0.09245414306084061\n",
      "weights=[-7.56840847e-01  2.13088766e-01 -1.41284105e+00 -1.61766921e-01\n",
      "  5.21453761e-01  7.59636439e-01  1.41150350e-01 -5.92373135e-01\n",
      "  3.14893809e-01 -7.70022195e-03 -2.74330821e-01 -7.22916502e-01\n",
      "  1.55342814e+00  7.01136737e-01  1.03249140e+00  4.39626916e-01\n",
      "  1.62270978e-01 -7.86730428e-04 -8.92054353e-02  5.89802809e-01\n",
      "  1.81253515e-01 -1.19404676e-01  1.84092815e-01  2.27113752e-01\n",
      "  1.45230550e-01 -1.34219555e-01  8.51293647e-02 -1.88963510e-01\n",
      " -3.93527489e-02 -1.33615058e-01  2.38569442e-01], loss=0.08914888593335403\n",
      "training loss=0.08930297820116996\n",
      "testing loss=0.08823516920672592\n",
      "After stochastic gradient descent: 72.7 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_ws, sgd_losses = stochastic_gradient_descent(y_training, tx_training, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=sgd_ws, l=sgd_losses))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, sgd_ws)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, sgd_ws)))\n",
    "print(\"After stochastic gradient descent: \"+str(round(100*np.sum(predict_labels(sgd_ws, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test = np.delete(tX_test, (4,5,6,12,26,27,28), axis=1)\n",
    "tX_test = correction_missing_values_test(tX_test, col_median_tX)\n",
    "tX_test = remove_outliers_test(tX_test, perc_25, perc_75, col_median_tX)\n",
    "tX_test = (tX_test - min_) / (max_ - min_)\n",
    "tX_test = (tX_test - mean) / std\n",
    "tX_test = build_poly(tX_test, degree)\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#OUTPUT_PATH = 'test_ridge_deg4_without_norm.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_ridge, tX_test, False)\n",
    "y_pred[np.where(y_pred == 0)] = -1\n",
    "#create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168290\n",
      "399948\n"
     ]
    }
   ],
   "source": [
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_pred==-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
