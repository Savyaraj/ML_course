{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = correction_missing_values(tX)\n",
    "tX, mean_x, std_x = standardize(tX)\n",
    "tX = normalize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=13202.974980721261, testing loss=2707.455838522011\n",
      "Current iteration=1000, training loss=2319.486070705529, testing loss=578.2460366938533\n",
      "Current iteration=2000, training loss=2221.9820996961935, testing loss=553.0657058362972\n",
      "Current iteration=3000, training loss=2168.3610518956666, testing loss=538.8621953280376\n",
      "Current iteration=4000, training loss=2134.493751755974, testing loss=529.723206455981\n",
      "Current iteration=5000, training loss=2111.230406923831, testing loss=523.3833618045206\n",
      "Current iteration=6000, training loss=2094.2792969131915, testing loss=518.7583848867007\n",
      "Current iteration=7000, training loss=2081.3499343698713, testing loss=515.2542817145379\n",
      "Current iteration=8000, training loss=2071.116476250776, testing loss=512.5173755946313\n",
      "Current iteration=9000, training loss=2062.766467742662, testing loss=510.32471612092024\n",
      "[ -2.28326064  -0.34200037 -11.05624085  -1.3402781    2.77177594\n",
      "   2.59045558   1.15707665  -2.6932996    3.65981466  -2.01679605\n",
      "   1.11737199  -4.38793659   6.79480502   3.68771099   6.0767472\n",
      "   0.91284344   0.13385548   1.34736439   0.66233705   0.86811358\n",
      "  -0.81616513  -0.51427059   0.63417722   0.80929958   0.29411653\n",
      "  -1.60755207  -0.69408192  -2.59062634   0.05519813  -0.99274029\n",
      "   0.41120872] 2055.785946261586\n",
      "training loss=2055.779514374804\n",
      "testing loss=508.53118590393285\n",
      "Accuracy from our code:\n",
      "\n",
      "Logistic regression: 72.0 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from logistic_regression import *\n",
    "#Using our code\n",
    "#Logistic regression\n",
    "\n",
    "gamma = 0.0001\n",
    "n_iter = 10000\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w = np.random.rand(tX.shape[1])\n",
    "for i in range(n_iter):\n",
    "    w, loss = learning_by_gradient_descent(y_training, tx_training, w, gamma)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Current iteration={iter}, training loss={l_tr}, testing loss={l_te}\".format(iter=i, l_tr=loss, \n",
    "                                                                                                         l_te=calculate_loss(y_testing, tx_testing, w)))\n",
    "print(w, loss)\n",
    "print(\"training loss={l_tr}\".format(l_tr=calculate_loss(y_training, tx_training, w)))\n",
    "print(\"testing loss={l_te}\".format(l_te=calculate_loss(y_testing, tx_testing, w)))\n",
    "\n",
    "print(\"Accuracy from our code:\\n\")\n",
    "print(\"Logistic regression: \"+str(round(100*np.sum(predict_labels(w, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=13204.066997849734, testing loss=2707.391303628788\n",
      "Current iteration=1000, training loss=2323.237518955365, testing loss=578.5833774545952\n",
      "Current iteration=2000, training loss=2230.9237714691308, testing loss=553.8627539355654\n",
      "Current iteration=3000, training loss=2182.2360575977823, testing loss=540.0583366568821\n",
      "Current iteration=4000, training loss=2152.7775161649447, testing loss=531.2539086419839\n",
      "Current iteration=5000, training loss=2133.40358979678, testing loss=525.1918822733754\n",
      "Current iteration=6000, training loss=2119.889748543378, testing loss=520.7982865514483\n",
      "Current iteration=7000, training loss=2110.022044900256, testing loss=517.489196586293\n",
      "Current iteration=8000, training loss=2102.543579117453, testing loss=514.9194301449003\n",
      "Current iteration=9000, training loss=2096.698912410284, testing loss=512.8726934446728\n",
      "[ -2.10811681  -0.33184585 -10.30199765  -1.2324899    2.62263618\n",
      "   2.44060578   1.13540108  -2.59279571   3.28471398  -1.87757786\n",
      "   1.14260951  -4.14548518   6.33254882   3.45140548   5.63167344\n",
      "   0.79872133   0.078839     1.15096776   0.61441354   0.75898884\n",
      "  -0.76368397  -0.47705132   0.65287725   0.8279343    0.28006468\n",
      "  -1.5004022   -0.70023846  -2.42117963   0.0375839   -0.95546458\n",
      "   0.43750443] 2092.0172992918197\n",
      "training loss=2065.591265080149\n",
      "testing loss=511.2087925753038\n",
      "After regularization: 71.3 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from reg_logistic_regression import *\n",
    "#Reguralized logistic regression\n",
    "\n",
    "gamma = 0.0001\n",
    "lamda_ = 0.1\n",
    "n_iter = 10000\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w_reg = np.random.rand(tX.shape[1])\n",
    "for i in range(n_iter):\n",
    "    w_reg, loss = learning_by_penalized_gradient(y_training, tx_training, w_reg, gamma, lamda_)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Current iteration={iter}, training loss={l_tr}, testing loss={l_te}\".format(iter=i, l_tr=loss, \n",
    "                                                                                                         l_te=calculate_loss(y_testing, tx_testing, w_reg)))\n",
    "print(w_reg, loss)\n",
    "print(\"training loss={l_tr}\".format(l_tr=calculate_loss(y_training, tx_training, w_reg)))\n",
    "print(\"testing loss={l_te}\".format(l_te=calculate_loss(y_testing, tx_testing, w_reg)))\n",
    "\n",
    "print(\"After regularization: \"+str(round(100*np.sum(predict_labels(w_reg, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights=[-3.36066135e+03  4.93617329e-02 -3.13631732e+00 -2.72965654e+00\n",
      " -7.00355544e-01  4.23332738e-01  3.33081824e-01 -3.00803568e-01\n",
      "  2.86701353e+00 -2.49617854e-01 -5.31349592e+04 -1.78070042e+00\n",
      "  1.53263823e+00  9.97322365e-01  9.51532465e+03  1.37178942e-01\n",
      "  9.39604101e-02  9.62015906e+03  2.77519998e-01  2.30665883e-01\n",
      "  1.37176864e+00 -9.77410894e-02 -5.02513386e-01  3.09587984e-01\n",
      " -3.43462834e-01 -4.03004387e-01 -7.22579053e-02 -6.47831367e-01\n",
      "  3.56178937e-02 -1.54837693e-01  4.55817932e+04], loss=0.08226957578474213\n",
      "training loss=0.08226957578474213\n",
      "testing loss=0.08055938490307227\n",
      "After least squares: 76.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "#least squares\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w_lsq, loss = least_squares(y_training, tx_training)\n",
    "print(\"weights={w}, loss={l}\".format(w=w_lsq, l=loss))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, w_lsq)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, w_lsq)))\n",
    "print(\"After least squares: \"+str(round(100*np.sum(predict_labels(w_lsq, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th experiment, degree=1, rmse=11.321584492339047\n",
      "Processing 2th experiment, degree=3, rmse=11.321584492339047\n",
      "Processing 3th experiment, degree=7, rmse=11.321584492339047\n",
      "Processing 4th experiment, degree=12, rmse=11.321584492339047\n"
     ]
    }
   ],
   "source": [
    "def polynomial_regression():\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 3, 7, 12]\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        tx = build_poly(tX, degree)\n",
    "        weights, mse = least_squares(y, tX)\n",
    "        rmse = np.sqrt(2 * mse)\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, rmse={loss}\".format(\n",
    "              i=ind + 1, d=degree, loss=rmse))\n",
    "        \n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.000, training loss=0.083, testing loss=0.081\n",
      "lambda=0.000, training loss=0.083, testing loss=0.081\n",
      "lambda=0.000, training loss=0.084, testing loss=0.081\n",
      "lambda=0.000, training loss=0.086, testing loss=0.082\n",
      "lambda=0.000, training loss=0.089, testing loss=0.084\n",
      "lambda=0.001, training loss=0.092, testing loss=0.086\n",
      "lambda=0.001, training loss=0.096, testing loss=0.090\n",
      "lambda=0.003, training loss=0.101, testing loss=0.095\n",
      "lambda=0.007, training loss=0.105, testing loss=0.100\n",
      "lambda=0.016, training loss=0.107, testing loss=0.104\n",
      "lambda=0.037, training loss=0.110, testing loss=0.107\n",
      "lambda=0.085, training loss=0.112, testing loss=0.108\n",
      "lambda=0.193, training loss=0.115, testing loss=0.109\n",
      "lambda=0.439, training loss=0.121, testing loss=0.111\n",
      "lambda=1.000, training loss=0.130, testing loss=0.116\n",
      "weights=[0.05898849 0.01702444 0.01349676 0.01675819 0.01916236 0.01891701\n",
      " 0.01868751 0.01543015 0.01700008 0.01694654 0.01876702 0.01527165\n",
      " 0.02002863 0.01888765 0.01951832 0.01725549 0.01712848 0.01684741\n",
      " 0.0173858  0.01740115 0.01739162 0.01723937 0.01856934 0.01865364\n",
      " 0.01810351 0.01680362 0.01675715 0.01686174 0.01719905 0.01671958\n",
      " 0.01859722], loss=0.12956722492822362\n",
      "training loss=0.1168530151919449\n",
      "testing loss=0.11587231092601533\n",
      "After ridge regression: 67.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ridge_regression import ridge_regression\n",
    "#ridge regression\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    # ridge regression with a given lambda\n",
    "    weight_ridge, loss = ridge_regression(y_training, tx_training, lambda_)\n",
    "    print(\"lambda={lam:.3f}, training loss={l_tr:.3f}, testing loss={l_te:.3f}\".format(lam=lambda_, l_tr=loss, \n",
    "                                                                                       l_te=compute_loss(y_testing, tx_testing, weight_ridge)))\n",
    "    \n",
    "print(\"weights={w}, loss={l}\".format(w=weight_ridge, l=loss))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, weight_ridge)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, weight_ridge)))\n",
    "\n",
    "print(\"After ridge regression: \"+str(round(100*np.sum(predict_labels(weight_ridge, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=10.68415524953714\n",
      "Current iteration=1000, loss=0.1006983499804822\n",
      "Current iteration=2000, loss=0.09432945739651882\n",
      "Current iteration=3000, loss=0.09105909807248724\n",
      "Current iteration=4000, loss=0.08919778187398651\n",
      "weights=[-0.67632045  0.1963382  -1.51889326 -0.17343314  0.47233771  0.7712353\n",
      "  0.18353181 -0.526078    0.31597263 -0.11399315 -0.23923952 -0.58794283\n",
      "  1.53959726  0.62252622  1.06487568  0.40417022  0.22069267  0.16713433\n",
      " -0.04947574  0.48755469 -0.09969004 -0.036905    0.15744464  0.23181153\n",
      "  0.16792303 -0.20231044  0.12625754 -0.20213052  0.00415655 -0.16511453\n",
      "  0.24322293], loss=0.08803858500144947\n",
      "training loss=0.08803764894542244\n",
      "testing loss=0.08708009963533041\n",
      "After gradient descent: 74.4 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import *\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_ws, gradient_losses= gradient_descent(y_training, tx_training, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=gradient_ws, l=gradient_losses))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, gradient_ws)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, gradient_ws)))\n",
    "print(\"After gradient descent: \"+str(round(100*np.sum(predict_labels(gradient_ws, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=10.684155249537138\n",
      "Current iteration=100, loss=0.11611652153666797\n",
      "Current iteration=200, loss=0.14554730629189477\n",
      "Current iteration=300, loss=0.15729584012939785\n",
      "Current iteration=400, loss=0.14270265710658306\n",
      "Current iteration=500, loss=0.14298708683238764\n",
      "Current iteration=600, loss=0.10556115934044442\n",
      "Current iteration=700, loss=0.10730606578847834\n",
      "Current iteration=800, loss=0.14551074619988377\n",
      "Current iteration=900, loss=0.21453318010213285\n",
      "Current iteration=1000, loss=0.11551455058310207\n",
      "Current iteration=1100, loss=0.16175518591596363\n",
      "Current iteration=1200, loss=0.1205090926539504\n",
      "Current iteration=1300, loss=0.12327826509625146\n",
      "Current iteration=1400, loss=0.10193669583300485\n",
      "Current iteration=1500, loss=0.0981311189563209\n",
      "Current iteration=1600, loss=0.10670949634004609\n",
      "Current iteration=1700, loss=0.10871143412603702\n",
      "Current iteration=1800, loss=0.09593688576563207\n",
      "Current iteration=1900, loss=0.09624031797805332\n",
      "Current iteration=2000, loss=0.14465157112595062\n",
      "Current iteration=2100, loss=0.11531081540132122\n",
      "Current iteration=2200, loss=0.12581218995274956\n",
      "Current iteration=2300, loss=0.09500207807191174\n",
      "Current iteration=2400, loss=0.12523628169823547\n",
      "Current iteration=2500, loss=0.12174955822264694\n",
      "Current iteration=2600, loss=0.10160210166529182\n",
      "Current iteration=2700, loss=0.15615198040815262\n",
      "Current iteration=2800, loss=0.18584399019535516\n",
      "Current iteration=2900, loss=0.0972637838642432\n",
      "Current iteration=3000, loss=0.10865722189086531\n",
      "Current iteration=3100, loss=0.11356751220163719\n",
      "Current iteration=3200, loss=0.09288599100999133\n",
      "Current iteration=3300, loss=0.13428916213334555\n",
      "Current iteration=3400, loss=0.19894353148093608\n",
      "Current iteration=3500, loss=0.11667871788847228\n",
      "Current iteration=3600, loss=0.09954323456578638\n",
      "Current iteration=3700, loss=0.13818435891922054\n",
      "Current iteration=3800, loss=0.21580122317832234\n",
      "Current iteration=3900, loss=0.0959180464846572\n",
      "Current iteration=4000, loss=0.08985200046465622\n",
      "Current iteration=4100, loss=0.08982641574038834\n",
      "Current iteration=4200, loss=0.11899354662930639\n",
      "Current iteration=4300, loss=0.09148029602972145\n",
      "Current iteration=4400, loss=0.0958351315616195\n",
      "Current iteration=4500, loss=0.09805403685822828\n",
      "Current iteration=4600, loss=0.08936670407373677\n",
      "Current iteration=4700, loss=0.10812971209843943\n",
      "Current iteration=4800, loss=0.25566428874790276\n",
      "Current iteration=4900, loss=0.09245414306084061\n",
      "weights=[-7.56840847e-01  2.13088766e-01 -1.41284105e+00 -1.61766921e-01\n",
      "  5.21453761e-01  7.59636439e-01  1.41150350e-01 -5.92373135e-01\n",
      "  3.14893809e-01 -7.70022195e-03 -2.74330821e-01 -7.22916502e-01\n",
      "  1.55342814e+00  7.01136737e-01  1.03249140e+00  4.39626916e-01\n",
      "  1.62270978e-01 -7.86730428e-04 -8.92054353e-02  5.89802809e-01\n",
      "  1.81253515e-01 -1.19404676e-01  1.84092815e-01  2.27113752e-01\n",
      "  1.45230550e-01 -1.34219555e-01  8.51293647e-02 -1.88963510e-01\n",
      " -3.93527489e-02 -1.33615058e-01  2.38569442e-01], loss=0.08914888593335403\n",
      "training loss=0.08930297820116996\n",
      "testing loss=0.08823516920672592\n",
      "After stochastic gradient descent: 72.7 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_ws, sgd_losses = stochastic_gradient_descent(y_training, tx_training, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=sgd_ws, l=sgd_losses))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, sgd_ws)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, sgd_ws)))\n",
    "print(\"After stochastic gradient descent: \"+str(round(100*np.sum(predict_labels(sgd_ws, tx_testing)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test = correction_missing_values(tX_test)\n",
    "tX_test, _, _ = standardize(tX_test)\n",
    "tX_test = normalize(tX_test)\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'test_reg_log_regr.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w_reg, tX_test)\n",
    "y_pred[np.where(y_pred == 0)] = -1\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
