{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38114, 0, 0, 0, 177457, 177457, 177457, 0, 0, 0, 0, 0, 177457, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 99913, 99913, 99913, 177457, 177457, 177457, 0]\n"
     ]
    }
   ],
   "source": [
    "print([np.sum(tX[:,i]==-999) for i in range(tX.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH, sub_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tX.shape)\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "seed = 6\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_training = np.delete(tx_training, (4,5,6,12,26,27,28), axis=1)\n",
    "tx_training, col_median_tX = correction_missing_values(tx_training)\n",
    "#print([(np.max(tX[:,i]),np.min(tX[:,i]),np.mean(tX[:,i]),np.median(tX[:,i])) for i in range(tX.shape[1])])\n",
    "#plt.boxplot(tx_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_training, perc_25, perc_75, col_median_tX = remove_outliers(tx_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.boxplot(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_training, max_, min_ = normalize(tx_training)\n",
    "tx_training, mean, std = standardize(tx_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_testing = np.delete(tx_testing, (4,5,6,12,26,27,28), axis=1)\n",
    "tx_testing = correction_missing_values_test(tx_testing, col_median_tX)\n",
    "tx_testing = remove_outliers_test(tx_testing, perc_25, perc_75, col_median_tX)\n",
    "tx_testing = (tx_testing - min_) / (max_ - min_)\n",
    "tx_testing = (tx_testing - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from scikit learn - logistic regression: 81.8 %\n",
      "\n",
      "Accuracy from scikit learn - Support Vector Machine: 93.6 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_logistic = LogisticRegression(random_state=0).fit(tx_training, y_training)\n",
    "clf_SVC = SVC(gamma='auto').fit(tx_training, y_training)\n",
    "print(\"Accuracy from scikit learn - logistic regression: \"+str(round(100*clf_logistic.score(tx_training, y_training),5))+' %\\n')\n",
    "print(\"Accuracy from scikit learn - Support Vector Machine: \"+str(round(100*clf_SVC.score(tx_training, y_training),5))+' %\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=5521.413283753469, testing loss=1260.454865442583\n",
      "Current iteration=10, training loss=2831.4255880359387, testing loss=687.4562267101987\n",
      "Current iteration=20, training loss=2204.258403944562, testing loss=563.861775675578\n",
      "Current iteration=30, training loss=2019.4559514421726, testing loss=529.3800951470937\n",
      "Current iteration=40, training loss=1947.8456766483139, testing loss=517.0578460250651\n",
      "Current iteration=50, training loss=1915.3428246331996, testing loss=512.0994962269394\n",
      "Current iteration=60, training loss=1898.8732012918622, testing loss=510.01874568679307\n",
      "Current iteration=70, training loss=1889.8459935832577, testing loss=509.1796589371522\n",
      "Current iteration=80, training loss=1884.6050025078605, testing loss=508.9050868244675\n",
      "[-0.88978064  1.02862523 -0.70042264 -0.10334482 -0.18762193  0.05445285\n",
      " -0.09676001 -0.0338816  -0.30386946  0.36810643  0.17987915 -0.01235336\n",
      "  0.03359423  0.04786577 -0.07723513  0.08833339 -0.06242726  0.01086544\n",
      "  0.06979042  0.09047133  0.20900599  0.02445142  0.03904657  0.04382067] 1881.681118951448\n",
      "training loss=1881.4270675367095\n",
      "testing loss=508.8850324966115\n",
      "Accuracy from our code:\n",
      "\n",
      "Logistic regression: 76.5 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from logistic_regression import *\n",
    "#Using our code\n",
    "#Logistic regression\n",
    "\n",
    "gamma = 0.0001\n",
    "n_iter = 90\n",
    "\n",
    "tx_training = np.c_[np.ones((y_training.shape[0], 1)), tx_training]\n",
    "tx_testing = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing]\n",
    "\n",
    "w = np.random.rand(tx_training.shape[1])\n",
    "for i in range(n_iter):\n",
    "    w, loss = learning_by_gradient_descent(y_training, tx_training, w, gamma)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Current iteration={iter}, training loss={l_tr}, testing loss={l_te}\".format(iter=i, l_tr=loss, \n",
    "                                                                                                         l_te=calculate_loss(y_testing, tx_testing, w)))\n",
    "print(w, loss)\n",
    "print(\"training loss={l_tr}\".format(l_tr=calculate_loss(y_training, tx_training, w)))\n",
    "print(\"testing loss={l_te}\".format(l_te=calculate_loss(y_testing, tx_testing, w)))\n",
    "\n",
    "print(\"Accuracy from our code:\\n\")\n",
    "print(\"Logistic regression: \"+str(round(100*np.sum(predict_labels(w, tx_testing, True)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(w, tx_testing, True)\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "283\n",
      "338\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_testing==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "#test on all data points from train.csv\n",
    "y_, tX_, ids_ = load_csv_data('../data/train.csv', sub_sample=False)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_ = np.delete(tX_, (4,5,6,12,26,27,28), axis=1)\n",
    "tX_ = correction_missing_values_test(tX_, col_median_tX)\n",
    "tX_ = remove_outliers_test(tX_, perc_25, perc_75, col_median_tX)\n",
    "tX_ = (tX_ - min_) / (max_ - min_)\n",
    "tX_ = (tX_ - mean) / std\n",
    "tX_ = np.c_[np.ones((tX_.shape[0], 1)), tX_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71465\n",
      "85667\n",
      "Logistic regression: 77.5576 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w, tX_, True)\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_==1]))\n",
    "print(\"Logistic regression: \"+str(round(100*np.sum(y_pred==y_)/len(y_),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, training loss=5522.318831095153, testing loss=1260.4333781867192\n",
      "Current iteration=10, training loss=2831.5040238577826, testing loss=687.3783248511893\n",
      "Current iteration=20, training loss=2204.334581461228, testing loss=563.8113249033254\n",
      "Current iteration=30, training loss=2019.5940856386665, testing loss=529.3483253001523\n",
      "Current iteration=40, training loss=1948.0357998134643, testing loss=517.0361769260752\n",
      "Current iteration=50, training loss=1915.5701962888809, testing loss=512.0828977498253\n",
      "Current iteration=60, training loss=1899.1268346255213, testing loss=510.0044088618023\n",
      "Current iteration=70, training loss=1890.118269395124, testing loss=509.16597625588525\n",
      "Current iteration=80, training loss=1884.8906396331927, testing loss=508.8911341349611\n",
      "[-0.8892887   1.02793143 -0.70016142 -0.10307377 -0.18741807  0.05434073\n",
      " -0.0966743  -0.03384179 -0.30361892  0.36787893  0.17993639 -0.01237023\n",
      "  0.03351597  0.04774421 -0.07715804  0.08822358 -0.06233701  0.01085407\n",
      "  0.0697599   0.09030497  0.208854    0.02442397  0.03895345  0.04389875] 1881.9755854479163\n",
      "training loss=1881.4347966323626\n",
      "testing loss=508.87038880719297\n",
      "After regularization: 76.5 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from reg_logistic_regression import *\n",
    "#Reguralized logistic regression\n",
    "\n",
    "gamma = 0.0001\n",
    "lamda_ = 0.105\n",
    "n_iter = 90\n",
    "\n",
    "tx_training = np.c_[np.ones((y_training.shape[0], 1)), tx_training]\n",
    "tx_testing = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing]\n",
    "\n",
    "w_reg = np.random.rand(tx_training.shape[1])\n",
    "for i in range(n_iter):\n",
    "    w_reg, loss = learning_by_penalized_gradient(y_training, tx_training, w_reg, gamma, lamda_)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Current iteration={iter}, training loss={l_tr}, testing loss={l_te}\".format(iter=i, l_tr=loss, \n",
    "                                                                                                         l_te=calculate_loss(y_testing, tx_testing, w_reg)))\n",
    "print(w_reg, loss)\n",
    "print(\"training loss={l_tr}\".format(l_tr=calculate_loss(y_training, tx_training, w_reg)))\n",
    "print(\"testing loss={l_te}\".format(l_te=calculate_loss(y_testing, tx_testing, w_reg)))\n",
    "\n",
    "print(\"After regularization: \"+str(round(100*np.sum(predict_labels(w_reg, tx_testing, True)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(w_reg, tx_testing, True)\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "283\n",
      "338\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_testing==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on all data points from train.csv\n",
    "y_, tX_, ids_ = load_csv_data('../data/train.csv', sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_ = np.delete(tX_, (4,5,6,12,26,27,28), axis=1)\n",
    "tX_ = correction_missing_values_test(tX_, col_median_tX)\n",
    "tX_ = remove_outliers_test(tX_, perc_25, perc_75, col_median_tX)\n",
    "tX_ = (tX_ - min_) / (max_ - min_)\n",
    "tX_ = (tX_ - mean) / std\n",
    "tX_ = np.c_[np.ones((tX_.shape[0], 1)), tX_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71468\n",
      "85667\n",
      "Regularized logistic regression: 77.554 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w_reg, tX_, True)\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_==1]))\n",
    "print(\"Regularized logistic regression: \"+str(round(100*np.sum(y_pred==y_)/len(y_),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_search import *\n",
    "# optimising the lambda for regularized logistic regression \n",
    "\n",
    "gamma = 0.0001\n",
    "n_iter = 10000\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "n_intervals = 20\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "w_reg = np.random.rand(tX.shape[1])\n",
    "\n",
    "lambdas = generate_lambda(n_intervals)\n",
    "min_loss, best_lambda = grid_search(y_training, y_testing, tx_training, tx_testing, w_reg, lambdas, gamma, n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The optimum lambda is : \" + str(best_lambda))\n",
    "print(\"The corresponding loss is : \" + str(min_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights=[ 3.66598441e-01  2.09398782e-01 -3.05068879e-03 -1.99818081e-02\n",
      " -1.83384785e-01 -2.66200489e-02  3.07285822e-02  4.52252769e-02\n",
      " -4.47537037e-02 -3.10521859e-03 -5.95299805e-02 -3.76918551e-03\n",
      "  7.79476558e-03 -2.41872499e-02 -1.72349806e-02 -2.32072324e-04\n",
      " -1.85236296e-02  2.80616785e-02 -4.32533197e-03  5.53024378e-03\n",
      "  4.05712481e-03 -3.72715217e-03 -2.29538811e-02  9.83291248e-03\n",
      " -5.47150075e-04  5.86382336e-02  3.54795335e-02 -1.12674819e-02\n",
      "  5.87072500e-02 -3.34374623e-03 -2.06295721e-03 -4.72162526e-03\n",
      " -1.72343279e-02 -4.89215164e-04  7.56382855e-03 -8.47417844e-03\n",
      " -2.58754396e-03 -2.59801853e-02  2.87535229e-03  4.10879532e-04\n",
      "  2.79087785e-03 -3.04015839e-02 -4.17405857e-03  1.79978944e-02\n",
      " -6.36495793e-03 -2.52412325e-03  3.76673750e-03  8.94330230e-03\n",
      " -1.46389377e-03  3.42666271e-02  1.50008577e-03 -1.81931039e-02\n",
      "  3.50202375e-04 -6.25274602e-03  9.15348524e-04  7.63380754e-02\n",
      "  2.34957727e-02 -3.80639468e-02  2.57594706e-02 -1.55082641e-02\n",
      "  3.98318897e-03 -1.41403584e-03  3.01647940e-02  5.39518174e-04\n",
      "  1.49200055e-02  3.34820492e-03 -2.13973190e-03 -1.75506880e-02\n",
      "  3.56894965e-02 -9.72747885e-03], loss=0.0697153219398077\n",
      "training loss=0.0697153219398077\n",
      "testing loss=0.07828847199031279\n",
      "After least squares: 78.7 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from least_squares import *\n",
    "#least squares\n",
    "\n",
    "#try polynomial\n",
    "degree = 3\n",
    "tx_training = build_poly(tx_training, degree)\n",
    "tx_training = np.c_[np.ones((y_training.shape[0], 1)), tx_training]\n",
    "tx_testing = build_poly(tx_testing, degree)\n",
    "tx_testing = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing]\n",
    "\n",
    "w_lsq, loss = least_squares(y_training, tx_training)\n",
    "print(\"weights={w}, loss={l}\".format(w=w_lsq, l=loss))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, w_lsq)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, w_lsq)))\n",
    "print(\"After least squares: \"+str(round(100*np.sum(predict_labels(w_lsq, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(w_lsq, tx_testing, False)\n",
    "#print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "287\n",
      "338\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred))\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_testing==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n"
     ]
    }
   ],
   "source": [
    "#test on all data points from train.csv\n",
    "y_, tX_, ids_ = load_csv_data('../data/train.csv', sub_sample=False)\n",
    "print(y_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_ = np.delete(tX_, (4,5,6,12,26,27,28), axis=1)\n",
    "tX_ = correction_missing_values_test(tX_, col_median_tX)\n",
    "tX_ = remove_outliers_test(tX_, perc_25, perc_75, col_median_tX)\n",
    "tX_ = (tX_ - min_) / (max_ - min_)\n",
    "tX_ = (tX_ - mean) / std\n",
    "tX_ = build_poly(tX_, degree)\n",
    "tX_ = np.c_[np.ones((tX_.shape[0], 1)), tX_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75823\n",
      "85667\n",
      "Least squares: 80.3456 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w_lsq, tX_, False)\n",
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_==1]))\n",
    "print(\"Least squares: \"+str(round(100*np.sum(y_pred==y_)/len(y_),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss=0.07906238228549702\n",
      "testing loss=0.08644314556672941\n",
      "Processing 1th experiment, degree=1, training rmse=0.3976490469886656, testing rmse=0.08644314556672941\n",
      "training loss=0.07257482112180884\n",
      "testing loss=0.08039932152981177\n",
      "Processing 2th experiment, degree=2, training rmse=0.380985094516331, testing rmse=0.08039932152981177\n",
      "training loss=0.0697153219398077\n",
      "testing loss=0.07828847199031279\n",
      "Processing 3th experiment, degree=3, training rmse=0.3734041294356764, testing rmse=0.07828847199031279\n",
      "training loss=0.06881541109575426\n",
      "testing loss=0.07782891476206491\n",
      "Processing 4th experiment, degree=4, training rmse=0.3709862830233869, testing rmse=0.07782891476206491\n",
      "training loss=30.954725931273213\n",
      "testing loss=34.610948733122264\n",
      "Processing 5th experiment, degree=5, training rmse=7.8682559606653895, testing rmse=34.610948733122264\n",
      "training loss=5.199442406434914\n",
      "testing loss=5.231264891349185\n",
      "Processing 6th experiment, degree=6, training rmse=3.2247301922594747, testing rmse=5.231264891349185\n",
      "training loss=5.810118049533278\n",
      "testing loss=5.6879516296408505\n",
      "Processing 7th experiment, degree=7, training rmse=3.4088467403311866, testing rmse=5.6879516296408505\n",
      "training loss=1.193676713602778\n",
      "testing loss=1.4272511852704153\n",
      "Processing 8th experiment, degree=8, training rmse=1.5451062834658191, testing rmse=1.4272511852704153\n"
     ]
    }
   ],
   "source": [
    "#choosing polynomial degree\n",
    "def polynomial_regression():\n",
    "    \"\"\"Constructing the polynomial basis function expansion of the data,\n",
    "       and then running least squares regression.\"\"\"\n",
    "    # define parameters\n",
    "    degrees = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "    for ind, degree in enumerate(degrees):\n",
    "        tx_training_ = build_poly(tx_training, degree)\n",
    "        tx_training_ = np.c_[np.ones((y_training.shape[0], 1)), tx_training_]\n",
    "        tx_testing_ = build_poly(tx_testing, degree)\n",
    "        tx_testing_ = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing_]\n",
    "\n",
    "        weights, mse = least_squares(y_training, tx_training_)\n",
    "        print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training_, weights)))\n",
    "        print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing_, weights)))\n",
    "        rmse = np.sqrt(2 * mse)\n",
    "\n",
    "        print(\"Processing {i}th experiment, degree={d}, training rmse={tr_loss}, testing rmse={te_loss}\".format(\n",
    "              i=ind + 1, d=degree, tr_loss=rmse, te_loss=compute_loss(y_testing, tx_testing_, weights)))\n",
    "        \n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, degree):\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "\n",
    "    y_testing = y[te_indice]\n",
    "    y_training = y[tr_indice]\n",
    "    tx_testing = tX[te_indice]\n",
    "    tx_training = tX[tr_indice]\n",
    "\n",
    "    tx_training = np.delete(tx_training, (4,5,6,12,26,27,28), axis=1)\n",
    "    tx_training, col_median_tX = correction_missing_values(tx_training)\n",
    "    tx_training, perc_25, perc_75, col_median_tX = remove_outliers(tx_training)\n",
    "    tx_training, max_, min_ = normalize(tx_training)\n",
    "    tx_training, mean, std = standardize(tx_training)\n",
    "    \n",
    "    tx_testing = np.delete(tx_testing, (4,5,6,12,26,27,28), axis=1)\n",
    "    tx_testing = correction_missing_values_test(tx_testing, col_median_tX)\n",
    "    tx_testing = remove_outliers_test(tx_testing, perc_25, perc_75, col_median_tX)\n",
    "    tx_testing = (tx_testing - min_) / (max_ - min_)\n",
    "    tx_testing = (tx_testing - mean) / std\n",
    "    \n",
    "    tx_training_ = build_poly(tx_training, degree)\n",
    "    tx_training_ = np.c_[np.ones((y_training.shape[0], 1)), tx_training_]\n",
    "    tx_testing_ = build_poly(tx_testing, degree)\n",
    "    tx_testing_ = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing_]\n",
    "\n",
    "    w_lsq, loss = least_squares(y_training, tx_training_)\n",
    "\n",
    "    loss_tr = np.sqrt(2 * compute_loss(y_training, tx_training_, w_lsq))\n",
    "    loss_te = np.sqrt(2 * compute_loss(y_testing, tx_testing_, w_lsq))\n",
    "\n",
    "    return loss_tr, loss_te, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_degree_selection(degrees, k_fold, seed = 1):\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    #for each degree, we compute the best lambdas and the associated rmse\n",
    "    #best_lambdas = []\n",
    "    best_rmses = []\n",
    "    #vary degree\n",
    "    for degree in degrees:\n",
    "        # cross validation\n",
    "        #rmse_te = []\n",
    "        #for lambda_ in lambdas:\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            _, loss_te, _ = cross_validation(y, tX, k_indices, k, degree)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        best_rmses.append(np.mean(rmse_te_tmp))\n",
    "        \n",
    "        #ind_lambda_opt = np.argmin(rmse_te)\n",
    "        #best_lambdas.append(lambdas[ind_lambda_opt])\n",
    "        #best_rmses.append(rmse_te[ind_lambda_opt])\n",
    "        \n",
    "    ind_best_degree =  np.argmin(best_rmses)\n",
    "        \n",
    "    return degrees[ind_best_degree]\n",
    "\n",
    "best_degree_selection(np.arange(1,11), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00001, training loss=0.06978, testing loss=0.07825\n",
      "lambda=0.00002, training loss=0.06978, testing loss=0.07825\n",
      "lambda=0.00005, training loss=0.06979, testing loss=0.07825\n",
      "lambda=0.00012, training loss=0.06980, testing loss=0.07825\n",
      "lambda=0.00027, training loss=0.06984, testing loss=0.07826\n",
      "lambda=0.00061, training loss=0.06992, testing loss=0.07826\n",
      "lambda=0.00139, training loss=0.07009, testing loss=0.07828\n",
      "lambda=0.00316, training loss=0.07044, testing loss=0.07832\n",
      "lambda=0.00720, training loss=0.07110, testing loss=0.07844\n",
      "lambda=0.01638, training loss=0.07226, testing loss=0.07867\n",
      "lambda=0.03728, training loss=0.07425, testing loss=0.07917\n",
      "lambda=0.08483, training loss=0.07748, testing loss=0.08023\n",
      "lambda=0.19307, training loss=0.08242, testing loss=0.08234\n",
      "lambda=0.43940, training loss=0.08946, testing loss=0.08616\n",
      "lambda=1.00000, training loss=0.09867, testing loss=0.09211\n",
      "weights=[ 2.37134529e-02  3.66651444e-02  2.33090657e-02  1.26343821e-02\n",
      " -3.17986506e-02  2.42137423e-02 -1.52199342e-02  1.97144846e-02\n",
      " -1.22396694e-02 -5.38657076e-03 -5.65907599e-03  1.42779104e-02\n",
      " -2.16970966e-03  9.04581125e-03  2.61453372e-05 -4.97941638e-03\n",
      " -7.86143733e-03  1.30354700e-02 -1.84634976e-03 -4.38581704e-04\n",
      "  4.17485546e-03  1.99865791e-03 -2.51170226e-02  1.07422457e-02\n",
      " -6.26714433e-04  1.31578393e-02  2.98647141e-02  1.73717795e-02\n",
      "  2.15788225e-02  1.34222843e-02 -1.84114643e-03 -9.45330281e-04\n",
      "  3.40315903e-03 -1.42595562e-03 -2.06214198e-04  1.78821908e-02\n",
      " -5.83634213e-04 -9.08427353e-03  1.08729298e-02 -9.85285206e-04\n",
      " -9.87854358e-04  1.43569838e-03 -4.28595869e-03  2.61473777e-03\n",
      "  1.99350853e-02  2.13693280e-03 -1.01829741e-02  2.18054215e-02\n",
      " -1.44876797e-03  2.57181160e-03  1.98370586e-02 -1.98632413e-03\n",
      "  1.93041489e-03  4.37581551e-03  2.67516297e-03 -3.02887819e-03\n",
      "  1.79594013e-02 -1.15804303e-02  2.61740601e-03  4.45975292e-03\n",
      "  3.04303624e-03  3.67076479e-04  2.86840111e-02  4.65716395e-04\n",
      " -1.99076040e-03  1.98149266e-02  6.52715696e-04 -3.59402865e-03\n",
      "  1.88544779e-02  1.26522483e-03], loss=0.09866597849713626\n",
      "training loss=0.08621922693141325\n",
      "testing loss=0.0921054361637092\n",
      "After ridge regression: 70.8 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ridge_regression import *\n",
    "#ridge regression\n",
    "\n",
    "\n",
    "degree = 3\n",
    "tx_training = build_poly(tx_training, degree)\n",
    "tx_training = np.c_[np.ones((y_training.shape[0], 1)), tx_training]\n",
    "tx_testing = build_poly(tx_testing, degree)\n",
    "tx_testing = np.c_[np.ones((y_testing.shape[0], 1)), tx_testing]\n",
    "\n",
    "lambdas = np.logspace(-5, 0, 15)\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    # ridge regression with a given lambda\n",
    "    weight_ridge, loss = ridge_regression(y_training, tx_training, lambda_)\n",
    "    print(\"lambda={lam:.5f}, training loss={l_tr:.5f}, testing loss={l_te:.5f}\".format(lam=lambda_, l_tr=loss, \n",
    "                                                                                       l_te=compute_loss(y_testing, tx_testing, weight_ridge)))\n",
    "    \n",
    "print(\"weights={w}, loss={l}\".format(w=weight_ridge, l=loss))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, weight_ridge)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, weight_ridge)))\n",
    "\n",
    "print(\"After ridge regression: \"+str(round(100*np.sum(predict_labels(weight_ridge, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=12.291883010453807\n",
      "Current iteration=1000, loss=0.10071737801961865\n",
      "Current iteration=2000, loss=0.09508135294572446\n",
      "Current iteration=3000, loss=0.09235130154805092\n",
      "Current iteration=4000, loss=0.09083357303280999\n",
      "weights=[-0.63533999 -0.1550554   0.25046177 -1.56523086 -0.19626246  0.80848127\n",
      "  0.5743233   0.39290491 -0.62852223  0.93930804 -0.39008463  0.67514876\n",
      " -0.56810384  1.03366328  0.58848327  1.40398672 -0.10265195  0.26138214\n",
      " -0.56507097  0.06959221  0.31976244 -0.09861862  0.2379758   0.12575469\n",
      "  0.39449208  0.2449233   0.15825706  0.3090895  -0.36485586 -0.01063269\n",
      "  0.13791344 -0.24721618], training loss=0.08989135678696848\n",
      "training loss=0.08989059311055095\n",
      "testing loss=0.0916365284054964\n",
      "After gradient descent: 72.1 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gradient_descent import *\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_ws, gradient_losses= gradient_descent(y_training, tx_training, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=gradient_ws, l=gradient_losses))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, gradient_ws)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, gradient_ws)))\n",
    "print(\"After gradient descent: \"+str(round(100*np.sum(predict_labels(gradient_ws, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=10.684155249537138\n",
      "Current iteration=100, loss=0.11611652153666797\n",
      "Current iteration=200, loss=0.14554730629189477\n",
      "Current iteration=300, loss=0.15729584012939785\n",
      "Current iteration=400, loss=0.14270265710658306\n",
      "Current iteration=500, loss=0.14298708683238764\n",
      "Current iteration=600, loss=0.10556115934044442\n",
      "Current iteration=700, loss=0.10730606578847834\n",
      "Current iteration=800, loss=0.14551074619988377\n",
      "Current iteration=900, loss=0.21453318010213285\n",
      "Current iteration=1000, loss=0.11551455058310207\n",
      "Current iteration=1100, loss=0.16175518591596363\n",
      "Current iteration=1200, loss=0.1205090926539504\n",
      "Current iteration=1300, loss=0.12327826509625146\n",
      "Current iteration=1400, loss=0.10193669583300485\n",
      "Current iteration=1500, loss=0.0981311189563209\n",
      "Current iteration=1600, loss=0.10670949634004609\n",
      "Current iteration=1700, loss=0.10871143412603702\n",
      "Current iteration=1800, loss=0.09593688576563207\n",
      "Current iteration=1900, loss=0.09624031797805332\n",
      "Current iteration=2000, loss=0.14465157112595062\n",
      "Current iteration=2100, loss=0.11531081540132122\n",
      "Current iteration=2200, loss=0.12581218995274956\n",
      "Current iteration=2300, loss=0.09500207807191174\n",
      "Current iteration=2400, loss=0.12523628169823547\n",
      "Current iteration=2500, loss=0.12174955822264694\n",
      "Current iteration=2600, loss=0.10160210166529182\n",
      "Current iteration=2700, loss=0.15615198040815262\n",
      "Current iteration=2800, loss=0.18584399019535516\n",
      "Current iteration=2900, loss=0.0972637838642432\n",
      "Current iteration=3000, loss=0.10865722189086531\n",
      "Current iteration=3100, loss=0.11356751220163719\n",
      "Current iteration=3200, loss=0.09288599100999133\n",
      "Current iteration=3300, loss=0.13428916213334555\n",
      "Current iteration=3400, loss=0.19894353148093608\n",
      "Current iteration=3500, loss=0.11667871788847228\n",
      "Current iteration=3600, loss=0.09954323456578638\n",
      "Current iteration=3700, loss=0.13818435891922054\n",
      "Current iteration=3800, loss=0.21580122317832234\n",
      "Current iteration=3900, loss=0.0959180464846572\n",
      "Current iteration=4000, loss=0.08985200046465622\n",
      "Current iteration=4100, loss=0.08982641574038834\n",
      "Current iteration=4200, loss=0.11899354662930639\n",
      "Current iteration=4300, loss=0.09148029602972145\n",
      "Current iteration=4400, loss=0.0958351315616195\n",
      "Current iteration=4500, loss=0.09805403685822828\n",
      "Current iteration=4600, loss=0.08936670407373677\n",
      "Current iteration=4700, loss=0.10812971209843943\n",
      "Current iteration=4800, loss=0.25566428874790276\n",
      "Current iteration=4900, loss=0.09245414306084061\n",
      "weights=[-7.56840847e-01  2.13088766e-01 -1.41284105e+00 -1.61766921e-01\n",
      "  5.21453761e-01  7.59636439e-01  1.41150350e-01 -5.92373135e-01\n",
      "  3.14893809e-01 -7.70022195e-03 -2.74330821e-01 -7.22916502e-01\n",
      "  1.55342814e+00  7.01136737e-01  1.03249140e+00  4.39626916e-01\n",
      "  1.62270978e-01 -7.86730428e-04 -8.92054353e-02  5.89802809e-01\n",
      "  1.81253515e-01 -1.19404676e-01  1.84092815e-01  2.27113752e-01\n",
      "  1.45230550e-01 -1.34219555e-01  8.51293647e-02 -1.88963510e-01\n",
      " -3.93527489e-02 -1.33615058e-01  2.38569442e-01], loss=0.08914888593335403\n",
      "training loss=0.08930297820116996\n",
      "testing loss=0.08823516920672592\n",
      "After stochastic gradient descent: 72.7 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stochastic_gradient_descent import *\n",
    "\n",
    "ratio = 0.8\n",
    "seed = 6\n",
    "\n",
    "tX = np.c_[np.ones((y.shape[0], 1)), tX]\n",
    "\n",
    "tx_training, tx_testing, y_training, y_testing = split_data(tX, y, ratio, seed)\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.random.rand(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_ws, sgd_losses = stochastic_gradient_descent(y_training, tx_training, w_initial, max_iters, gamma)\n",
    "print(\"weights={w}, training loss={l}\".format(w=sgd_ws, l=sgd_losses))\n",
    "print(\"training loss={l_tr}\".format(l_tr=compute_loss(y_training, tx_training, sgd_ws)))\n",
    "print(\"testing loss={l_te}\".format(l_te=compute_loss(y_testing, tx_testing, sgd_ws)))\n",
    "print(\"After stochastic gradient descent: \"+str(round(100*np.sum(predict_labels(sgd_ws, tx_testing, False)==y_testing)/len(y_testing),5))+\" %\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test = np.delete(tX_test, (4,5,6,12,26,27,28), axis=1)\n",
    "tX_test = correction_missing_values_test(tX_test, col_median_tX)\n",
    "tX_test = remove_outliers_test(tX_test, perc_25, perc_75, col_median_tX)\n",
    "tX_test = (tX_test - min_) / (max_ - min_)\n",
    "tX_test = (tX_test - mean) / std\n",
    "#tX_test = build_poly(tX_test, 3)\n",
    "tX_test = np.c_[np.ones((tX_test.shape[0], 1)), tX_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#OUTPUT_PATH = 'test_lsq4.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w, tX_test, True)\n",
    "y_pred[np.where(y_pred == 0)] = -1\n",
    "#create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162573\n",
      "405665\n"
     ]
    }
   ],
   "source": [
    "print(np.sum([y_pred==1]))\n",
    "print(np.sum([y_pred==-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
